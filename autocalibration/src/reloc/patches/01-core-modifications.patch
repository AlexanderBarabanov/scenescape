diff -Naur '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=pipelines' '--exclude=extractors' '--exclude=matchers' /tmp/hloc-latest/hloc/colmap_from_nvm.py hloc/colmap_from_nvm.py
--- /tmp/hloc-latest/hloc/colmap_from_nvm.py	2026-01-16 16:13:56.813698236 -0700
+++ hloc/colmap_from_nvm.py	2026-01-16 16:16:08.542578602 -0700
@@ -1,204 +1,193 @@
+# SPDX-FileCopyrightText: (C) 2025 Intel Corporation
+# SPDX-License-Identifier: Apache-2.0
+
 import argparse
 import sqlite3
+from tqdm import tqdm
 from collections import defaultdict
-from pathlib import Path
-
 import numpy as np
-from tqdm import tqdm
+from pathlib import Path
 
 from . import logger
-from .utils.read_write_model import (
-    CAMERA_MODEL_NAMES,
-    Camera,
-    Image,
-    Point3D,
-    write_model,
-)
+from .utils.read_write_model import Camera, Image, Point3D, CAMERA_MODEL_NAMES
+from .utils.read_write_model import write_model
 
 
 def recover_database_images_and_ids(database_path):
-    images = {}
-    cameras = {}
-    db = sqlite3.connect(str(database_path))
-    ret = db.execute("SELECT name, image_id, camera_id FROM images;")
-    for name, image_id, camera_id in ret:
-        images[name] = image_id
-        cameras[name] = camera_id
-    db.close()
-    logger.info(f"Found {len(images)} images and {len(cameras)} cameras in database.")
-    return images, cameras
+  images = {}
+  cameras = {}
+  db = sqlite3.connect(str(database_path))
+  ret = db.execute("SELECT name, image_id, camera_id FROM images;")
+  for name, image_id, camera_id in ret:
+    images[name] = image_id
+    cameras[name] = camera_id
+  db.close()
+  logger.info(
+      f'Found {len(images)} images and {len(cameras)} cameras in database.')
+  return images, cameras
 
 
 def quaternion_to_rotation_matrix(qvec):
-    qvec = qvec / np.linalg.norm(qvec)
-    w, x, y, z = qvec
-    R = np.array(
-        [
-            [1 - 2 * y * y - 2 * z * z, 2 * x * y - 2 * z * w, 2 * x * z + 2 * y * w],
-            [2 * x * y + 2 * z * w, 1 - 2 * x * x - 2 * z * z, 2 * y * z - 2 * x * w],
-            [2 * x * z - 2 * y * w, 2 * y * z + 2 * x * w, 1 - 2 * x * x - 2 * y * y],
-        ]
-    )
-    return R
+  qvec = qvec / np.linalg.norm(qvec)
+  w, x, y, z = qvec
+  R = np.array([
+      [1 - 2 * y * y - 2 * z * z, 2 * x * y - 2 * z * w, 2 * x * z + 2 * y * w],
+      [2 * x * y + 2 * z * w, 1 - 2 * x * x - 2 * z * z, 2 * y * z - 2 * x * w],
+      [2 * x * z - 2 * y * w, 2 * y * z + 2 * x * w, 1 - 2 * x * x - 2 * y * y]])
+  return R
 
 
 def camera_center_to_translation(c, qvec):
-    R = quaternion_to_rotation_matrix(qvec)
-    return (-1) * np.matmul(R, c)
+  R = quaternion_to_rotation_matrix(qvec)
+  return (-1) * np.matmul(R, c)
 
 
-def read_nvm_model(nvm_path, intrinsics_path, image_ids, camera_ids, skip_points=False):
-    with open(intrinsics_path, "r") as f:
-        raw_intrinsics = f.readlines()
-
-    logger.info(f"Reading {len(raw_intrinsics)} cameras...")
-    cameras = {}
-    for intrinsics in raw_intrinsics:
-        intrinsics = intrinsics.strip("\n").split(" ")
-        name, camera_model, width, height = intrinsics[:4]
-        params = [float(p) for p in intrinsics[4:]]
-        camera_model = CAMERA_MODEL_NAMES[camera_model]
-        assert len(params) == camera_model.num_params
-        camera_id = camera_ids[name]
-        camera = Camera(
-            id=camera_id,
-            model=camera_model.model_name,
-            width=int(width),
-            height=int(height),
-            params=params,
-        )
-        cameras[camera_id] = camera
+def read_nvm_model(
+        nvm_path, intrinsics_path, image_ids, camera_ids, skip_points=False):
 
-    nvm_f = open(nvm_path, "r")
+  with open(intrinsics_path, 'r') as f:
+    raw_intrinsics = f.readlines()
+
+  logger.info(f'Reading {len(raw_intrinsics)} cameras...')
+  cameras = {}
+  for intrinsics in raw_intrinsics:
+    intrinsics = intrinsics.strip('\n').split(' ')
+    name, camera_model, width, height = intrinsics[:4]
+    params = [float(p) for p in intrinsics[4:]]
+    camera_model = CAMERA_MODEL_NAMES[camera_model]
+    assert len(params) == camera_model.num_params
+    camera_id = camera_ids[name]
+    camera = Camera(
+        id=camera_id, model=camera_model.model_name,
+        width=int(width), height=int(height), params=params)
+    cameras[camera_id] = camera
+
+  nvm_f = open(nvm_path, 'r')
+  line = nvm_f.readline()
+  while line == '\n' or line.startswith('NVM_V3'):
     line = nvm_f.readline()
-    while line == "\n" or line.startswith("NVM_V3"):
-        line = nvm_f.readline()
-    num_images = int(line)
-    assert num_images == len(cameras)
-
-    logger.info(f"Reading {num_images} images...")
-    image_idx_to_db_image_id = []
-    image_data = []
-    i = 0
-    while i < num_images:
-        line = nvm_f.readline()
-        if line == "\n":
-            continue
-        data = line.strip("\n").split(" ")
-        image_data.append(data)
-        image_idx_to_db_image_id.append(image_ids[data[0]])
-        i += 1
+  num_images = int(line)
+  assert num_images == len(cameras)
 
+  logger.info(f'Reading {num_images} images...')
+  image_idx_to_db_image_id = []
+  image_data = []
+  i = 0
+  while i < num_images:
     line = nvm_f.readline()
-    while line == "\n":
-        line = nvm_f.readline()
-    num_points = int(line)
-
-    if skip_points:
-        logger.info(f"Skipping {num_points} points.")
-        num_points = 0
+    if line == '\n':
+      continue
+    data = line.strip('\n').split(' ')
+    image_data.append(data)
+    image_idx_to_db_image_id.append(image_ids[data[0]])
+    i += 1
+
+  line = nvm_f.readline()
+  while line == '\n':
+    line = nvm_f.readline()
+  num_points = int(line)
+
+  if skip_points:
+    logger.info(f'Skipping {num_points} points.')
+    num_points = 0
+  else:
+    logger.info(f'Reading {num_points} points...')
+  points3D = {}
+  image_idx_to_keypoints = defaultdict(list)
+  i = 0
+  pbar = tqdm(total=num_points, unit='pts')
+  while i < num_points:
+    line = nvm_f.readline()
+    if line == '\n':
+      continue
+
+    data = line.strip('\n').split(' ')
+    x, y, z, r, g, b, num_observations = data[:7]
+    obs_image_ids, point2D_idxs = [], []
+    for j in range(int(num_observations)):
+      s = 7 + 4*j
+      img_index, kp_index, kx, ky = data[s:s+4]
+      image_idx_to_keypoints[int(img_index)].append(
+          (int(kp_index), float(kx), float(ky), i))
+      db_image_id = image_idx_to_db_image_id[int(img_index)]
+      obs_image_ids.append(db_image_id)
+      point2D_idxs.append(kp_index)
+
+    point = Point3D(
+        id=i,
+        xyz=np.array([x, y, z], float),
+        rgb=np.array([r, g, b], int),
+        error=1.,  # fake
+        image_ids=np.array(obs_image_ids, int),
+        point2D_idxs=np.array(point2D_idxs, int))
+    points3D[i] = point
+
+    i += 1
+    pbar.update(1)
+  pbar.close()
+
+  logger.info('Parsing image data...')
+  images = {}
+  for i, data in enumerate(image_data):
+    # Skip the focal length. Skip the distortion and terminal 0.
+    name, _, qw, qx, qy, qz, cx, cy, cz, _, _ = data
+    qvec = np.array([qw, qx, qy, qz], float)
+    c = np.array([cx, cy, cz], float)
+    t = camera_center_to_translation(c, qvec)
+
+    if i in image_idx_to_keypoints:
+      # NVM only stores triangulated 2D keypoints: add dummy ones
+      keypoints = image_idx_to_keypoints[i]
+      point2D_idxs = np.array([d[0] for d in keypoints])
+      tri_xys = np.array([[x, y] for _, x, y, _ in keypoints])
+      tri_ids = np.array([i for _, _, _, i in keypoints])
+
+      num_2Dpoints = max(point2D_idxs) + 1
+      xys = np.zeros((num_2Dpoints, 2), float)
+      point3D_ids = np.full(num_2Dpoints, -1, int)
+      xys[point2D_idxs] = tri_xys
+      point3D_ids[point2D_idxs] = tri_ids
     else:
-        logger.info(f"Reading {num_points} points...")
-    points3D = {}
-    image_idx_to_keypoints = defaultdict(list)
-    i = 0
-    pbar = tqdm(total=num_points, unit="pts")
-    while i < num_points:
-        line = nvm_f.readline()
-        if line == "\n":
-            continue
-
-        data = line.strip("\n").split(" ")
-        x, y, z, r, g, b, num_observations = data[:7]
-        obs_image_ids, point2D_idxs = [], []
-        for j in range(int(num_observations)):
-            s = 7 + 4 * j
-            img_index, kp_index, kx, ky = data[s : s + 4]
-            image_idx_to_keypoints[int(img_index)].append(
-                (int(kp_index), float(kx), float(ky), i)
-            )
-            db_image_id = image_idx_to_db_image_id[int(img_index)]
-            obs_image_ids.append(db_image_id)
-            point2D_idxs.append(kp_index)
-
-        point = Point3D(
-            id=i,
-            xyz=np.array([x, y, z], float),
-            rgb=np.array([r, g, b], int),
-            error=1.0,  # fake
-            image_ids=np.array(obs_image_ids, int),
-            point2D_idxs=np.array(point2D_idxs, int),
-        )
-        points3D[i] = point
-
-        i += 1
-        pbar.update(1)
-    pbar.close()
-
-    logger.info("Parsing image data...")
-    images = {}
-    for i, data in enumerate(image_data):
-        # Skip the focal length. Skip the distortion and terminal 0.
-        name, _, qw, qx, qy, qz, cx, cy, cz, _, _ = data
-        qvec = np.array([qw, qx, qy, qz], float)
-        c = np.array([cx, cy, cz], float)
-        t = camera_center_to_translation(c, qvec)
-
-        if i in image_idx_to_keypoints:
-            # NVM only stores triangulated 2D keypoints: add dummy ones
-            keypoints = image_idx_to_keypoints[i]
-            point2D_idxs = np.array([d[0] for d in keypoints])
-            tri_xys = np.array([[x, y] for _, x, y, _ in keypoints])
-            tri_ids = np.array([i for _, _, _, i in keypoints])
-
-            num_2Dpoints = max(point2D_idxs) + 1
-            xys = np.zeros((num_2Dpoints, 2), float)
-            point3D_ids = np.full(num_2Dpoints, -1, int)
-            xys[point2D_idxs] = tri_xys
-            point3D_ids[point2D_idxs] = tri_ids
-        else:
-            xys = np.zeros((0, 2), float)
-            point3D_ids = np.full(0, -1, int)
-
-        image_id = image_ids[name]
-        image = Image(
-            id=image_id,
-            qvec=qvec,
-            tvec=t,
-            camera_id=camera_ids[name],
-            name=name,
-            xys=xys,
-            point3D_ids=point3D_ids,
-        )
-        images[image_id] = image
+      xys = np.zeros((0, 2), float)
+      point3D_ids = np.full(0, -1, int)
+
+    image_id = image_ids[name]
+    image = Image(
+        id=image_id,
+        qvec=qvec,
+        tvec=t,
+        camera_id=camera_ids[name],
+        name=name,
+        xys=xys,
+        point3D_ids=point3D_ids)
+    images[image_id] = image
 
-    return cameras, images, points3D
+  return cameras, images, points3D
 
 
 def main(nvm, intrinsics, database, output, skip_points=False):
-    assert nvm.exists(), nvm
-    assert intrinsics.exists(), intrinsics
-    assert database.exists(), database
-
-    image_ids, camera_ids = recover_database_images_and_ids(database)
-
-    logger.info("Reading the NVM model...")
-    model = read_nvm_model(
-        nvm, intrinsics, image_ids, camera_ids, skip_points=skip_points
-    )
-
-    logger.info("Writing the COLMAP model...")
-    output.mkdir(exist_ok=True, parents=True)
-    write_model(*model, path=str(output), ext=".bin")
-    logger.info("Done.")
-
-
-if __name__ == "__main__":
-    parser = argparse.ArgumentParser()
-    parser.add_argument("--nvm", required=True, type=Path)
-    parser.add_argument("--intrinsics", required=True, type=Path)
-    parser.add_argument("--database", required=True, type=Path)
-    parser.add_argument("--output", required=True, type=Path)
-    parser.add_argument("--skip_points", action="store_true")
-    args = parser.parse_args()
-    main(**args.__dict__)
+  assert nvm.exists(), nvm
+  assert intrinsics.exists(), intrinsics
+  assert database.exists(), database
+
+  image_ids, camera_ids = recover_database_images_and_ids(database)
+
+  logger.info('Reading the NVM model...')
+  model = read_nvm_model(
+      nvm, intrinsics, image_ids, camera_ids, skip_points=skip_points)
+
+  logger.info('Writing the COLMAP model...')
+  output.mkdir(exist_ok=True, parents=True)
+  write_model(*model, path=str(output), ext='.bin')
+  logger.info('Done.')
+
+
+if __name__ == '__main__':
+  parser = argparse.ArgumentParser()
+  parser.add_argument('--nvm', required=True, type=Path)
+  parser.add_argument('--intrinsics', required=True, type=Path)
+  parser.add_argument('--database', required=True, type=Path)
+  parser.add_argument('--output', required=True, type=Path)
+  parser.add_argument('--skip_points', action='store_true')
+  args = parser.parse_args()
+  main(**args.__dict__)
diff -Naur '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=pipelines' '--exclude=extractors' '--exclude=matchers' /tmp/hloc-latest/hloc/extract_features.py hloc/extract_features.py
--- /tmp/hloc-latest/hloc/extract_features.py	2026-01-16 16:13:56.813698236 -0700
+++ hloc/extract_features.py	2026-01-16 16:16:08.542849538 -0700
@@ -1,136 +1,56 @@
+# SPDX-FileCopyrightText: (C) 2025 Intel Corporation
+# SPDX-License-Identifier: Apache-2.0
+
 import argparse
-import collections.abc as collections
-import glob
-import pprint
+import torch
 from pathlib import Path
+from addict import Dict
+from typing import List, Union, Optional
+import h5py
 from types import SimpleNamespace
-from typing import Dict, List, Optional, Union
-
 import cv2
-import h5py
 import numpy as np
-import PIL.Image
-import torch
 from tqdm import tqdm
+import pprint
+import collections.abc as collections
+import PIL.Image
 
 from . import extractors, logger
-from .utils.base_model import dynamic_load
-from .utils.io import list_h5_names, read_image
+from .utils.base_model import cached_load
+from .utils.tools import map_tensor
 from .utils.parsers import parse_image_lists
+from .utils.io import read_image, list_h5_names, base64_to_image
+
 
-"""
-A set of standard configurations that can be directly selected from the command
-line using their name. Each is a dictionary with the following entries:
-    - output: the name of the feature file that will be generated.
-    - model: the model configuration, as passed to a feature extractor.
-    - preprocessing: how to preprocess the images read from disk.
-"""
 confs = {
     "superpoint_aachen": {
         "output": "feats-superpoint-n4096-r1024",
-        "model": {
-            "name": "superpoint",
-            "nms_radius": 3,
-            "max_keypoints": 4096,
-        },
-        "preprocessing": {
-            "grayscale": True,
-            "resize_max": 1024,
-        },
+        "model": {"name": "superpoint", "nms_radius": 3, "max_keypoints": 4096},
+        "preprocessing": {"grayscale": True, "resize_max": 1024},
     },
     # Resize images to 1600px even if they are originally smaller.
     # Improves the keypoint localization if the images are of good quality.
     "superpoint_max": {
         "output": "feats-superpoint-n4096-rmax1600",
-        "model": {
-            "name": "superpoint",
-            "nms_radius": 3,
-            "max_keypoints": 4096,
-        },
-        "preprocessing": {
-            "grayscale": True,
-            "resize_max": 1600,
-            "resize_force": True,
-        },
+        "model": {"name": "superpoint", "nms_radius": 3, "max_keypoints": 4096},
+        "preprocessing": {"grayscale": True, "resize_max": 1600, "resize_force": True},
     },
     "superpoint_inloc": {
         "output": "feats-superpoint-n4096-r1600",
-        "model": {
-            "name": "superpoint",
-            "nms_radius": 4,
-            "max_keypoints": 4096,
-        },
-        "preprocessing": {
-            "grayscale": True,
-            "resize_max": 1600,
-        },
-    },
-    "r2d2": {
-        "output": "feats-r2d2-n5000-r1024",
-        "model": {
-            "name": "r2d2",
-            "max_keypoints": 5000,
-        },
-        "preprocessing": {
-            "grayscale": False,
-            "resize_max": 1024,
-        },
-    },
-    "d2net-ss": {
-        "output": "feats-d2net-ss",
-        "model": {
-            "name": "d2net",
-            "multiscale": False,
-        },
-        "preprocessing": {
-            "grayscale": False,
-            "resize_max": 1600,
-        },
+        "model": {"name": "superpoint", "nms_radius": 4, "max_keypoints": 4096},
+        "preprocessing": {"grayscale": True, "resize_max": 1600},
     },
     "sift": {
         "output": "feats-sift",
         "model": {"name": "dog"},
-        "preprocessing": {
-            "grayscale": True,
-            "resize_max": 1600,
-        },
+        "preprocessing": {"grayscale": True, "resize_max": 1600},
     },
     "sosnet": {
         "output": "feats-sosnet",
         "model": {"name": "dog", "descriptor": "sosnet"},
-        "preprocessing": {
-            "grayscale": True,
-            "resize_max": 1600,
-        },
-    },
-    "disk": {
-        "output": "feats-disk",
-        "model": {
-            "name": "disk",
-            "max_keypoints": 5000,
-        },
-        "preprocessing": {
-            "grayscale": False,
-            "resize_max": 1600,
-        },
-    },
-    "aliked-n16": {
-        "output": "feats-aliked-n16",
-        "model": {
-            "name": "aliked",
-            "model_name": "aliked-n16",
-        },
-        "preprocessing": {
-            "grayscale": False,
-            "resize_max": 1024,
-        },
+        "preprocessing": {"grayscale": True, "resize_max": 1600},
     },
     # Global descriptors
-    "dir": {
-        "output": "global-feats-dir",
-        "model": {"name": "dir"},
-        "preprocessing": {"resize_max": 1024},
-    },
     "netvlad": {
         "output": "global-feats-netvlad",
         "model": {"name": "netvlad"},
@@ -141,92 +61,100 @@
         "model": {"name": "openibl"},
         "preprocessing": {"resize_max": 1024},
     },
-    "megaloc": {
-        "output": "global-feats-megaloc",
-        "model": {"name": "megaloc"},
-        "preprocessing": {"resize_max": 1024},
-    },
 }
+"""A set of standard configurations that can be directly selected from the
+command line using their name. Each is a dictionary with the following entries:
+    - output: the name of the feature file that will be generated.
+    - model: the model configuration, as passed to a feature extractor.
+    - preprocessing: how to preprocess the images read from disk.
+"""
 
 
 def resize_image(image, size, interp):
-    if interp.startswith("cv2_"):
-        interp = getattr(cv2, "INTER_" + interp[len("cv2_") :].upper())
-        h, w = image.shape[:2]
-        if interp == cv2.INTER_AREA and (w < size[0] or h < size[1]):
-            interp = cv2.INTER_LINEAR
-        resized = cv2.resize(image, size, interpolation=interp)
-    elif interp.startswith("pil_"):
-        interp = getattr(PIL.Image, interp[len("pil_") :].upper())
-        resized = PIL.Image.fromarray(image.astype(np.uint8))
-        resized = resized.resize(size, resample=interp)
-        resized = np.asarray(resized, dtype=image.dtype)
-    else:
-        raise ValueError(f"Unknown interpolation {interp}.")
-    return resized
+  if interp.startswith("cv2_"):
+    interp = getattr(cv2, "INTER_" + interp[len("cv2_"):].upper())
+    h, w = image.shape[:2]
+    if interp == cv2.INTER_AREA and (w < size[0] or h < size[1]):
+      interp = cv2.INTER_LINEAR
+    resized = cv2.resize(image, size, interpolation=interp)
+  elif interp.startswith("pil_"):
+    interp = getattr(PIL.Image, interp[len("pil_"):].upper())
+    resized = PIL.Image.fromarray(image.astype(np.uint8))
+    resized = resized.resize(size, resample=interp)
+    resized = np.asarray(resized, dtype=image.dtype)
+  else:
+    raise ValueError(f"Unknown interpolation {interp}.")
+  return resized
 
 
 class ImageDataset(torch.utils.data.Dataset):
-    default_conf = {
-        "globs": ["*.jpg", "*.png", "*.jpeg", "*.JPG", "*.PNG"],
-        "grayscale": False,
-        "resize_max": None,
-        "resize_force": False,
-        "interpolation": "cv2_area",  # pil_linear is more accurate but slower
-    }
-
-    def __init__(self, root, conf, paths=None):
-        self.conf = conf = SimpleNamespace(**{**self.default_conf, **conf})
-        self.root = root
-
-        if paths is None:
-            paths = []
-            for g in conf.globs:
-                paths += glob.glob((Path(root) / "**" / g).as_posix(), recursive=True)
-            if len(paths) == 0:
-                raise ValueError(f"Could not find any image in root: {root}.")
-            paths = sorted(set(paths))
-            self.names = [Path(p).relative_to(root).as_posix() for p in paths]
-            logger.info(f"Found {len(self.names)} images in root {root}.")
-        else:
-            if isinstance(paths, (Path, str)):
-                self.names = parse_image_lists(paths)
-            elif isinstance(paths, collections.Iterable):
-                self.names = [p.as_posix() if isinstance(p, Path) else p for p in paths]
-            else:
-                raise ValueError(f"Unknown format for path argument {paths}.")
-
-            for name in self.names:
-                if not (root / name).exists():
-                    raise ValueError(f"Image {name} does not exists in root: {root}.")
-
-    def __getitem__(self, idx):
-        name = self.names[idx]
-        image = read_image(self.root / name, self.conf.grayscale)
-        image = image.astype(np.float32)
-        size = image.shape[:2][::-1]
-
-        if self.conf.resize_max and (
-            self.conf.resize_force or max(size) > self.conf.resize_max
-        ):
-            scale = self.conf.resize_max / max(size)
-            size_new = tuple(int(round(x * scale)) for x in size)
-            image = resize_image(image, size_new, self.conf.interpolation)
-
-        if self.conf.grayscale:
-            image = image[None]
-        else:
-            image = image.transpose((2, 0, 1))  # HxWxC to CxHxW
-        image = image / 255.0
-
-        data = {
-            "image": image,
-            "original_size": np.array(size),
-        }
-        return data
+  default_conf = {
+      "globs": ["*.jpg", "*.jpeg", "*.JPG"],
+      "grayscale": False,
+      "resize_max": None,
+      "resize_force": False,
+      "interpolation": "cv2_area",  # pil_linear is more accurate but slower
+  }
+
+  def __init__(self, root, conf, paths=None):
+    self.conf = Dict(self.default_conf) | Dict(conf)
+    self.root = root
+
+    if paths is None:
+      paths = []
+      for g in self.conf.globs:
+        paths += list(Path(root).glob("**/" + g))
+      if len(paths) == 0:
+        raise ValueError(f"Could not find any image in root: {root}.")
+      paths = sorted(list(set(paths)))
+      self.names = [i.relative_to(root).as_posix() for i in paths]
+      logger.info(f"Found {len(self.names)} images in root {root}.")
+    elif isinstance(paths, dict):  # SceneScape message
+      self.root = (paths,)
+      self.names = (paths["name"],)
+    else:
+      if isinstance(paths, (Path, str)):
+        self.names = parse_image_lists(paths)
+      elif isinstance(paths, collections.Iterable):
+        self.names = [p.as_posix() if isinstance(p, Path) else p for p in paths]
+      else:
+        raise ValueError(f"Unknown format for path argument {paths}.")
+
+      for name in self.names:
+        if not (root / name).exists():
+          raise ValueError(f"Image {name} does not exists in root: {root}.")
+
+  def __getitem__(self, idx):
+    name = self.names[idx]
+    if isinstance(self.root, tuple):  # SceneScape message
+      image = base64_to_image(
+          self.root[idx]["image_data"],
+          self.conf.grayscale,
+          log_as=self.names[idx],
+      )
+    else:
+      image = read_image(self.root / name, self.conf.grayscale)
+    image = image.astype(np.float32)
+    size = image.shape[:2][::-1]
+
+    if self.conf.resize_max and (
+        self.conf.resize_force or max(size) > self.conf.resize_max
+    ):
+      scale = self.conf.resize_max / max(size)
+      size_new = tuple(int(round(x * scale)) for x in size)
+      image = resize_image(image, size_new, self.conf.interpolation)
 
-    def __len__(self):
-        return len(self.names)
+    if self.conf.grayscale:
+      image = image[None]
+    else:
+      image = image.transpose((2, 0, 1))  # HxWxC to CxHxW
+    image = image / 255.0
+
+    data = {"name": name, "image": image, "original_size": np.array(size)}
+    return data
+
+  def __len__(self):
+    return len(self.names)
 
 
 @torch.no_grad()
@@ -235,94 +163,83 @@
     image_dir: Path,
     export_dir: Optional[Path] = None,
     as_half: bool = True,
-    image_list: Optional[Union[Path, List[str]]] = None,
+    image_list: Optional[Union[Path, List[str], Dict]] = None,
     feature_path: Optional[Path] = None,
     overwrite: bool = False,
 ) -> Path:
-    logger.info(
-        "Extracting local features with configuration:" f"\n{pprint.pformat(conf)}"
-    )
-
-    dataset = ImageDataset(image_dir, conf["preprocessing"], image_list)
-    if feature_path is None:
-        feature_path = Path(export_dir, conf["output"] + ".h5")
-    feature_path.parent.mkdir(exist_ok=True, parents=True)
-    skip_names = set(
-        list_h5_names(feature_path) if feature_path.exists() and not overwrite else ()
-    )
-    dataset.names = [n for n in dataset.names if n not in skip_names]
-    if len(dataset.names) == 0:
-        logger.info("Skipping the extraction.")
-        return feature_path
-
-    device = "cuda" if torch.cuda.is_available() else "cpu"
-    Model = dynamic_load(extractors, conf["model"]["name"])
-    model = Model(conf["model"]).eval().to(device)
-
-    loader = torch.utils.data.DataLoader(
-        dataset, num_workers=1, shuffle=False, pin_memory=True
-    )
-    for idx, data in enumerate(tqdm(loader)):
-        name = dataset.names[idx]
-        pred = model({"image": data["image"].to(device, non_blocking=True)})
-        pred = {k: v[0].cpu().numpy() for k, v in pred.items()}
+  logger.info(
+      "Extracting local features with configuration:" f"\n{pprint.pformat(conf)}"
+  )
+
+  loader = ImageDataset(image_dir, conf["preprocessing"], image_list)
+  loader = torch.utils.data.DataLoader(loader, num_workers=0)
+
+  if feature_path is None:
+    feature_path = Path(export_dir, conf["output"] + ".h5")
+  feature_path.parent.mkdir(exist_ok=True, parents=True)
+  skip_names = set(
+      list_h5_names(feature_path) if feature_path.exists() and not overwrite else ()
+  )
+  if set(loader.dataset.names).issubset(set(skip_names)):
+    logger.info("Skipping the extraction.")
+    return feature_path
 
-        pred["image_size"] = original_size = data["original_size"][0].numpy()
+  model = cached_load(extractors, conf["model"])
+  for data in tqdm(loader):
+    name = data["name"][0]  # remove batch dimension
+    if name in skip_names:
+      continue
+
+    pred = model(map_tensor(data, lambda x: x.to(cached_load.device)))
+    pred = {k: v[0].cpu().numpy() for k, v in pred.items()}
+
+    pred["image_size"] = original_size = data["original_size"][0].numpy()
+    if "keypoints" in pred:
+      size = np.array(data["image"].shape[-2:][::-1])
+      scales = (original_size / size).astype(np.float32)
+      pred["keypoints"] = (pred["keypoints"] + 0.5) * scales[None] - 0.5
+      # add keypoint uncertainties scaled to the original resolution
+      uncertainty = getattr(model, "detection_noise", 1) * scales.mean()
+
+    if as_half:
+      for k in pred:
+        dt = pred[k].dtype
+        if (dt == np.float32) and (dt != np.float16):
+          pred[k] = pred[k].astype(np.float16)
+
+    with h5py.File(str(feature_path), 'a', libver='latest') as fd:
+      try:
+        if name in fd:
+          del fd[name]
+        grp = fd.create_group(name)
+        for k, v in pred.items():
+          grp.create_dataset(k, data=v)
         if "keypoints" in pred:
-            size = np.array(data["image"].shape[-2:][::-1])
-            scales = (original_size / size).astype(np.float32)
-            pred["keypoints"] = (pred["keypoints"] + 0.5) * scales[None] - 0.5
-            if "scales" in pred:
-                pred["scales"] *= scales.mean()
-            # add keypoint uncertainties scaled to the original resolution
-            uncertainty = getattr(model, "detection_noise", 1) * scales.mean()
-
-        if as_half:
-            for k in pred:
-                dt = pred[k].dtype
-                if (dt == np.float32) and (dt != np.float16):
-                    pred[k] = pred[k].astype(np.float16)
-
-        with h5py.File(str(feature_path), "a", libver="latest") as fd:
-            try:
-                if name in fd:
-                    del fd[name]
-                grp = fd.create_group(name)
-                for k, v in pred.items():
-                    grp.create_dataset(k, data=v)
-                if "keypoints" in pred:
-                    grp["keypoints"].attrs["uncertainty"] = uncertainty
-            except OSError as error:
-                if "No space left on device" in error.args[0]:
-                    logger.error(
-                        "Out of disk space: storing features on disk can take "
-                        "significant space, did you enable the as_half flag?"
-                    )
-                    del grp, fd[name]
-                raise error
+          grp["keypoints"].attrs["uncertainty"] = uncertainty
+      except OSError as error:
+        if "No space left on device" in error.args[0]:
+          logger.error(
+              "Out of disk space: storing features on disk can take "
+              "significant space, did you enable the as_half flag?"
+          )
+          del grp, fd[name]
+        raise error
 
-        del pred
+    del pred
 
-    logger.info("Finished exporting features.")
-    return feature_path
+  logger.info("Finished exporting features.")
+  return feature_path
 
 
 if __name__ == "__main__":
-    parser = argparse.ArgumentParser()
-    parser.add_argument("--image_dir", type=Path, required=True)
-    parser.add_argument("--export_dir", type=Path, required=True)
-    parser.add_argument(
-        "--conf", type=str, default="superpoint_aachen", choices=list(confs.keys())
-    )
-    parser.add_argument("--as_half", action="store_true")
-    parser.add_argument("--image_list", type=Path)
-    parser.add_argument("--feature_path", type=Path)
-    args = parser.parse_args()
-    main(
-        confs[args.conf],
-        args.image_dir,
-        args.export_dir,
-        args.as_half,
-        args.image_list,
-        args.feature_path,
-    )
+  parser = argparse.ArgumentParser()
+  parser.add_argument("--image_dir", type=Path, required=True)
+  parser.add_argument("--export_dir", type=Path, required=True)
+  parser.add_argument(
+      "--conf", type=str, default="superpoint_aachen", choices=list(confs.keys())
+  )
+  parser.add_argument("--as_half", action="store_true")
+  parser.add_argument("--image_list", type=Path)
+  parser.add_argument("--feature_path", type=Path)
+  args = parser.parse_args()
+  main(confs[args.conf], args.image_dir, args.export_dir, args.as_half)
diff -Naur '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=pipelines' '--exclude=extractors' '--exclude=matchers' /tmp/hloc-latest/hloc/__init__.py hloc/__init__.py
--- /tmp/hloc-latest/hloc/__init__.py	2026-01-16 16:13:56.812282752 -0700
+++ hloc/__init__.py	2026-01-16 16:16:08.543004539 -0700
@@ -1,8 +1,11 @@
-import logging
+# SPDX-FileCopyrightText: (C) 2025 Intel Corporation
+# SPDX-License-Identifier: Apache-2.0
 
+from warnings import filterwarnings
+import logging
 from packaging import version
 
-__version__ = "1.5"
+__version__ = "1.3"
 
 formatter = logging.Formatter(
     fmt="[%(asctime)s %(name)s %(levelname)s] %(message)s", datefmt="%Y/%m/%d %H:%M:%S"
@@ -17,20 +20,24 @@
 logger.propagate = False
 
 try:
-    import pycolmap
+  import pycolmap
 except ImportError:
-    logger.warning("pycolmap is not installed, some features may not work.")
+  logger.warning("pycolmap is not installed, some features may not work.")
 else:
-    min_version = version.parse("3.13.0")
-    found_version = pycolmap.__version__
-    if found_version != "dev":
-        version = version.parse(found_version)
-        if version < min_version:
-            s = f"pycolmap>={min_version}"
-            logger.warning(
-                "hloc requires %s but found pycolmap==%s, "
-                'please upgrade with `pip install --upgrade "%s"`',
-                s,
-                found_version,
-                s,
-            )
+  minimal_version = version.parse('0.3.0')
+  found_version = version.parse(getattr(pycolmap, '__version__'))
+  if found_version < minimal_version:
+    logger.warning(
+        "hloc now requires pycolmap>=%s but found pycolmap==%s, "
+        "please upgrade with `pip install --upgrade pycolmap`",
+        minimal_version,
+        found_version,
+    )
+
+# Warnings filter
+
+filterwarnings(
+    "ignore",
+    message=r"""__floordiv__ is deprecated, and its behavior will change in a future """
+    r"""version of pytorch. It currently rounds toward 0.*""",
+)
diff -Naur '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=pipelines' '--exclude=extractors' '--exclude=matchers' /tmp/hloc-latest/hloc/localize_inloc.py hloc/localize_inloc.py
--- /tmp/hloc-latest/hloc/localize_inloc.py	2026-01-16 16:13:56.813844948 -0700
+++ hloc/localize_inloc.py	2026-01-16 16:16:08.543183960 -0700
@@ -1,178 +1,216 @@
+# SPDX-FileCopyrightText: (C) 2025 Intel Corporation
+# SPDX-License-Identifier: Apache-2.0
+
 import argparse
-import pickle
 from pathlib import Path
-
-import cv2
-import h5py
+import csv
+import pickle
 import numpy as np
-import pycolmap
-import torch
+import h5py
 from scipy.io import loadmat
+import torch
 from tqdm import tqdm
+import cv2
+import pycolmap
 
 from . import logger
-from .utils.io import write_poses
-from .utils.parsers import names_to_pair, parse_retrieval
+from .utils.parsers import parse_retrieval, names_to_pair
+from .utils.evaluate import evaluate
 
 
 def interpolate_scan(scan, kp):
-    h, w, c = scan.shape
-    kp = kp / np.array([[w - 1, h - 1]]) * 2 - 1
-    assert np.all(kp > -1) and np.all(kp < 1)
-    scan = torch.from_numpy(scan).permute(2, 0, 1)[None]
-    kp = torch.from_numpy(kp)[None, None]
-    grid_sample = torch.nn.functional.grid_sample
-
-    # To maximize the number of points that have depth:
-    # do bilinear interpolation first and then nearest for the remaining points
-    interp_lin = grid_sample(scan, kp, align_corners=True, mode="bilinear")[0, :, 0]
-    interp_nn = torch.nn.functional.grid_sample(
-        scan, kp, align_corners=True, mode="nearest"
-    )[0, :, 0]
-    interp = torch.where(torch.isnan(interp_lin), interp_nn, interp_lin)
-    valid = ~torch.any(torch.isnan(interp), 0)
-
-    kp3d = interp.T.numpy()
-    valid = valid.numpy()
-    return kp3d, valid
+  h, w, c = scan.shape
+  kp = kp / np.array([[w - 1, h - 1]]) * 2 - 1
+  assert np.all(kp > -1) and np.all(kp < 1)
+  scan = torch.from_numpy(scan).permute(2, 0, 1)[None]
+  kp = torch.from_numpy(kp)[None, None]
+  grid_sample = torch.nn.functional.grid_sample
+
+  # To maximize the number of points that have depth:
+  # do bilinear interpolation first and then nearest for the remaining points
+  interp_lin = grid_sample(scan, kp, align_corners=True,
+                           mode='bilinear')[0, :, 0]
+  interp_nn = torch.nn.functional.grid_sample(scan,
+                                              kp,
+                                              align_corners=True,
+                                              mode='nearest')[0, :, 0]
+  interp = torch.where(torch.isnan(interp_lin), interp_nn, interp_lin)
+  valid = ~torch.any(torch.isnan(interp), 0)
+
+  kp3d = interp.T.numpy()
+  valid = valid.numpy()
+  return kp3d, valid
 
 
 def get_scan_pose(dataset_dir, rpath):
-    split_image_rpath = rpath.split("/")
-    floor_name = split_image_rpath[-3]
-    scan_id = split_image_rpath[-2]
-    image_name = split_image_rpath[-1]
-    building_name = image_name[:3]
-
-    path = Path(
-        dataset_dir,
-        "database/alignments",
-        floor_name,
-        f"transformations/{building_name}_trans_{scan_id}.txt",
-    )
-    with open(path) as f:
-        raw_lines = f.readlines()
-
-    P_after_GICP = np.array(
-        [
-            np.fromstring(raw_lines[7], sep=" "),
-            np.fromstring(raw_lines[8], sep=" "),
-            np.fromstring(raw_lines[9], sep=" "),
-            np.fromstring(raw_lines[10], sep=" "),
-        ]
-    )
-
-    return P_after_GICP
-
-
-def pose_from_cluster(dataset_dir, q, retrieved, feature_file, match_file, skip=None):
-    height, width = cv2.imread(str(dataset_dir / q)).shape[:2]
-    cx = 0.5 * width
-    cy = 0.5 * height
-    focal_length = 4032.0 * 28.0 / 36.0
-
-    all_mkpq = []
-    all_mkpr = []
-    all_mkp3d = []
-    all_indices = []
-    kpq = feature_file[q]["keypoints"].__array__()
-    num_matches = 0
-
-    for i, r in enumerate(retrieved):
-        kpr = feature_file[r]["keypoints"].__array__()
-        pair = names_to_pair(q, r)
-        m = match_file[pair]["matches0"].__array__()
-        v = m > -1
-
-        if skip and (np.count_nonzero(v) < skip):
-            continue
-
-        mkpq, mkpr = kpq[v], kpr[m[v]]
-        num_matches += len(mkpq)
-
-        scan_r = loadmat(Path(dataset_dir, r + ".mat"))["XYZcut"]
-        mkp3d, valid = interpolate_scan(scan_r, mkpr)
-        Tr = get_scan_pose(dataset_dir, r)
-        mkp3d = (Tr[:3, :3] @ mkp3d.T + Tr[:3, -1:]).T
-
-        all_mkpq.append(mkpq[valid])
-        all_mkpr.append(mkpr[valid])
-        all_mkp3d.append(mkp3d[valid])
-        all_indices.append(np.full(np.count_nonzero(valid), i))
-
-    all_mkpq = np.concatenate(all_mkpq, 0)
-    all_mkpr = np.concatenate(all_mkpr, 0)
-    all_mkp3d = np.concatenate(all_mkp3d, 0)
-    all_indices = np.concatenate(all_indices, 0)
-
-    cfg = {
-        "model": "SIMPLE_PINHOLE",
-        "width": width,
-        "height": height,
-        "params": [focal_length, cx, cy],
-    }
-    estimation_options = pycolmap.AbsolutePoseEstimationOptions()
-    estimation_options.ransac.max_error = 48
-    ret = pycolmap.estimate_and_refine_absolute_pose(
-        all_mkpq, all_mkp3d, cfg, estimation_options
-    )
-    ret["cfg"] = cfg
-    return ret, all_mkpq, all_mkpr, all_mkp3d, all_indices, num_matches
-
-
-def main(dataset_dir, retrieval, features, matches, results, skip_matches=None):
-    assert retrieval.exists(), retrieval
-    assert features.exists(), features
-    assert matches.exists(), matches
-
-    retrieval_dict = parse_retrieval(retrieval)
-    queries = list(retrieval_dict.keys())
-
-    feature_file = h5py.File(features, "r", libver="latest")
-    match_file = h5py.File(matches, "r", libver="latest")
-
-    poses = {}
-    logs = {
-        "features": features,
-        "matches": matches,
-        "retrieval": retrieval,
-        "loc": {},
+  split_image_rpath = rpath.split('/')
+  floor_name = split_image_rpath[-3]
+  scan_id = split_image_rpath[-2]
+  image_name = split_image_rpath[-1]
+  building_name = image_name[:3]
+
+  path = Path(dataset_dir, 'database/alignments', floor_name,
+              f'transformations/{building_name}_trans_{scan_id}.txt')
+  with open(path) as f:
+    raw_lines = f.readlines()
+
+  P_after_GICP = np.array([
+      np.fromstring(raw_lines[7], sep=' '),
+      np.fromstring(raw_lines[8], sep=' '),
+      np.fromstring(raw_lines[9], sep=' '),
+      np.fromstring(raw_lines[10], sep=' ')
+  ])
+
+  return P_after_GICP
+
+
+def pose_from_cluster(dataset_dir,
+                      q,
+                      retrieved,
+                      feature_file,
+                      match_file,
+                      skip=None,
+                      match_dense=False):
+  height, width = cv2.imread(str(dataset_dir / q)).shape[:2]
+  cx = .5 * width
+  cy = .5 * height
+  focal_length = 4032. * 28. / 36.
+
+  all_mkpq = []
+  all_mkpr = []
+  all_mkp3d = []
+  all_indices = []
+  kpq = feature_file[q]['keypoints'].__array__()
+  num_matches = 0
+
+  for i, r in enumerate(retrieved):
+    pair = names_to_pair(q, r)
+    if match_dense:
+      mkpq = match_file[pair]['keypoints0'].__array__()
+      mkpr = match_file[pair]['keypoints1'].__array__()
+      if skip and (mkpr.shape[0] < skip):
+        continue
+    else:
+      kpr = feature_file[r]['keypoints'].__array__()
+      m = match_file[pair]['matches0'].__array__()
+      v = (m > -1)
+      if skip and (np.count_nonzero(v) < skip):
+        continue
+      mkpq, mkpr = kpq[v], kpr[m[v]]
+
+    num_matches += len(mkpq)
+
+    scan_r = loadmat(Path(dataset_dir, r + '.mat'))["XYZcut"]
+    mkp3d, valid = interpolate_scan(scan_r, mkpr)
+    Tr = get_scan_pose(dataset_dir, r)
+    mkp3d = (Tr[:3, :3] @ mkp3d.T + Tr[:3, -1:]).T
+
+    all_mkpq.append(mkpq[valid])
+    all_mkpr.append(mkpr[valid])
+    all_mkp3d.append(mkp3d[valid])
+    all_indices.append(np.full(np.count_nonzero(valid), i))
+
+  all_mkpq = np.concatenate(all_mkpq, 0)
+  all_mkpr = np.concatenate(all_mkpr, 0)
+  all_mkp3d = np.concatenate(all_mkp3d, 0)
+  all_indices = np.concatenate(all_indices, 0)
+
+  cfg = {
+      'model': 'SIMPLE_PINHOLE',
+      'width': width,
+      'height': height,
+      'params': [focal_length, cx, cy]
+  }
+  ret = pycolmap.absolute_pose_estimation(all_mkpq, all_mkp3d, cfg, 48.00)
+  ret['cfg'] = cfg
+  return ret, all_mkpq, all_mkpr, all_mkp3d, all_indices, num_matches
+
+
+def main(dataset_dir,
+         retrieval,
+         features,
+         matches,
+         results,
+         skip_matches=None,
+         match_dense=False,
+         poses_gt_file=None):
+
+  assert retrieval.exists(), retrieval
+  assert features.exists(), features
+  assert matches.exists(), matches
+
+  retrieval_dict = parse_retrieval(retrieval)
+  queries = list(retrieval_dict.keys())
+
+  feature_file = h5py.File(features, 'r', libver='latest')
+  match_file = h5py.File(matches, 'r', libver='latest')
+
+  poses = {}
+  logs = {
+      'features': features,
+      'matches': matches,
+      'retrieval': retrieval,
+      'loc': {},
+  }
+  logger.info('Starting localization...')
+  for q in tqdm(queries):
+    db = retrieval_dict[q]
+    ret, mkpq, mkpr, mkp3d, indices, num_matches = pose_from_cluster(
+        dataset_dir, q, db, feature_file, match_file, skip_matches,
+        match_dense)
+
+    poses[q] = (ret['qvec'], ret['tvec'])
+    logs['loc'][q] = {
+        'db': db,
+        'PnP_ret': ret,
+        'keypoints_query': mkpq,
+        'keypoints_db': mkpr,
+        '3d_points': mkp3d,
+        'indices_db': indices,
+        'num_matches': num_matches,
     }
-    logger.info("Starting localization...")
-    for q in tqdm(queries):
-        db = retrieval_dict[q]
-        ret, mkpq, mkpr, mkp3d, indices, num_matches = pose_from_cluster(
-            dataset_dir, q, db, feature_file, match_file, skip_matches
-        )
-
-        poses[q] = ret["cam_from_world"]
-        logs["loc"][q] = {
-            "db": db,
-            "PnP_ret": ret,
-            "keypoints_query": mkpq,
-            "keypoints_db": mkpr,
-            "3d_points": mkp3d,
-            "indices_db": indices,
-            "num_matches": num_matches,
-        }
-
-    logger.info(f"Writing poses to {results}...")
-    write_poses(poses, results, prepend_camera_name=False)
-
-    logs_path = f"{results}_logs.pkl"
-    logger.info(f"Writing logs to {logs_path}...")
-    with open(logs_path, "wb") as f:
-        pickle.dump(logs, f)
-    logger.info("Done!")
-
-
-if __name__ == "__main__":
-    parser = argparse.ArgumentParser()
-    parser.add_argument("--dataset_dir", type=Path, required=True)
-    parser.add_argument("--retrieval", type=Path, required=True)
-    parser.add_argument("--features", type=Path, required=True)
-    parser.add_argument("--matches", type=Path, required=True)
-    parser.add_argument("--results", type=Path, required=True)
-    parser.add_argument("--skip_matches", type=int)
-    args = parser.parse_args()
-    main(**args.__dict__)
+
+  if poses_gt_file is not None:
+    logger.info('Evaluating...')
+    poses_gt = {}
+    with open(dataset_dir / poses_gt_file, 'r') as poses_gt_csv:
+      for row in csv.reader(poses_gt_csv, delimiter=' '):
+        if row[0] in retrieval_dict:  # queries
+          qtvec = np.fromiter(
+              (float(row[idx]) for idx in range(1, 8)), dtype=float)
+          poses_gt[row[0]] = (qtvec[:4], qtvec[4:7])
+
+    thresholds_tRdeg = ((0.01, 1.), (0.02, 2.), (0.05, 5.), (0.1, 10.))
+    med_err_t, med_err_R, ratio_localized = evaluate(
+        poses, poses_gt, thresholds_tRdeg=thresholds_tRdeg)
+    out = f'Median errors: {med_err_t:.3f}m, {med_err_R:.3f}deg'
+    out += '\nImages localized within threshold:'
+    for (th_t, th_R), ratio_loc in zip(thresholds_tRdeg, ratio_localized):
+      out += f'\n{th_t*100:.0f}cm, {th_R:.0f}deg : {ratio_loc*100:.2f}%'
+    logger.info(out)
+
+  logger.info(f'Writing poses to {results}...')
+  with open(results, 'w') as f:
+    for q in queries:
+      qvec, tvec = poses[q]
+      qvec = ' '.join(map(str, qvec))
+      tvec = ' '.join(map(str, tvec))
+      f.write(f'{q} {qvec} {tvec}\n')
+
+  logs_path = f'{results}_logs.pkl'
+  logger.info(f'Writing logs to {logs_path}...')
+  with open(logs_path, 'wb') as f:
+    pickle.dump(logs, f)
+  logger.info('Done!')
+
+
+if __name__ == '__main__':
+  parser = argparse.ArgumentParser()
+  parser.add_argument('--dataset_dir', type=Path, required=True)
+  parser.add_argument('--retrieval', type=Path, required=True)
+  parser.add_argument('--features', type=Path, required=True)
+  parser.add_argument('--matches', type=Path, required=True)
+  parser.add_argument('--results', type=Path, required=True)
+  parser.add_argument('--skip_matches', type=int)
+  args = parser.parse_args()
+  main(**args.__dict__)
diff -Naur '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=pipelines' '--exclude=extractors' '--exclude=matchers' /tmp/hloc-latest/hloc/localize_sfm.py hloc/localize_sfm.py
--- /tmp/hloc-latest/hloc/localize_sfm.py	2026-01-16 16:13:56.813844948 -0700
+++ hloc/localize_sfm.py	2026-01-16 16:16:08.543398969 -0700
@@ -1,235 +1,240 @@
+# SPDX-FileCopyrightText: (C) 2025 Intel Corporation
+# SPDX-License-Identifier: Apache-2.0
+
 import argparse
-import pickle
-from collections import defaultdict
+import numpy as np
 from pathlib import Path
+from collections import defaultdict
 from typing import Dict, List, Union
-
-import numpy as np
-import pycolmap
 from tqdm import tqdm
+import pickle
+import pycolmap
 
 from . import logger
-from .utils.io import get_keypoints, get_matches, write_poses
+from .utils.io import get_keypoints, get_matches
 from .utils.parsers import parse_image_lists, parse_retrieval
 
 
-def do_covisibility_clustering(
-    frame_ids: List[int], reconstruction: pycolmap.Reconstruction
-):
-    clusters = []
-    visited = set()
-    for frame_id in frame_ids:
-        # Check if already labeled
-        if frame_id in visited:
-            continue
-
-        # New component
-        clusters.append([])
-        queue = {frame_id}
-        while len(queue):
-            exploration_frame = queue.pop()
-
-            # Already part of the component
-            if exploration_frame in visited:
-                continue
-            visited.add(exploration_frame)
-            clusters[-1].append(exploration_frame)
-
-            observed = reconstruction.images[exploration_frame].points2D
-            connected_frames = {
-                obs.image_id
-                for p2D in observed
-                if p2D.has_point3D()
-                for obs in reconstruction.points3D[p2D.point3D_id].track.elements
-            }
-            connected_frames &= set(frame_ids)
-            connected_frames -= visited
-            queue |= connected_frames
+def do_covisibility_clustering(frame_ids: List[int],
+                               reconstruction: pycolmap.Reconstruction):
+  clusters = []
+  visited = set()
+  for frame_id in frame_ids:
+    # Check if already labeled
+    if frame_id in visited:
+      continue
+
+    # New component
+    clusters.append([])
+    queue = {frame_id}
+    while len(queue):
+      exploration_frame = queue.pop()
+
+      # Already part of the component
+      if exploration_frame in visited:
+        continue
+      visited.add(exploration_frame)
+      clusters[-1].append(exploration_frame)
+
+      observed = reconstruction.images[exploration_frame].points2D
+      connected_frames = {
+          obs.image_id
+          for p2D in observed if p2D.has_point3D()
+          for obs in
+          reconstruction.points3D[p2D.point3D_id].track.elements
+      }
+      connected_frames &= set(frame_ids)
+      connected_frames -= visited
+      queue |= connected_frames
 
-    clusters = sorted(clusters, key=len, reverse=True)
-    return clusters
+  clusters = sorted(clusters, key=len, reverse=True)
+  return clusters
 
 
 class QueryLocalizer:
-    def __init__(self, reconstruction, config=None):
-        self.reconstruction = reconstruction
-        self.config = config or {}
-
-    def localize(self, points2D_all, points2D_idxs, points3D_id, query_camera):
-        points2D = points2D_all[points2D_idxs]
-        points3D = [self.reconstruction.points3D[j].xyz for j in points3D_id]
-        if points2D.shape[0] == 0:
-            return None
-        ret = pycolmap.estimate_and_refine_absolute_pose(
-            points2D,
-            points3D,
-            query_camera,
-            estimation_options=self.config.get("estimation", {}),
-            refinement_options=self.config.get("refinement", {}),
-        )
-        return ret
+  def __init__(self, reconstruction, config=None):
+    self.reconstruction = reconstruction
+    self.config = config or {}
+
+  def localize(self, points2D_all, points2D_idxs, points3D_id, query_camera):
+    points2D = points2D_all[points2D_idxs]
+    points3D = [self.reconstruction.points3D[j].xyz for j in points3D_id]
+    ret = pycolmap.absolute_pose_estimation(
+        points2D, points3D, query_camera,
+        estimation_options=self.config.get('estimation', {}),
+        refinement_options=self.config.get('refinement', {}),
+    )
+    return ret
 
 
 def pose_from_cluster(
-    localizer: QueryLocalizer,
-    qname: str,
-    query_camera: pycolmap.Camera,
-    db_ids: List[int],
-    features_path: Path,
-    matches_path: Path,
-    **kwargs,
-):
-    kpq = get_keypoints(features_path, qname)
-    kpq += 0.5  # COLMAP coordinates
-
-    kp_idx_to_3D = defaultdict(list)
-    kp_idx_to_3D_to_db = defaultdict(lambda: defaultdict(list))
-    num_matches = 0
-    for i, db_id in enumerate(db_ids):
-        image = localizer.reconstruction.images[db_id]
-        if image.num_points3D == 0:
-            logger.debug(f"No 3D points found for {image.name}.")
-            continue
-        points3D_ids = np.array(
-            [p.point3D_id if p.has_point3D() else -1 for p in image.points2D]
-        )
-
-        matches, _ = get_matches(matches_path, qname, image.name)
-        matches = matches[points3D_ids[matches[:, 1]] != -1]
-        num_matches += len(matches)
-        for idx, m in matches:
-            id_3D = points3D_ids[m]
-            kp_idx_to_3D_to_db[idx][id_3D].append(i)
-            # avoid duplicate observations
-            if id_3D not in kp_idx_to_3D[idx]:
-                kp_idx_to_3D[idx].append(id_3D)
-
-    idxs = list(kp_idx_to_3D.keys())
-    mkp_idxs = [i for i in idxs for _ in kp_idx_to_3D[i]]
-    mp3d_ids = [j for i in idxs for j in kp_idx_to_3D[i]]
-    ret = localizer.localize(kpq, mkp_idxs, mp3d_ids, query_camera, **kwargs)
-    if ret is not None:
-        ret["camera"] = query_camera
-
-    # mostly for logging and post-processing
-    mkp_to_3D_to_db = [
-        (j, kp_idx_to_3D_to_db[i][j]) for i in idxs for j in kp_idx_to_3D[i]
-    ]
-    log = {
-        "db": db_ids,
-        "PnP_ret": ret,
-        "keypoints_query": kpq[mkp_idxs],
-        "points3D_ids": mp3d_ids,
-        "points3D_xyz": None,  # we don't log xyz anymore because of file size
-        "num_matches": num_matches,
-        "keypoint_index_to_db": (mkp_idxs, mkp_to_3D_to_db),
-    }
-    return ret, log
-
-
-def main(
-    reference_sfm: Union[Path, pycolmap.Reconstruction],
-    queries: Path,
-    retrieval: Path,
-    features: Path,
-    matches: Path,
-    results: Path,
-    ransac_thresh: int = 12,
-    covisibility_clustering: bool = False,
-    prepend_camera_name: bool = False,
-    config: Dict = None,
-):
-    assert retrieval.exists(), retrieval
-    assert features.exists(), features
-    assert matches.exists(), matches
-
-    queries = parse_image_lists(queries, with_intrinsics=True)
-    retrieval_dict = parse_retrieval(retrieval)
-
-    logger.info("Reading the 3D model...")
-    if not isinstance(reference_sfm, pycolmap.Reconstruction):
-        reference_sfm = pycolmap.Reconstruction(reference_sfm)
-    db_name_to_id = {img.name: i for i, img in reference_sfm.images.items()}
-
-    config = {"estimation": {"ransac": {"max_error": ransac_thresh}}, **(config or {})}
-    localizer = QueryLocalizer(reference_sfm, config)
-
-    cam_from_world = {}
-    logs = {
-        "features": features,
-        "matches": matches,
-        "retrieval": retrieval,
-        "loc": {},
-    }
-    logger.info("Starting localization...")
-    for qname, qcam in tqdm(queries):
-        if qname not in retrieval_dict:
-            logger.warning(f"No images retrieved for query image {qname}. Skipping...")
-            continue
-        db_names = retrieval_dict[qname]
-        db_ids = []
-        for n in db_names:
-            if n not in db_name_to_id:
-                logger.warning(f"Image {n} was retrieved but not in database")
-                continue
-            db_ids.append(db_name_to_id[n])
-
-        if covisibility_clustering:
-            clusters = do_covisibility_clustering(db_ids, reference_sfm)
-            best_inliers = 0
-            best_cluster = None
-            logs_clusters = []
-            for i, cluster_ids in enumerate(clusters):
-                ret, log = pose_from_cluster(
-                    localizer, qname, qcam, cluster_ids, features, matches
-                )
-                if ret is not None and ret["num_inliers"] > best_inliers:
-                    best_cluster = i
-                    best_inliers = ret["num_inliers"]
-                logs_clusters.append(log)
-            if best_cluster is not None:
-                ret = logs_clusters[best_cluster]["PnP_ret"]
-                cam_from_world[qname] = ret["cam_from_world"]
-            logs["loc"][qname] = {
-                "db": db_ids,
-                "best_cluster": best_cluster,
-                "log_clusters": logs_clusters,
-                "covisibility_clustering": covisibility_clustering,
-            }
-        else:
-            ret, log = pose_from_cluster(
-                localizer, qname, qcam, db_ids, features, matches
-            )
-            if ret is not None:
-                cam_from_world[qname] = ret["cam_from_world"]
-            else:
-                closest = reference_sfm.images[db_ids[0]]
-                cam_from_world[qname] = closest.cam_from_world()
-            log["covisibility_clustering"] = covisibility_clustering
-            logs["loc"][qname] = log
-
-    logger.info(f"Localized {len(cam_from_world)} / {len(queries)} images.")
-    logger.info(f"Writing poses to {results}...")
-    write_poses(cam_from_world, results, prepend_camera_name=prepend_camera_name)
-
-    logs_path = f"{results}_logs.pkl"
-    logger.info(f"Writing logs to {logs_path}...")
-    # TODO: Resolve pickling issue with pycolmap objects.
-    with open(logs_path, "wb") as f:
-        pickle.dump(logs, f)
-    logger.info("Done!")
-
-
-if __name__ == "__main__":
-    parser = argparse.ArgumentParser()
-    parser.add_argument("--reference_sfm", type=Path, required=True)
-    parser.add_argument("--queries", type=Path, required=True)
-    parser.add_argument("--features", type=Path, required=True)
-    parser.add_argument("--matches", type=Path, required=True)
-    parser.add_argument("--retrieval", type=Path, required=True)
-    parser.add_argument("--results", type=Path, required=True)
-    parser.add_argument("--ransac_thresh", type=float, default=12.0)
-    parser.add_argument("--covisibility_clustering", action="store_true")
-    parser.add_argument("--prepend_camera_name", action="store_true")
-    args = parser.parse_args()
-    main(**args.__dict__)
+        localizer: QueryLocalizer,
+        qname: str,
+        query_camera: pycolmap.Camera,
+        db_ids: List[int],
+        features_path: Path,
+        matches_path: Path,
+        **kwargs):
+
+  kpq = get_keypoints(features_path, qname)
+  kpq += 0.5  # COLMAP coordinates
+
+  kp_idx_to_3D = defaultdict(list)
+  kp_idx_to_3D_to_db = defaultdict(lambda: defaultdict(list))
+  num_matches = 0
+  for i, db_id in enumerate(db_ids):
+    image = localizer.reconstruction.images[db_id]
+    if image.num_points3D() == 0:
+      logger.debug(f'No 3D points found for {image.name}.')
+      continue
+    points3D_ids = np.array([p.point3D_id if p.has_point3D() else -1
+                             for p in image.points2D])
+
+    matches, _ = get_matches(matches_path, qname, image.name)
+    matches = matches[points3D_ids[matches[:, 1]] != -1]
+    num_matches += len(matches)
+    for idx, m in matches:
+      id_3D = points3D_ids[m]
+      kp_idx_to_3D_to_db[idx][id_3D].append(i)
+      # avoid duplicate observations
+      if id_3D not in kp_idx_to_3D[idx]:
+        kp_idx_to_3D[idx].append(id_3D)
+
+  idxs = list(kp_idx_to_3D.keys())
+  mkp_idxs = [i for i in idxs for _ in kp_idx_to_3D[i]]
+  mp3d_ids = [j for i in idxs for j in kp_idx_to_3D[i]]
+  ret = localizer.localize(kpq, mkp_idxs, mp3d_ids, query_camera, **kwargs)
+  ret['camera'] = {
+      'model': query_camera.model_name,
+      'width': query_camera.width,
+      'height': query_camera.height,
+      'params': query_camera.params,
+  }
+
+  # mostly for logging and post-processing
+  mkp_to_3D_to_db = [(j, kp_idx_to_3D_to_db[i][j])
+                     for i in idxs for j in kp_idx_to_3D[i]]
+  log = {
+      'db': db_ids,
+      'PnP_ret': ret,
+      'keypoints_query': kpq[mkp_idxs],
+      'points3D_ids': mp3d_ids,
+      'points3D_xyz': None,  # we don't log xyz anymore because of file size
+      'num_matches': num_matches,
+      'keypoint_index_to_db': (mkp_idxs, mkp_to_3D_to_db),
+  }
+  return ret, log
+
+
+def main(reference_sfm: Union[Path, pycolmap.Reconstruction],
+         queries: Path,
+         retrieval: Path,
+         features: Path,
+         matches: Path,
+         results: Path,
+         ransac_thresh: int = 12,
+         covisibility_clustering: bool = False,
+         prepend_camera_name: bool = False,
+         config: Dict = None):
+
+  assert retrieval.exists(), retrieval
+  assert features.exists(), features
+  assert matches.exists(), matches
+
+  queries = parse_image_lists(queries, with_intrinsics=True)
+  retrieval_dict = parse_retrieval(retrieval)
+
+  logger.info('Reading the 3D model...')
+  if not isinstance(reference_sfm, pycolmap.Reconstruction):
+    reference_sfm = pycolmap.Reconstruction(reference_sfm)
+  db_name_to_id = {img.name: i for i, img in reference_sfm.images.items()}
+
+  config = {"estimation": {"ransac": {"max_error": ransac_thresh}},
+            **(config or {})}
+  localizer = QueryLocalizer(reference_sfm, config)
+
+  poses = {}
+  logs = {
+      'features': features,
+      'matches': matches,
+      'retrieval': retrieval,
+      'loc': {},
+  }
+  logger.info('Starting localization...')
+  for qname, qcam in tqdm(queries):
+    if qname not in retrieval_dict:
+      logger.warning(
+          f'No images retrieved for query image {qname}. Skipping...')
+      continue
+    db_names = retrieval_dict[qname]
+    db_ids = []
+    for n in db_names:
+      if n not in db_name_to_id:
+        logger.warning(f'Image {n} was retrieved but not in database')
+        continue
+      db_ids.append(db_name_to_id[n])
+
+    if covisibility_clustering:
+      clusters = do_covisibility_clustering(db_ids, reference_sfm)
+      best_inliers = 0
+      best_cluster = None
+      logs_clusters = []
+      for i, cluster_ids in enumerate(clusters):
+        ret, log = pose_from_cluster(
+            localizer, qname, qcam, cluster_ids, features, matches)
+        if ret['success'] and ret['num_inliers'] > best_inliers:
+          best_cluster = i
+          best_inliers = ret['num_inliers']
+        logs_clusters.append(log)
+      if best_cluster is not None:
+        ret = logs_clusters[best_cluster]['PnP_ret']
+        poses[qname] = (ret['qvec'], ret['tvec'])
+      logs['loc'][qname] = {
+          'db': db_ids,
+          'best_cluster': best_cluster,
+          'log_clusters': logs_clusters,
+          'covisibility_clustering': covisibility_clustering,
+      }
+    else:
+      ret, log = pose_from_cluster(
+          localizer, qname, qcam, db_ids, features, matches)
+      if ret['success']:
+        poses[qname] = (ret['qvec'], ret['tvec'])
+      else:
+        closest = reference_sfm.images[db_ids[0]]
+        poses[qname] = (closest.qvec, closest.tvec)
+      log['covisibility_clustering'] = covisibility_clustering
+      logs['loc'][qname] = log
+
+  logger.info(f'Localized {len(poses)} / {len(queries)} images.')
+  logger.info(f'Writing poses to {results}...')
+  with open(results, 'w') as f:
+    for q in poses:
+      qvec, tvec = poses[q]
+      qvec = ' '.join(map(str, qvec))
+      tvec = ' '.join(map(str, tvec))
+      name = q.split('/')[-1]
+      if prepend_camera_name:
+        name = q.split('/')[-2] + '/' + name
+      f.write(f'{name} {qvec} {tvec}\n')
+
+  logs_path = f'{results}_logs.pkl'
+  logger.info(f'Writing logs to {logs_path}...')
+  with open(logs_path, 'wb') as f:
+    pickle.dump(logs, f)
+  logger.info('Done!')
+
+
+if __name__ == '__main__':
+  parser = argparse.ArgumentParser()
+  parser.add_argument('--reference_sfm', type=Path, required=True)
+  parser.add_argument('--queries', type=Path, required=True)
+  parser.add_argument('--features', type=Path, required=True)
+  parser.add_argument('--matches', type=Path, required=True)
+  parser.add_argument('--retrieval', type=Path, required=True)
+  parser.add_argument('--results', type=Path, required=True)
+  parser.add_argument('--ransac_thresh', type=float, default=12.0)
+  parser.add_argument('--covisibility_clustering', action='store_true')
+  parser.add_argument('--prepend_camera_name', action='store_true')
+  args = parser.parse_args()
+  main(**args.__dict__)
diff -Naur '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=pipelines' '--exclude=extractors' '--exclude=matchers' /tmp/hloc-latest/hloc/match_dense.py hloc/match_dense.py
--- /tmp/hloc-latest/hloc/match_dense.py	2026-01-16 16:13:56.813844948 -0700
+++ hloc/match_dense.py	2026-01-16 16:16:08.558985177 -0700
@@ -1,72 +1,123 @@
-import argparse
+# SPDX-FileCopyrightText: (C) 2025 Intel Corporation
+# SPDX-License-Identifier: Apache-2.0
+
+from typing import Optional, List, Tuple, Union, Sequence
+from pathlib import Path
 import pprint
-from collections import Counter, defaultdict
+import argparse
+from collections import defaultdict, Counter
 from itertools import chain
-from pathlib import Path
-from types import SimpleNamespace
-from typing import Dict, Iterable, List, Optional, Set, Tuple, Union
+from concurrent.futures import Executor, ThreadPoolExecutor
+import threading
 
-import h5py
+from tqdm import tqdm
 import numpy as np
+import h5py
 import torch
+from addict import Dict
 import torchvision.transforms.functional as F
 from scipy.spatial import KDTree
-from tqdm import tqdm
 
-from . import logger, matchers
-from .extract_features import read_image, resize_image
+from . import matchers, logger
+from .utils.base_model import cached_load
+from .utils.parsers import parse_retrieval, names_to_pair
 from .match_features import find_unique_new_pairs
-from .utils.base_model import dynamic_load
-from .utils.io import list_h5_names
-from .utils.parsers import names_to_pair, parse_retrieval
-
-# Default usage:
-# dense_conf = confs['loftr']
-# features, matches = main(dense_conf, pairs, images, export_dir=outputs)
-
-# Use SuperPoint keypoints as anchors:
-# feature_conf = extract_features.confs['superpoint_aachen']
-# features_sp = extract_features.main(feature_conf, images)
-# features, matches = main(dense_conf, pairs, images,
-#                           export_dir=outputs,
-#                          features_ref=features_sp)
-
-# Localization:
-# loc_features, loc_matches = main(matcher_conf, loc_pairs,
-#      images, export_dir=outputs, features_ref=features, max_kps=None)
+from .extract_features import resize_image
+from .utils.io import list_h5_names, read_image, base64_to_image
 
 confs = {
-    # Best quality but loads of points. Only use for small scenes
+    "qta_indoor": {
+        "output": "matches-qta",
+        "model": {
+            "name": "qta_loftr",
+            "weights": "indoor",
+            "match_coarse": {"thr": 0.1},
+            "coarse": {"topks": [64, 16, 16]},
+        },
+        "preprocessing": {"grayscale": True, "resize_max": 832, "dfactor": 32},
+        "max_error": 1,  # max error for assigned keypoints (in px)
+        "cell_size": 1,  # size of quantization patch
+        "init_ref_score": 10.0,
+        "top_k": False,
+        "batch_size": 1,
+    },
+    "qta_indoor_fast": {
+        "output": "matches-qta",
+        "model": {
+            "name": "qta_loftr",
+            "weights": "indoor",
+            "match_threshold": 0.2,
+            "topks": [16, 8, 8],
+        },
+        "preprocessing": {"grayscale": True, "resize_max": 1024, "dfactor": 32},
+        "max_error": 0,  # max error for assigned keypoints (in px)
+        "cell_size": 0,  # size of quantization patch
+        "init_ref_score": 10.0,
+        "top_k": False,
+        "batch_size": 1,
+    },
+    "qta_outdoor": {
+        "output": "matches-qta",
+        "model": {
+            "name": "qta_loftr",
+            "weights": "outdoor",
+            "match_threshold": 0.2,
+            "topks": [32, 16, 16],
+        },
+        "preprocessing": {"grayscale": True, "resize_max": 1024, "dfactor": 32},
+        "max_error": 0,  # max error for assigned keypoints (in px)
+        "cell_size": 0,  # size of quantization patch
+        "init_ref_score": 10.0,
+        "top_k": False,
+        "batch_size": 1,
+    },
     "loftr": {
         "output": "matches-loftr",
         "model": {"name": "loftr", "weights": "outdoor"},
         "preprocessing": {"grayscale": True, "resize_max": 1024, "dfactor": 8},
         "max_error": 1,  # max error for assigned keypoints (in px)
-        "cell_size": 1,  # size of quantization patch (max 1 kp/patch)
+        "cell_size": 1,  # size of quantization patch
+        "init_ref_score": 10.0,
+        "batch_size": 1,
     },
-    # Semi-scalable loftr which limits detected keypoints
     "loftr_aachen": {
         "output": "matches-loftr_aachen",
         "model": {"name": "loftr", "weights": "outdoor"},
         "preprocessing": {"grayscale": True, "resize_max": 1024, "dfactor": 8},
         "max_error": 2,  # max error for assigned keypoints (in px)
-        "cell_size": 8,  # size of quantization patch (max 1 kp/patch)
+        "cell_size": 8,  # size of quantization patch
+        "init_ref_score": 10.0,
+        "batch_size": 1,
     },
-    # Use for matching superpoint feats with loftr
-    "loftr_superpoint": {
-        "output": "matches-loftr_aachen",
-        "model": {"name": "loftr", "weights": "outdoor"},
+    "loftr_inloc": {
+        "output": "matches-loftr_inloc",
+        "model": {"name": "loftr", "weights": "indoor"},
         "preprocessing": {"grayscale": True, "resize_max": 1024, "dfactor": 8},
-        "max_error": 4,  # max error for assigned keypoints (in px)
-        "cell_size": 4,  # size of quantization patch (max 1 kp/patch)
+        "max_error": 0,  # max error for assigned keypoints (in px)
+        "cell_size": 0,  # size of quantization patch
+        "init_ref_score": 10.0,
+        "top_k": False,
+        "batch_size": 1,
     },
 }
 
 
-def to_cpts(kpts, ps):
-    if ps > 0.0:
-        kpts = np.round(np.round((kpts + 0.5) / ps) * ps - 0.5, 2)
-    return [tuple(cpt) for cpt in kpts]
+def add_keypoints(
+    kpts: np.ndarray,
+    other_cpts: np.ndarray,
+    update: bool = False,
+    # ref_bins: List = [],
+    # scores: np.ndarray = [],
+):
+  if update:
+    kpt_ids = np.arange(
+        len(other_cpts), len(other_cpts) + len(kpts), dtype=np.int32
+    )
+  else:
+    kpt_ids = np.arange(len(kpts), dtype=np.int32)
+  other_cpts.append(kpts)
+  # ref_bins.append(scores)
+  return kpt_ids
 
 
 def assign_keypoints(
@@ -78,460 +129,457 @@
     scores: Optional[np.ndarray] = None,
     cell_size: Optional[int] = None,
 ):
-    if not update:
-        # Without update this is just a NN search
-        if len(other_cpts) == 0 or len(kpts) == 0:
-            return np.full(len(kpts), -1)
-        dist, kpt_ids = KDTree(np.array(other_cpts)).query(kpts)
-        valid = dist <= max_error
-        kpt_ids[~valid] = -1
-        return kpt_ids
-    else:
-        ps = cell_size if cell_size is not None else max_error
-        ps = max(ps, max_error)
-        # With update we quantize and bin (optionally)
-        assert isinstance(other_cpts, list)
-        kpt_ids = []
-        cpts = to_cpts(kpts, ps)
-        bpts = to_cpts(kpts, int(max_error))
-        cp_to_id = {val: i for i, val in enumerate(other_cpts)}
-        for i, (cpt, bpt) in enumerate(zip(cpts, bpts)):
-            try:
-                kid = cp_to_id[cpt]
-            except KeyError:
-                kid = len(cp_to_id)
-                cp_to_id[cpt] = kid
-                other_cpts.append(cpt)
-                if ref_bins is not None:
-                    ref_bins.append(Counter())
-            if ref_bins is not None:
-                score = scores[i] if scores is not None else 1
-                ref_bins[cp_to_id[cpt]][bpt] += score
-            kpt_ids.append(kid)
-        return np.array(kpt_ids)
+  if max_error == 0 or cell_size == 0:
+    return add_keypoints(kpts, other_cpts, update)
+  if not update:
+    # Without update this is just a NN search
+    dist, kpt_ids = KDTree(np.array(other_cpts)).query(kpts)
+    valid = dist <= max_error
+    kpt_ids[~valid] = -1
+    return kpt_ids
+  else:
+    ps = cell_size if cell_size is not None else max_error
+    ps = max(cell_size, max_error)
+    # With update we quantize and bin (optionally)
+    assert isinstance(other_cpts, list)
+    kpt_ids = []
+    cpts = to_cpts(kpts, ps)
+    bpts = to_cpts(kpts, max_error)
+    cp_to_id = {val: i for i, val in enumerate(other_cpts)}
+    for i, (cpt, bpt) in enumerate(zip(cpts, bpts)):
+      try:
+        kid = cp_to_id[cpt]
+      except KeyError:
+        kid = len(cp_to_id)
+        cp_to_id[cpt] = kid
+        other_cpts.append(cpt)
+        if ref_bins is not None:
+          ref_bins.append(Counter())
+      if ref_bins is not None:
+        score = scores[i] if scores is not None else 1
+        ref_bins[cp_to_id[cpt]][bpt] += score
+      kpt_ids.append(kid)
+    return np.array(kpt_ids)
 
 
 def get_grouped_ids(array):
-    # Group array indices based on its values
-    # all duplicates are grouped as a set
-    idx_sort = np.argsort(array)
-    sorted_array = array[idx_sort]
-    _, ids, _ = np.unique(sorted_array, return_counts=True, return_index=True)
-    res = np.split(idx_sort, ids[1:])
-    return res
+  # Group array indices based on its values
+  # all duplicates are grouped as a set
+  idx_sort = np.argsort(array)
+  sorted_array = array[idx_sort]
+  _, ids, _ = np.unique(sorted_array, return_counts=True, return_index=True)
+  res = np.split(idx_sort, ids[1:])
+  return res
 
 
 def get_unique_matches(match_ids, scores):
-    if len(match_ids.shape) == 1:
-        return [0]
+  if len(match_ids.shape) == 1:
+    return [0]
 
-    isets1 = get_grouped_ids(match_ids[:, 0])
-    isets2 = get_grouped_ids(match_ids[:, 1])
-    uid1s = [ids[scores[ids].argmax()] for ids in isets1 if len(ids) > 0]
-    uid2s = [ids[scores[ids].argmax()] for ids in isets2 if len(ids) > 0]
-    uids = list(set(uid1s).intersection(uid2s))
-    return match_ids[uids], scores[uids]
+  isets1 = get_grouped_ids(match_ids[:, 0])
+  isets2 = get_grouped_ids(match_ids[:, 0])
+  uid1s = [ids[scores[ids].argmax()] for ids in isets1 if len(ids) > 0]
+  uid2s = [ids[scores[ids].argmax()] for ids in isets2 if len(ids) > 0]
+  uids = list(set(uid1s).intersection(uid2s))
+  return match_ids[uids], scores[uids]
+
+
+def to_cpts(kpts, ps):
+  cpts = np.round(np.round((kpts + 0.5) / ps) * ps - 0.5, 2)
+  return [tuple(cpt) for cpt in cpts]
 
 
 def matches_to_matches0(matches, scores):
-    if len(matches) == 0:
-        return np.zeros(0, dtype=np.int32), np.zeros(0, dtype=np.float16)
-    n_kps0 = np.max(matches[:, 0]) + 1
-    matches0 = -np.ones((n_kps0,))
-    scores0 = np.zeros((n_kps0,))
-    matches0[matches[:, 0]] = matches[:, 1]
-    scores0[matches[:, 0]] = scores
-    return matches0.astype(np.int32), scores0.astype(np.float16)
+  if matches.shape[0] == 0:
+    return (np.zeros([0, 2], dtype=np.uint32), np.zeros([0], dtype=np.float32))
+  n_kps0 = np.max(matches[:, 0]) + 1
+  matches0 = -np.ones((n_kps0,))
+  scores0 = np.zeros((n_kps0,))
+  matches0[matches[:, 0]] = matches[:, 1]
+  scores0[matches[:, 0]] = scores
+  return matches0.astype(np.int32), scores0.astype(np.float16)
 
 
 def kpids_to_matches0(kpt_ids0, kpt_ids1, scores):
-    valid = (kpt_ids0 != -1) & (kpt_ids1 != -1)
-    matches = np.dstack([kpt_ids0[valid], kpt_ids1[valid]])
-    matches = matches.reshape(-1, 2)
-    scores = scores[valid]
-
-    # Remove n-to-1 matches
-    matches, scores = get_unique_matches(matches, scores)
-    return matches_to_matches0(matches, scores)
+  valid = (kpt_ids0 != -1) & (kpt_ids1 != -1)
+  matches = np.dstack([kpt_ids0[valid], kpt_ids1[valid]])
+  matches = matches.reshape(-1, 2)
+  scores = scores[valid]
+
+  # Remove n-to-1 matches
+  matches, scores = get_unique_matches(matches, scores)
+  return matches_to_matches0(matches, scores)
 
 
 def scale_keypoints(kpts, scale):
-    if np.any(scale != 1.0):
-        kpts *= kpts.new_tensor(scale)
-    return kpts
+  if torch.any(scale != 1.0):
+    kpts *= scale
+  return kpts
 
 
-class ImagePairDataset(torch.utils.data.Dataset):
-    default_conf = {
-        "grayscale": True,
-        "resize_max": 1024,
-        "dfactor": 8,
-        "cache_images": False,
-    }
-
-    def __init__(self, image_dir, conf, pairs):
-        self.image_dir = image_dir
-        self.conf = conf = SimpleNamespace(**{**self.default_conf, **conf})
-        self.pairs = pairs
-        if self.conf.cache_images:
-            image_names = set(sum(pairs, ()))  # unique image names in pairs
-            logger.info(f"Loading and caching {len(image_names)} unique images.")
-            self.images = {}
-            self.scales = {}
-            for name in tqdm(image_names):
-                image = read_image(self.image_dir / name, self.conf.grayscale)
-                self.images[name], self.scales[name] = self.preprocess(image)
-
-    def preprocess(self, image: np.ndarray):
-        image = image.astype(np.float32, copy=False)
-        size = image.shape[:2][::-1]
-        scale = np.array([1.0, 1.0])
-
-        if self.conf.resize_max:
-            scale = self.conf.resize_max / max(size)
-            if scale < 1.0:
-                size_new = tuple(int(round(x * scale)) for x in size)
-                image = resize_image(image, size_new, "cv2_area")
-                scale = np.array(size) / np.array(size_new)
-
-        if self.conf.grayscale:
-            assert image.ndim == 2, image.shape
-            image = image[None]
-        else:
-            image = image.transpose((2, 0, 1))  # HxWxC to CxHxW
-        image = torch.from_numpy(image / 255.0).float()
+class SimpleExecutor(Executor):
+  """Simple executor that runs the function in the main thread."""
 
-        # assure that the size is divisible by dfactor
-        size_new = tuple(
-            map(
-                lambda x: int(x // self.conf.dfactor * self.conf.dfactor),
-                image.shape[-2:],
-            )
-        )
-        image = F.resize(image, size=size_new)
-        scale = np.array(size) / np.array(size_new)[::-1]
-        return image, scale
+  def __init__(self, max_workers=0):
+    pass
 
-    def __len__(self):
-        return len(self.pairs)
+  def submit(self, fn, *args, **kwargs):
+    return fn(*args, **kwargs)
 
-    def __getitem__(self, idx):
-        name0, name1 = self.pairs[idx]
-        if self.conf.cache_images:
-            image0, scale0 = self.images[name0], self.scales[name0]
-            image1, scale1 = self.images[name1], self.scales[name1]
-        else:
-            image0 = read_image(self.image_dir / name0, self.conf.grayscale)
-            image1 = read_image(self.image_dir / name1, self.conf.grayscale)
-            image0, scale0 = self.preprocess(image0)
-            image1, scale1 = self.preprocess(image1)
-        return image0, image1, scale0, scale1, name0, name1
+  def map(self, fn, *iterables, timeout=None, chunksize=1):
+    return map(fn, *iterables)
 
+  def shutdown(self, wait=True):
+    pass
 
-@torch.no_grad()
-def match_dense(
-    conf: Dict,
-    pairs: List[Tuple[str, str]],
-    image_dir: Path,
-    match_path: Path,  # out
-    existing_refs: Optional[List] = [],
-):
-    device = "cuda" if torch.cuda.is_available() else "cpu"
-    Model = dynamic_load(matchers, conf["model"]["name"])
-    model = Model(conf["model"]).eval().to(device)
-
-    dataset = ImagePairDataset(image_dir, conf["preprocessing"], pairs)
-    loader = torch.utils.data.DataLoader(
-        dataset, num_workers=16, batch_size=1, shuffle=False
-    )
-
-    logger.info("Performing dense matching...")
-    with h5py.File(str(match_path), "a") as fd:
-        for data in tqdm(loader, smoothing=0.1):
-            # load image-pair data
-            image0, image1, scale0, scale1, (name0,), (name1,) = data
-            scale0, scale1 = scale0[0].numpy(), scale1[0].numpy()
-            image0, image1 = image0.to(device), image1.to(device)
-
-            # match semi-dense
-            # for consistency with pairs_from_*: refine kpts of image0
-            if name0 in existing_refs:
-                # special case: flip to enable refinement in query image
-                pred = model({"image0": image1, "image1": image0})
-                pred = {
-                    **pred,
-                    "keypoints0": pred["keypoints1"],
-                    "keypoints1": pred["keypoints0"],
-                }
-            else:
-                # usual case
-                pred = model({"image0": image0, "image1": image1})
-
-            # Rescale keypoints and move to cpu
-            kpts0, kpts1 = pred["keypoints0"], pred["keypoints1"]
-            kpts0 = scale_keypoints(kpts0 + 0.5, scale0) - 0.5
-            kpts1 = scale_keypoints(kpts1 + 0.5, scale1) - 0.5
-            kpts0 = kpts0.cpu().numpy()
-            kpts1 = kpts1.cpu().numpy()
-            scores = pred["scores"].cpu().numpy()
-
-            # Write matches and matching scores in hloc format
-            pair = names_to_pair(name0, name1)
-            if pair in fd:
-                del fd[pair]
-            grp = fd.create_group(pair)
-
-            # Write dense matching output
-            grp.create_dataset("keypoints0", data=kpts0)
-            grp.create_dataset("keypoints1", data=kpts1)
-            grp.create_dataset("scores", data=scores)
-    del model, loader
-
-
-# default: quantize all!
-def load_keypoints(
-    conf: Dict, feature_paths_refs: List[Path], quantize: Optional[set] = None
-):
-    name2ref = {
-        n: i for i, p in enumerate(feature_paths_refs) for n in list_h5_names(p)
-    }
-
-    existing_refs = set(name2ref.keys())
-    if quantize is None:
-        quantize = existing_refs  # quantize all
-    if len(existing_refs) > 0:
-        logger.info(f"Loading keypoints from {len(existing_refs)} images.")
-
-    # Load query keypoints
-    cpdict = defaultdict(list)
-    bindict = defaultdict(list)
-    for name in existing_refs:
-        with h5py.File(str(feature_paths_refs[name2ref[name]]), "r") as fd:
-            kps = fd[name]["keypoints"].__array__()
-            if name not in quantize:
-                cpdict[name] = kps
-            else:
-                if "scores" in fd[name].keys():
-                    kp_scores = fd[name]["scores"].__array__()
-                else:
-                    # we set the score to 1.0 if not provided
-                    # increase for more weight on reference keypoints for
-                    # stronger anchoring
-                    kp_scores = [1.0 for _ in range(kps.shape[0])]
-                # bin existing keypoints of reference images for association
-                assign_keypoints(
-                    kps,
-                    cpdict[name],
-                    conf["max_error"],
-                    True,
-                    bindict[name],
-                    kp_scores,
-                    conf["cell_size"],
-                )
-    return cpdict, bindict
+  def __enter__(self):
+    return self
 
+  def __exit__(self, *args):
+    self.shutdown()
 
-def aggregate_matches(
-    conf: Dict,
-    pairs: List[Tuple[str, str]],
-    match_path: Path,
-    feature_path: Path,
-    required_queries: Optional[Set[str]] = None,
-    max_kps: Optional[int] = None,
-    cpdict: Dict[str, Iterable] = defaultdict(list),
-    bindict: Dict[str, List[Counter]] = defaultdict(list),
-):
-    if required_queries is None:
-        required_queries = set(sum(pairs, ()))
-        # default: do not overwrite existing features in feature_path!
-        required_queries -= set(list_h5_names(feature_path))
-
-    # if an entry in cpdict is provided as np.ndarray we assume it is fixed
-    required_queries -= set([k for k, v in cpdict.items() if isinstance(v, np.ndarray)])
-
-    # sort pairs for reduced RAM
-    pairs_per_q = Counter(list(chain(*pairs)))
-    pairs_score = [min(pairs_per_q[i], pairs_per_q[j]) for i, j in pairs]
-    pairs = [p for _, p in sorted(zip(pairs_score, pairs))]
-
-    if len(required_queries) > 0:
-        logger.info(f"Aggregating keypoints for {len(required_queries)} images.")
-    n_kps = 0
-    with h5py.File(str(match_path), "a") as fd:
-        for name0, name1 in tqdm(pairs, smoothing=0.1):
-            pair = names_to_pair(name0, name1)
-            grp = fd[pair]
-            kpts0 = grp["keypoints0"].__array__()
-            kpts1 = grp["keypoints1"].__array__()
-            scores = grp["scores"].__array__()
-
-            # Aggregate local features
-            update0 = name0 in required_queries
-            update1 = name1 in required_queries
-
-            # in localization we do not want to bin the query kp
-            # assumes that the query is name0!
-            if update0 and not update1 and max_kps is None:
-                max_error0 = cell_size0 = 0.0
-            else:
-                max_error0 = conf["max_error"]
-                cell_size0 = conf["cell_size"]
-
-            # Get match ids and extend query keypoints (cpdict)
-            mkp_ids0 = assign_keypoints(
-                kpts0,
-                cpdict[name0],
-                max_error0,
-                update0,
-                bindict[name0],
-                scores,
-                cell_size0,
-            )
-            mkp_ids1 = assign_keypoints(
-                kpts1,
-                cpdict[name1],
-                conf["max_error"],
-                update1,
-                bindict[name1],
-                scores,
-                conf["cell_size"],
-            )
 
-            # Build matches from assignments
-            matches0, scores0 = kpids_to_matches0(mkp_ids0, mkp_ids1, scores)
+class ImagePairDataset(torch.utils.data.Dataset):
+  """Dataset and dataloader class for image pairs. Features:
+  - multi-threaded data loading.
+  - batch-size
+  - prefetch
+  """
+
+  default_conf = {
+      "grayscale": True,
+      "resize_max": 1024,
+      "dfactor": 8,
+      "prefetch_factor": 2,  # number of prefetched batches
+      "num_workers": 1,
+  }
+
+  def __init__(
+      self,
+      image_dir: Union[Path, Tuple[Path, Path]],
+      conf: Dict,
+      pairs: Sequence,
+      batch_size: int = 1,
+      image_cache: Dict = None,
+  ):
+    self.image_dir = (
+        (image_dir, image_dir) if isinstance(image_dir, Path) else image_dir
+    )
+    self.conf = self.default_conf | Dict(conf)
+    self.pairs = pairs
+    self.batch_size = batch_size
+    self.cache_size = (1 + self.conf.prefetch_factor) * batch_size * 2
+    self.imsize = None
+    self.n_keep = 0
+    self.images = {}
+    self.images_cv = threading.Condition()
+    if self.conf.num_workers > 0:
+      self.pool = ThreadPoolExecutor(
+          max_workers=self.conf.num_workers,
+          thread_name_prefix=self.__class__.__name__,
+      )
+    else:
+      self.pool = SimpleExecutor()
+    # preprocess first dataset image to get image size
+    self.preprocess(self.pairs[0][1], None, self.image_dir[1] / self.pairs[0][1])
+    if image_cache is not None:  # SceneScape message
+      self.n_keep = len(image_cache)
+      for name, b64im in image_cache.items():
+        self.pool.submit(self.preprocess, name, b64im)
+
+  def preprocess(self, name: str, b64im=None, filepath: Path = None):
+    """Reading, decoding and pre-processing runs in worker threads."""
+    if filepath is not None:
+      image = read_image(filepath, self.conf.grayscale)
+    else:
+      image = base64_to_image(b64im, self.conf.grayscale, log_as=name)
+    image = image.astype(np.float32, copy=False)
+    size = image.shape[:2][::-1]
+    scale = np.array([1.0, 1.0])
+    size_new = np.array(size)
 
-            assert kpts0.shape[0] == scores.shape[0]
-            grp.create_dataset("matches0", data=matches0)
-            grp.create_dataset("matching_scores0", data=scores0)
-
-            # Convert bins to kps if finished, and store them
-            for name in (name0, name1):
-                pairs_per_q[name] -= 1
-                if pairs_per_q[name] > 0 or name not in required_queries:
-                    continue
-                kp_score = [c.most_common(1)[0][1] for c in bindict[name]]
-                cpdict[name] = [c.most_common(1)[0][0] for c in bindict[name]]
-                cpdict[name] = np.array(cpdict[name], dtype=np.float32)
-
-                # Select top-k query kps by score (reassign matches later)
-                if max_kps:
-                    top_k = min(max_kps, cpdict[name].shape[0])
-                    top_k = np.argsort(kp_score)[::-1][:top_k]
-                    cpdict[name] = cpdict[name][top_k]
-                    kp_score = np.array(kp_score)[top_k]
-
-                # Write query keypoints
-                with h5py.File(feature_path, "a") as kfd:
-                    if name in kfd:
-                        del kfd[name]
-                    kgrp = kfd.create_group(name)
-                    kgrp.create_dataset("keypoints", data=cpdict[name])
-                    kgrp.create_dataset("score", data=kp_score)
-                    n_kps += cpdict[name].shape[0]
-                del bindict[name]
-
-    if len(required_queries) > 0:
-        avg_kp_per_image = round(n_kps / len(required_queries), 1)
-        logger.info(
-            f"Finished assignment, found {avg_kp_per_image} "
-            f"keypoints/image (avg.), total {n_kps}."
-        )
-    return cpdict
+    if self.imsize is not None:
+      size_new = self.imsize
+    else:
+      if self.conf.resize_max < max(size):
+        scale = self.conf.resize_max / max(size)
+        size_new = tuple(int(round(x * scale)) for x in size)
+      # assure that the size is divisible by dfactor
+      size_new = tuple(
+          int(x // self.conf.dfactor * self.conf.dfactor) for x in size_new
+      )
+    scale = torch.Tensor(size) / torch.Tensor(size_new)
+    image = resize_image(image, size_new, "cv2_area")
+
+    if self.conf.grayscale:
+      assert image.ndim == 2, image.shape
+      image = image[None, None]
+    else:
+      image = image.transpose((2, 0, 1))[None]  # HxWxC to CxHxW
+    image = torch.from_numpy(image / 255.0).float()
 
+    with self.images_cv:
+      if self.imsize is None:
+        self.imsize = size_new
+      self.images[name] = (image, scale[None])
+      self.images_cv.notify_all()
+
+  def __len__(self):
+    return (len(self.pairs) + self.batch_size - 1) // self.batch_size
+
+  def __getitem__(self, b_idx):
+    if b_idx >= len(self):
+      raise IndexError()
+    images0, images1, scales0, scales1, names0, names1 = ([] for _ in range(6))
+    stop = min(
+        len(self.pairs), (b_idx + 1 + self.conf.prefetch_factor) * self.batch_size
+    )
+    with self.images_cv:
+      for idx in range(b_idx * self.batch_size, stop):
+        name0, name1 = self.pairs[idx]
+        for i, name in enumerate((name0, name1)):
+          if name not in self.images and self.image_dir[i] is not None:
+            self.pool.submit(
+                self.preprocess, name, None, self.image_dir[i] / name
+            )
 
-def assign_matches(
-    pairs: List[Tuple[str, str]],
-    match_path: Path,
-    keypoints: Union[List[Path], Dict[str, np.array]],
-    max_error: float,
-):
-    if isinstance(keypoints, list):
-        keypoints = load_keypoints({}, keypoints, kpts_as_bin=set([]))
-    assert len(set(sum(pairs, ())) - set(keypoints.keys())) == 0
-    with h5py.File(str(match_path), "a") as fd:
-        for name0, name1 in tqdm(pairs):
-            pair = names_to_pair(name0, name1)
-            grp = fd[pair]
-            kpts0 = grp["keypoints0"].__array__()
-            kpts1 = grp["keypoints1"].__array__()
-            scores = grp["scores"].__array__()
-
-            # NN search across cell boundaries
-            mkp_ids0 = assign_keypoints(kpts0, keypoints[name0], max_error)
-            mkp_ids1 = assign_keypoints(kpts1, keypoints[name1], max_error)
-
-            matches0, scores0 = kpids_to_matches0(mkp_ids0, mkp_ids1, scores)
-
-            # overwrite matches0 and matching_scores0
-            del grp["matches0"], grp["matching_scores0"]
-            grp.create_dataset("matches0", data=matches0)
-            grp.create_dataset("matching_scores0", data=scores0)
+      for idx in range(
+          b_idx * self.batch_size,
+          min(len(self.pairs), (b_idx + 1) * self.batch_size),
+      ):
+        name0, name1 = self.pairs[idx]
+        names0.append(name0)
+        names1.append(name1)
+        if name0 not in self.images:
+          self.images_cv.wait_for(lambda: name0 in self.images)
+        ims = self.images[name0]
+        images0.append(ims[0])
+        scales0.append(ims[1])
+        if name1 not in self.images:
+          self.images_cv.wait_for(lambda: name1 in self.images)
+        ims = self.images[name1]
+        images1.append(ims[0])
+        scales1.append(ims[1])
+
+      if len(self.images) > self.cache_size:
+        remove_names = tuple(self.images.keys())[
+            self.n_keep: self.n_keep + self.cache_size
+        ]
+        for name in remove_names:
+          self.images.pop(name)
+
+    if self.batch_size == 1:
+      return images0[0], images1[0], scales0[0], scales1[0], names0, names1
+    return (  # torch.cat copies data, even for single elements
+        torch.cat(images0),
+        torch.cat(images1),
+        torch.cat(scales0),
+        torch.cat(scales1),
+        names0,
+        names1,
+    )
 
 
 @torch.no_grad()
-def match_and_assign(
+def match_dense_from_paths(
     conf: Dict,
     pairs_path: Path,
-    image_dir: Path,
+    image_dir: Union[Path, Tuple[Path, Path]],
     match_path: Path,  # out
     feature_path_q: Path,  # out
-    feature_paths_refs: Optional[List[Path]] = [],
-    max_kps: Optional[int] = 8192,
+    feature_paths_refs: Optional[List[Path]] = (),
+    # use reassign to reduce quant error (not in loc)
+    reassign: Union[bool, float] = True,
+    max_kps: Optional[int] = None,
     overwrite: bool = False,
+    image_cache: Dict = None,
 ) -> Path:
-    for path in feature_paths_refs:
-        if not path.exists():
-            raise FileNotFoundError(f"Reference feature file {path}.")
-    pairs = parse_retrieval(pairs_path)
-    pairs = [(q, r) for q, rs in pairs.items() for r in rs]
-    pairs = find_unique_new_pairs(pairs, None if overwrite else match_path)
-    required_queries = set(chain.from_iterable(pairs))
-
-    name2ref = {
-        n: i for i, p in enumerate(feature_paths_refs) for n in list_h5_names(p)
-    }
-    existing_refs = required_queries.intersection(set(name2ref.keys()))
-
-    # images which require feature extraction
-    required_queries = required_queries - existing_refs
-
-    if feature_path_q.exists():
-        existing_queries = set(list_h5_names(feature_path_q))
-        feature_paths_refs.append(feature_path_q)
-        existing_refs = set.union(existing_refs, existing_queries)
-        if not overwrite:
-            required_queries = required_queries - existing_queries
-
-    if len(pairs) == 0 and len(required_queries) == 0:
-        logger.info("All pairs exist. Skipping dense matching.")
-        return
-
-    # extract semi-dense matches
-    match_dense(conf, pairs, image_dir, match_path, existing_refs=existing_refs)
-
-    logger.info("Assigning matches...")
-
-    # Pre-load existing keypoints
-    cpdict, bindict = load_keypoints(
-        conf, feature_paths_refs, quantize=required_queries
-    )
+  conf = {"psize": 1} | Dict(conf)
+  pairs = parse_retrieval(pairs_path)
+  pairs = [(q, r) for q, rs in pairs.items() for r in rs]
+  pairs = find_unique_new_pairs(pairs, None if overwrite else match_path)
+  required_queries = set(sum(pairs, ()))
+
+  name2ref = {
+      n: i for i, p in enumerate(feature_paths_refs) for n in list_h5_names(p)
+  }
+  existing_refs = required_queries.intersection(set(name2ref.keys()))
+  required_queries = required_queries - existing_refs
+
+  if feature_path_q.exists() and not overwrite:
+    feature_paths_refs += (feature_path_q,)
+    existing_refs = set.union(existing_refs, list_h5_names(feature_path_q))
+    q_name2ref = {n: -1 for n in list_h5_names(feature_path_q)}
+    name2ref = {**name2ref, **q_name2ref}
+
+  if len(pairs) == 0 and len(required_queries) == 0:
+    logger.info("All pairs exist. Skipping dense matching.")
+    return
+
+  model = cached_load(matchers, conf["model"])
+
+  # Load query keypoins
+  cpdict = defaultdict(list)
+  bindict = defaultdict(list)
+
+  if len(existing_refs) > 0:
+    logger.info(f"Pre-loaded keypoints from {len(existing_refs)} images.")
+  for name in existing_refs:
+    with h5py.File(str(feature_paths_refs[name2ref[name]]), "r") as fd:
+      kps = fd[name]["keypoints"].__array__()
+      if name not in required_queries:
+        cpdict[name] = kps
+      else:
+        if "scores" in fd[name].keys():
+          kp_scores = fd[name]["scores"].__array__()
+        else:
+          kp_scores = [conf.init_ref_score for _ in range(kps.shape[0])]
+        assign_keypoints(
+            kps,
+            cpdict[name],
+            conf.max_error,
+            True,
+            bindict[name],
+            kp_scores,
+            conf.cell_size,
+        )
+
+  # sort pairs for reduced RAM
+  pairs_per_q = Counter(list(chain(*pairs)))
+  pairs_score = [min(pairs_per_q[i], pairs_per_q[j]) for i, j in pairs]
+  pairs = [p for _, p in sorted(zip(pairs_score, pairs))]
+
+  dataset = ImagePairDataset(
+      image_dir, conf.preprocessing, pairs, conf.batch_size, image_cache
+  )
+  logger.info(f"Performing dense matching for {len(dataset)} image pairs...")
+  n_kps = 0
+  with h5py.File(str(match_path), "a") as fd:
+    for data in tqdm(dataset, smoothing=0.1):
+      # load image-pair data
+      images0, images1, scales0, scales1, names0, names1 = data
+      images0 = images0.to(cached_load.device)
+      images1 = images1.to(cached_load.device)
+      scales0 = scales0.to(cached_load.device)
+      scales1 = scales1.to(cached_load.device)
+
+      # match semi-dense
+      pred = model({"image0": images0, "image1": images1})
+
+      for kpts0, kpts1, scores, scale0, scale1, name0, name1 in zip(
+          pred.keypoints0,
+          pred.keypoints1,
+          pred.scores,
+          scales0,
+          scales1,
+          names0,
+          names1,
+      ):
+        # Rescale keypoints and move to cpu
+        kpts0 = scale_keypoints(kpts0 + 0.5, scale0) - 0.5
+        kpts1 = scale_keypoints(kpts1 + 0.5, scale1) - 0.5
+        kpts0 = kpts0.cpu().numpy()
+        kpts1 = kpts1.cpu().numpy()
+        scores = scores.cpu().numpy()
+
+        # Aggregate local features
+        update0 = name0 in required_queries
+        update1 = name1 in required_queries
+        kpt_ids0 = assign_keypoints(
+            kpts0,
+            cpdict[name0],
+            conf.max_error,
+            update0,
+            bindict[name0],
+            scores,
+            conf.cell_size,
+        )
+        kpt_ids1 = assign_keypoints(
+            kpts1,
+            cpdict[name1],
+            conf.max_error,
+            update1,
+            bindict[name1],
+            scores,
+            conf.cell_size,
+        )
+
+        # Build matches from assignments
+        matches0, scores0 = kpids_to_matches0(kpt_ids0, kpt_ids1, scores)
 
-    # Reassign matches by aggregation
-    cpdict = aggregate_matches(
-        conf,
-        pairs,
-        match_path,
-        feature_path=feature_path_q,
-        required_queries=required_queries,
-        max_kps=max_kps,
-        cpdict=cpdict,
-        bindict=bindict,
+        # Write matches and matching scores in hloc format
+        pair = names_to_pair(name0, name1)
+        if pair in fd:
+          del fd[pair]
+        grp = fd.create_group(pair)
+        assert kpts0.shape[0] == scores.shape[0]
+
+        grp.create_dataset("matches0", data=matches0)
+        grp.create_dataset("matching_scores0", data=scores0)
+
+        # Write dense matching output
+        grp.create_dataset("keypoints0", data=kpts0)
+        grp.create_dataset("keypoints1", data=kpts1)
+        grp.create_dataset("scores", data=scores)
+
+        # Convert bins to kps if finished, and store them
+        for name in (name0, name1):
+          pairs_per_q[name] -= 1
+          if pairs_per_q[name] > 0 or name not in required_queries:
+            continue
+          if conf.cell_size == 0 or conf.max_error == 0:
+            kp_score = bindict[name]
+            cpdict[name] = np.vstack(cpdict[name])
+          else:
+            kp_score = [c.most_common(1)[0][1] for c in bindict[name]]
+            cpdict[name] = [c.most_common(1)[0][0] for c in bindict[name]]
+            cpdict[name] = np.array(cpdict[name], dtype=np.float32)
+          if max_kps:
+            top_k = min(max_kps, cpdict[name].shape[0])
+            top_k = np.argsort(kp_score)[::-1][:top_k]
+            cpdict[name] = cpdict[name][top_k]
+            kp_score = np.array(kp_score)[top_k]
+          with h5py.File(feature_path_q, "a") as kfd:
+            if name in kfd:
+              del kfd[name]
+            kgrp = kfd.create_group(name)
+            kgrp.create_dataset("keypoints", data=cpdict[name])
+            kgrp.create_dataset("score", data=kp_score)
+            n_kps += cpdict[name].shape[0]
+          del bindict[name]
+
+  if len(required_queries) > 0:
+    avg_kp_per_image = round(n_kps / len(required_queries), 1)
+    logger.info(
+        f"Finished assignment, found {avg_kp_per_image} "
+        f"keypoints/image (avg.), total {n_kps}."
     )
 
-    # Invalidate matches that are far from selected bin by reassignment
-    if max_kps is not None:
-        logger.info(f'Reassign matches with max_error={conf["max_error"]}.')
-        assign_matches(pairs, match_path, cpdict, max_error=conf["max_error"])
+  # Invalidate matches that are far from selected bin by reassignment
+  if reassign or conf.top_k:
+    max_error = conf.max_error
+    if not isinstance(reassign, bool):
+      max_error = reassign
+    logger.info(f"Reassign matches with max_error={max_error}.")
+    with h5py.File(str(match_path), "a") as fd:
+      for name0, name1 in tqdm(pairs):
+        pair = names_to_pair(name0, name1)
+        grp = fd[pair]
+        kpts0 = grp["keypoints0"].__array__()
+        kpts1 = grp["keypoints1"].__array__()
+        scores = grp["scores"].__array__()
+        if len(scores) == 0:
+          continue
+
+        kpids0 = assign_keypoints(kpts0, cpdict[name0], max_error)
+        kpids1 = assign_keypoints(kpts1, cpdict[name1], max_error)
+        matches0, scores0 = kpids_to_matches0(kpids0, kpids1, scores)
+
+        del grp["matches0"], grp["matching_scores0"]
+
+        # overwrite matches0 and matching_scores0
+        grp.create_dataset("matches0", data=matches0)
+        grp.create_dataset("matching_scores0", data=scores0)
 
 
 @torch.no_grad()
@@ -542,65 +590,73 @@
     export_dir: Optional[Path] = None,
     matches: Optional[Path] = None,  # out
     features: Optional[Path] = None,  # out
-    features_ref: Optional[Path] = None,
-    max_kps: Optional[int] = 8192,
+    features_ref: Union[Path, Sequence[Path]] = (),
+    reassign: Union[bool, float] = True,
     overwrite: bool = False,
+    image_cache: Dict = None,
 ) -> Path:
-    logger.info(
-        "Extracting semi-dense features with configuration:" f"\n{pprint.pformat(conf)}"
-    )
+  """
+  Args:
+      image_cache (Dict): Mapping of image name to base64 encoded jpeg
+          compressed image data.
+  """
+  logger.info(
+      "Extracting semi-dense features with configuration:\n%s", pprint.pformat(conf)
+  )
+
+  if features is None:
+    features = "feats_"
+
+  if isinstance(features, Path):
+    features_q = features
+    if matches is None:
+      raise ValueError(
+          "Either provide both features and matches as Path or both as names."
+      )
+  else:
+    if export_dir is None:
+      raise ValueError(
+          "Provide an export_dir if features and matches"
+          f" are not file paths: {features}, {matches}."
+      )
+    features_q = Path(export_dir, f'{features}_{conf["output"]}_.h5')
+    if matches is None:
+      matches = Path(export_dir, f'{conf["output"]}_{pairs.stem}.h5')
+
+  if isinstance(features_ref, Path):
+    features_ref = (features_ref,)
+
+  match_dense_from_paths(
+      conf,
+      pairs,
+      image_dir,
+      matches,
+      features_q,
+      feature_paths_refs=features_ref,
+      reassign=reassign,
+      overwrite=overwrite,
+      image_cache=image_cache,
+  )
 
-    if features is None:
-        features = "feats_"
-
-    if isinstance(features, Path):
-        features_q = features
-        if matches is None:
-            raise ValueError(
-                "Either provide both features and matches as Path" " or both as names."
-            )
-    else:
-        if export_dir is None:
-            raise ValueError(
-                "Provide an export_dir if features and matches"
-                f" are not file paths: {features}, {matches}."
-            )
-        features_q = Path(export_dir, f'{features}{conf["output"]}.h5')
-        if matches is None:
-            matches = Path(export_dir, f'{conf["output"]}_{pairs.stem}.h5')
-
-    if features_ref is None:
-        features_ref = []
-    elif isinstance(features_ref, list):
-        features_ref = list(features_ref)
-    elif isinstance(features_ref, Path):
-        features_ref = [features_ref]
-    else:
-        raise TypeError(str(features_ref))
-
-    match_and_assign(
-        conf, pairs, image_dir, matches, features_q, features_ref, max_kps, overwrite
-    )
-
-    return features_q, matches
+  return features_q, matches
 
 
 if __name__ == "__main__":
-    parser = argparse.ArgumentParser()
-    parser.add_argument("--pairs", type=Path, required=True)
-    parser.add_argument("--image_dir", type=Path, required=True)
-    parser.add_argument("--export_dir", type=Path, required=True)
-    parser.add_argument("--matches", type=Path, default=confs["loftr"]["output"])
-    parser.add_argument(
-        "--features", type=str, default="feats_" + confs["loftr"]["output"]
-    )
-    parser.add_argument("--conf", type=str, default="loftr", choices=list(confs.keys()))
-    args = parser.parse_args()
-    main(
-        confs[args.conf],
-        args.pairs,
-        args.image_dir,
-        args.export_dir,
-        args.matches,
-        args.features,
-    )
+  parser = argparse.ArgumentParser()
+  parser.add_argument("--pairs", type=Path, required=True)
+  parser.add_argument("--image_dir", type=Path, required=True)
+  parser.add_argument("--export_dir", type=Path, required=True)
+  parser.add_argument("--matches", type=Path, default=confs["loftr"]["output"])
+  parser.add_argument(
+      "--features", type=str, default="feats_" + confs["loftr"]["output"]
+  )
+  parser.add_argument("--conf", type=str, default="loftr", choices=list(confs.keys()))
+  args = parser.parse_args()
+  main(
+      confs[args.conf],
+      args.pairs,
+      args.image_dir,
+      args.export_dir,
+      args.matches,
+      args.features,
+  )
diff -Naur '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=pipelines' '--exclude=extractors' '--exclude=matchers' /tmp/hloc-latest/hloc/match_features.py hloc/match_features.py
--- /tmp/hloc-latest/hloc/match_features.py	2026-01-16 16:13:56.813844948 -0700
+++ hloc/match_features.py	2026-01-16 16:16:08.543810916 -0700
@@ -1,18 +1,20 @@
+# SPDX-FileCopyrightText: (C) 2025 Intel Corporation
+# SPDX-License-Identifier: Apache-2.0
+
 import argparse
-import pprint
-from functools import partial
+from typing import Union, Optional, Dict, List, Tuple
 from pathlib import Path
-from queue import Queue
-from threading import Thread
-from typing import Dict, List, Optional, Tuple, Union
-
+import pprint
+import collections.abc as collections
+from tqdm import tqdm
 import h5py
 import torch
-from tqdm import tqdm
 
-from . import logger, matchers
+from . import matchers, logger
 from .utils.base_model import dynamic_load
 from .utils.parsers import names_to_pair, names_to_pair_old, parse_retrieval
+from .utils.io import list_h5_names
+
 
 """
 A set of standard configurations that can be directly selected from the command
@@ -21,42 +23,17 @@
     - model: the model configuration, as passed to a feature matcher.
 """
 confs = {
-    "superpoint+lightglue": {
-        "output": "matches-superpoint-lightglue",
-        "model": {
-            "name": "lightglue",
-            "features": "superpoint",
-        },
-    },
-    "disk+lightglue": {
-        "output": "matches-disk-lightglue",
-        "model": {
-            "name": "lightglue",
-            "features": "disk",
-        },
-    },
-    "aliked+lightglue": {
-        "output": "matches-aliked-lightglue",
-        "model": {
-            "name": "lightglue",
-            "features": "aliked",
-        },
-    },
     "superglue": {
         "output": "matches-superglue",
-        "model": {
-            "name": "superglue",
-            "weights": "outdoor",
-            "sinkhorn_iterations": 50,
-        },
+        "model": {"name": "superglue", "weights": "outdoor", "sinkhorn_iterations": 50},
+    },
+    "superglue-indoor": {
+        "output": "matches-superglue-indoor",
+        "model": {"name": "superglue", "weights": "indoor", "sinkhorn_iterations": 50},
     },
     "superglue-fast": {
         "output": "matches-superglue-it5",
-        "model": {
-            "name": "superglue",
-            "weights": "outdoor",
-            "sinkhorn_iterations": 5,
-        },
+        "model": {"name": "superglue", "weights": "outdoor", "sinkhorn_iterations": 5},
     },
     "NN-superpoint": {
         "output": "matches-NN-mutual-dist.7",
@@ -76,80 +53,31 @@
     },
     "NN-mutual": {
         "output": "matches-NN-mutual",
-        "model": {
-            "name": "nearest_neighbor",
-            "do_mutual_check": True,
-        },
-    },
-    "adalam": {
-        "output": "matches-adalam",
-        "model": {"name": "adalam"},
+        "model": {"name": "nearest_neighbor", "do_mutual_check": True},
     },
 }
 
 
-class WorkQueue:
-    def __init__(self, work_fn, num_threads=1):
-        self.queue = Queue(num_threads)
-        self.threads = [
-            Thread(target=self.thread_fn, args=(work_fn,)) for _ in range(num_threads)
-        ]
-        for thread in self.threads:
-            thread.start()
-
-    def join(self):
-        for thread in self.threads:
-            self.queue.put(None)
-        for thread in self.threads:
-            thread.join()
-
-    def thread_fn(self, work_fn):
-        item = self.queue.get()
-        while item is not None:
-            work_fn(item)
-            item = self.queue.get()
-
-    def put(self, data):
-        self.queue.put(data)
-
-
-class FeaturePairsDataset(torch.utils.data.Dataset):
-    def __init__(self, pairs, feature_path_q, feature_path_r):
-        self.pairs = pairs
-        self.feature_path_q = feature_path_q
-        self.feature_path_r = feature_path_r
-
-    def __getitem__(self, idx):
-        name0, name1 = self.pairs[idx]
-        data = {}
-        with h5py.File(self.feature_path_q, "r") as fd:
-            grp = fd[name0]
-            for k, v in grp.items():
-                data[k + "0"] = torch.from_numpy(v.__array__()).float()
-            # some matchers might expect an image but only use its size
-            data["image0"] = torch.empty((1,) + tuple(grp["image_size"])[::-1])
-        with h5py.File(self.feature_path_r, "r") as fd:
-            grp = fd[name1]
-            for k, v in grp.items():
-                data[k + "1"] = torch.from_numpy(v.__array__()).float()
-            data["image1"] = torch.empty((1,) + tuple(grp["image_size"])[::-1])
-        return data
-
-    def __len__(self):
-        return len(self.pairs)
-
-
-def writer_fn(inp, match_path):
-    pair, pred = inp
-    with h5py.File(str(match_path), "a", libver="latest") as fd:
-        if pair in fd:
-            del fd[pair]
-        grp = fd.create_group(pair)
-        matches = pred["matches0"][0].cpu().short().numpy()
-        grp.create_dataset("matches0", data=matches)
-        if "matching_scores0" in pred:
-            scores = pred["matching_scores0"][0].cpu().half().numpy()
-            grp.create_dataset("matching_scores0", data=scores)
+# Reuse optimized model in repeat calls
+class MF:
+  device = "cuda" if torch.cuda.is_available() else "cpu"
+  conf = None
+  model = None
+
+  @classmethod
+  def get_optimized_model(cls, model_conf):
+    if cls.model is None or cls.conf != model_conf:
+      cls.conf = model_conf
+      Model = dynamic_load(matchers, model_conf["name"])
+      cls.model = Model(model_conf).eval().to(cls.device)
+      if cls.device == "cpu":
+        try:
+          import intel_extension_for_pytorch as ipex
+
+          cls.model = ipex.optimize(cls.model)
+        except ImportError:
+          pass
+    return cls.model
 
 
 def main(
@@ -161,49 +89,55 @@
     features_ref: Optional[Path] = None,
     overwrite: bool = False,
 ) -> Path:
-    if isinstance(features, Path) or Path(features).exists():
-        features_q = features
-        if matches is None:
-            raise ValueError(
-                "Either provide both features and matches as Path" " or both as names."
-            )
-    else:
-        if export_dir is None:
-            raise ValueError(
-                "Provide an export_dir if features is not" f" a file path: {features}."
-            )
-        features_q = Path(export_dir, features + ".h5")
-        if matches is None:
-            matches = Path(export_dir, f'{features}_{conf["output"]}_{pairs.stem}.h5')
-
-    if features_ref is None:
-        features_ref = features_q
-    match_from_paths(conf, pairs, matches, features_q, features_ref, overwrite)
 
-    return matches
+  if isinstance(features, Path) or Path(features).exists():
+    features_q = features
+    if matches is None:
+      raise ValueError(
+          "Either provide both features and matches as Path" " or both as names."
+      )
+  else:
+    if export_dir is None:
+      raise ValueError(
+          "Provide an export_dir if features is not" f" a file path: {features}."
+      )
+    features_q = Path(export_dir, features + ".h5")
+    if matches is None:
+      matches = Path(export_dir, f'{features}_{conf["output"]}_{pairs.stem}.h5')
+
+  if features_ref is None:
+    features_ref = features_q
+  if isinstance(features_ref, collections.Iterable):
+    features_ref = list(features_ref)
+  else:
+    features_ref = [features_ref]
+
+  match_from_paths(conf, pairs, matches, features_q, features_ref, overwrite)
+
+  return matches
 
 
 def find_unique_new_pairs(pairs_all: List[Tuple[str]], match_path: Path = None):
-    """Avoid to recompute duplicates to save time."""
-    pairs = set()
-    for i, j in pairs_all:
-        if (j, i) not in pairs:
-            pairs.add((i, j))
-    pairs = list(pairs)
-    if match_path is not None and match_path.exists():
-        with h5py.File(str(match_path), "r", libver="latest") as fd:
-            pairs_filtered = []
-            for i, j in pairs:
-                if (
-                    names_to_pair(i, j) in fd
-                    or names_to_pair(j, i) in fd
-                    or names_to_pair_old(i, j) in fd
-                    or names_to_pair_old(j, i) in fd
-                ):
-                    continue
-                pairs_filtered.append((i, j))
-        return pairs_filtered
-    return pairs
+  """Avoid to recompute duplicates to save time."""
+  pairs = set()
+  for i, j in pairs_all:
+    if (j, i) not in pairs:
+      pairs.add((i, j))
+  pairs = list(pairs)
+  if match_path is not None and match_path.exists():
+    with h5py.File(str(match_path), 'r', libver='latest') as fd:
+      pairs_filtered = []
+      for i, j in pairs:
+        if (
+            names_to_pair(i, j) in fd
+            or names_to_pair(j, i) in fd
+            or names_to_pair_old(i, j) in fd
+            or names_to_pair_old(j, i) in fd
+        ):
+          continue
+        pairs_filtered.append((i, j))
+    return pairs_filtered
+  return pairs
 
 
 @torch.no_grad()
@@ -212,57 +146,72 @@
     pairs_path: Path,
     match_path: Path,
     feature_path_q: Path,
-    feature_path_ref: Path,
+    feature_paths_refs: Path,
     overwrite: bool = False,
 ) -> Path:
-    logger.info(
-        "Matching local features with configuration:" f"\n{pprint.pformat(conf)}"
-    )
-
-    if not feature_path_q.exists():
-        raise FileNotFoundError(f"Query feature file {feature_path_q}.")
-    if not feature_path_ref.exists():
-        raise FileNotFoundError(f"Reference feature file {feature_path_ref}.")
-    match_path.parent.mkdir(exist_ok=True, parents=True)
-
-    assert pairs_path.exists(), pairs_path
-    pairs = parse_retrieval(pairs_path)
-    pairs = [(q, r) for q, rs in pairs.items() for r in rs]
-    pairs = find_unique_new_pairs(pairs, None if overwrite else match_path)
-    if len(pairs) == 0:
-        logger.info("Skipping the matching.")
-        return
-
-    device = "cuda" if torch.cuda.is_available() else "cpu"
-    Model = dynamic_load(matchers, conf["model"]["name"])
-    model = Model(conf["model"]).eval().to(device)
-
-    dataset = FeaturePairsDataset(pairs, feature_path_q, feature_path_ref)
-    loader = torch.utils.data.DataLoader(
-        dataset, num_workers=5, batch_size=1, shuffle=False, pin_memory=True
-    )
-    writer_queue = WorkQueue(partial(writer_fn, match_path=match_path), 5)
-
-    for idx, data in enumerate(tqdm(loader, smoothing=0.1)):
-        data = {
-            k: v if k.startswith("image") else v.to(device, non_blocking=True)
-            for k, v in data.items()
-        }
-        pred = model(data)
-        pair = names_to_pair(*pairs[idx])
-        writer_queue.put((pair, pred))
-    writer_queue.join()
-    logger.info("Finished exporting matches.")
+  logger.info(
+      "Matching local features with configuration:" f"\n{pprint.pformat(conf)}"
+  )
+
+  if not feature_path_q.exists():
+    raise FileNotFoundError(f"Query feature file {feature_path_q}.")
+  for path in feature_paths_refs:
+    if not path.exists():
+      raise FileNotFoundError(f"Reference feature file {path}.")
+  name2ref = {
+      n: i for i, p in enumerate(feature_paths_refs) for n in list_h5_names(p)
+  }
+  match_path.parent.mkdir(exist_ok=True, parents=True)
+
+  assert pairs_path.exists(), pairs_path
+  pairs = parse_retrieval(pairs_path)
+  pairs = [(q, r) for q, rs in pairs.items() for r in rs]
+  pairs = find_unique_new_pairs(pairs, None if overwrite else match_path)
+  if len(pairs) == 0:
+    logger.info("Skipping the matching.")
+    return
+
+  model = MF.get_optimized_model(conf["model"])
+
+  for (name0, name1) in tqdm(pairs, smoothing=0.1):
+    data = {}
+    with h5py.File(str(feature_path_q), 'r', libver='latest') as fd:
+      grp = fd[name0]
+      for k, v in grp.items():
+        data[k + "0"] = torch.from_numpy(v.__array__()).float().to(MF.device)
+      # some matchers might expect an image but only use its size
+      data['image0'] = torch.empty((1,)+tuple(grp['image_size'])[::-1])
+    with h5py.File(str(feature_paths_refs[name2ref[name1]]), 'r', libver='latest') as fd:
+      grp = fd[name1]
+      for k, v in grp.items():
+        data[k + "1"] = torch.from_numpy(v.__array__()).float().to(MF.device)
+      data["image1"] = torch.empty((1,) + tuple(grp["image_size"])[::-1])
+    data = {k: v[None] for k, v in data.items()}
+
+    pred = model(data)
+    pair = names_to_pair(name0, name1)
+    with h5py.File(str(match_path), 'a', libver='latest') as fd:
+      if pair in fd:
+        del fd[pair]
+      grp = fd.create_group(pair)
+      matches = pred["matches0"][0].cpu().short().numpy()
+      grp.create_dataset("matches0", data=matches)
+
+      if "matching_scores0" in pred:
+        scores = pred["matching_scores0"][0].cpu().half().numpy()
+        grp.create_dataset("matching_scores0", data=scores)
+
+  logger.info("Finished exporting matches.")
 
 
 if __name__ == "__main__":
-    parser = argparse.ArgumentParser()
-    parser.add_argument("--pairs", type=Path, required=True)
-    parser.add_argument("--export_dir", type=Path)
-    parser.add_argument("--features", type=str, default="feats-superpoint-n4096-r1024")
-    parser.add_argument("--matches", type=Path)
-    parser.add_argument(
-        "--conf", type=str, default="superglue", choices=list(confs.keys())
-    )
-    args = parser.parse_args()
-    main(confs[args.conf], args.pairs, args.features, args.export_dir)
+  parser = argparse.ArgumentParser()
+  parser.add_argument("--pairs", type=Path, required=True)
+  parser.add_argument("--export_dir", type=Path)
+  parser.add_argument("--features", type=str, default="feats-superpoint-n4096-r1024")
+  parser.add_argument("--matches", type=Path)
+  parser.add_argument(
+      "--conf", type=str, default="superglue", choices=list(confs.keys())
+  )
+  args = parser.parse_args()
+  main(confs[args.conf], args.pairs, args.features, args.export_dir)
diff -Naur '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=pipelines' '--exclude=extractors' '--exclude=matchers' /tmp/hloc-latest/hloc/pairs_from_covisibility.py hloc/pairs_from_covisibility.py
--- /tmp/hloc-latest/hloc/pairs_from_covisibility.py	2026-01-16 16:13:56.814320260 -0700
+++ hloc/pairs_from_covisibility.py	2026-01-16 16:16:08.543986602 -0700
@@ -1,60 +1,62 @@
+# SPDX-FileCopyrightText: (C) 2025 Intel Corporation
+# SPDX-License-Identifier: Apache-2.0
+
 import argparse
-from collections import defaultdict
 from pathlib import Path
-
 import numpy as np
 from tqdm import tqdm
+from collections import defaultdict
 
 from . import logger
 from .utils.read_write_model import read_model
 
 
 def main(model, output, num_matched):
-    logger.info("Reading the COLMAP model...")
-    cameras, images, points3D = read_model(model)
+  logger.info('Reading the COLMAP model...')
+  cameras, images, points3D = read_model(model)
 
-    logger.info("Extracting image pairs from covisibility info...")
-    pairs = []
-    for image_id, image in tqdm(images.items()):
-        matched = image.point3D_ids != -1
-        points3D_covis = image.point3D_ids[matched]
-
-        covis = defaultdict(int)
-        for point_id in points3D_covis:
-            for image_covis_id in points3D[point_id].image_ids:
-                if image_covis_id != image_id:
-                    covis[image_covis_id] += 1
-
-        if len(covis) == 0:
-            logger.info(f"Image {image_id} does not have any covisibility.")
-            continue
-
-        covis_ids = np.array(list(covis.keys()))
-        covis_num = np.array([covis[i] for i in covis_ids])
-
-        if len(covis_ids) <= num_matched:
-            top_covis_ids = covis_ids[np.argsort(-covis_num)]
-        else:
-            # get covisible image ids with top k number of common matches
-            ind_top = np.argpartition(covis_num, -num_matched)
-            ind_top = ind_top[-num_matched:]  # unsorted top k
-            ind_top = ind_top[np.argsort(-covis_num[ind_top])]
-            top_covis_ids = [covis_ids[i] for i in ind_top]
-            assert covis_num[ind_top[0]] == np.max(covis_num)
-
-        for i in top_covis_ids:
-            pair = (image.name, images[i].name)
-            pairs.append(pair)
-
-    logger.info(f"Found {len(pairs)} pairs.")
-    with open(output, "w") as f:
-        f.write("\n".join(" ".join([i, j]) for i, j in pairs))
+  logger.info('Extracting image pairs from covisibility info...')
+  pairs = []
+  for image_id, image in tqdm(images.items()):
+    matched = image.point3D_ids != -1
+    points3D_covis = image.point3D_ids[matched]
+
+    covis = defaultdict(int)
+    for point_id in points3D_covis:
+      for image_covis_id in points3D[point_id].image_ids:
+        if image_covis_id != image_id:
+          covis[image_covis_id] += 1
+
+    if len(covis) == 0:
+      logger.info(f'Image {image_id} does not have any covisibility.')
+      continue
+
+    covis_ids = np.array(list(covis.keys()))
+    covis_num = np.array([covis[i] for i in covis_ids])
+
+    if len(covis_ids) <= num_matched:
+      top_covis_ids = covis_ids[np.argsort(-covis_num)]
+    else:
+      # get covisible image ids with top k number of common matches
+      ind_top = np.argpartition(covis_num, -num_matched)
+      ind_top = ind_top[-num_matched:]  # unsorted top k
+      ind_top = ind_top[np.argsort(-covis_num[ind_top])]
+      top_covis_ids = [covis_ids[i] for i in ind_top]
+      assert covis_num[ind_top[0]] == np.max(covis_num)
+
+    for i in top_covis_ids:
+      pair = (image.name, images[i].name)
+      pairs.append(pair)
+
+  logger.info(f'Found {len(pairs)} pairs.')
+  with open(output, 'w') as f:
+    f.write('\n'.join(' '.join([i, j]) for i, j in pairs))
 
 
 if __name__ == "__main__":
-    parser = argparse.ArgumentParser()
-    parser.add_argument("--model", required=True, type=Path)
-    parser.add_argument("--output", required=True, type=Path)
-    parser.add_argument("--num_matched", required=True, type=int)
-    args = parser.parse_args()
-    main(**args.__dict__)
+  parser = argparse.ArgumentParser()
+  parser.add_argument('--model', required=True, type=Path)
+  parser.add_argument('--output', required=True, type=Path)
+  parser.add_argument('--num_matched', required=True, type=int)
+  args = parser.parse_args()
+  main(**args.__dict__)
diff -Naur '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=pipelines' '--exclude=extractors' '--exclude=matchers' /tmp/hloc-latest/hloc/pairs_from_exhaustive.py hloc/pairs_from_exhaustive.py
--- /tmp/hloc-latest/hloc/pairs_from_exhaustive.py	2026-01-16 16:13:56.814320260 -0700
+++ hloc/pairs_from_exhaustive.py	2026-01-16 16:16:08.544144137 -0700
@@ -1,64 +1,68 @@
+# SPDX-FileCopyrightText: (C) 2025 Intel Corporation
+# SPDX-License-Identifier: Apache-2.0
+
 import argparse
 import collections.abc as collections
 from pathlib import Path
-from typing import List, Optional, Union
+from typing import Optional, Union, List
 
 from . import logger
-from .utils.io import list_h5_names
 from .utils.parsers import parse_image_lists
+from .utils.io import list_h5_names
 
 
 def main(
-    output: Path,
-    image_list: Optional[Union[Path, List[str]]] = None,
-    features: Optional[Path] = None,
-    ref_list: Optional[Union[Path, List[str]]] = None,
-    ref_features: Optional[Path] = None,
-):
-    if image_list is not None:
-        if isinstance(image_list, (str, Path)):
-            names_q = parse_image_lists(image_list)
-        elif isinstance(image_list, collections.Iterable):
-            names_q = list(image_list)
-        else:
-            raise ValueError(f"Unknown type for image list: {image_list}")
-    elif features is not None:
-        names_q = list_h5_names(features)
+        output: Path,
+        image_list: Optional[Union[Path, List[str]]] = None,
+        features: Optional[Path] = None,
+        ref_list: Optional[Union[Path, List[str]]] = None,
+        ref_features: Optional[Path] = None):
+
+  if image_list is not None:
+    if isinstance(image_list, (str, Path)):
+      names_q = parse_image_lists(image_list)
+    elif isinstance(image_list, collections.Iterable):
+      names_q = list(image_list)
     else:
-        raise ValueError("Provide either a list of images or a feature file.")
-
-    self_matching = False
-    if ref_list is not None:
-        if isinstance(ref_list, (str, Path)):
-            names_ref = parse_image_lists(ref_list)
-        elif isinstance(image_list, collections.Iterable):
-            names_ref = list(ref_list)
-        else:
-            raise ValueError(f"Unknown type for reference image list: {ref_list}")
-    elif ref_features is not None:
-        names_ref = list_h5_names(ref_features)
+      raise ValueError(f'Unknown type for image list: {image_list}')
+  elif features is not None:
+    names_q = list_h5_names(features)
+  else:
+    raise ValueError('Provide either a list of images or a feature file.')
+
+  self_matching = False
+  if ref_list is not None:
+    if isinstance(ref_list, (str, Path)):
+      names_ref = parse_image_lists(ref_list)
+    elif isinstance(image_list, collections.Iterable):
+      names_ref = list(ref_list)
     else:
-        self_matching = True
-        names_ref = names_q
-
-    pairs = []
-    for i, n1 in enumerate(names_q):
-        for j, n2 in enumerate(names_ref):
-            if self_matching and j <= i:
-                continue
-            pairs.append((n1, n2))
-
-    logger.info(f"Found {len(pairs)} pairs.")
-    with open(output, "w") as f:
-        f.write("\n".join(" ".join([i, j]) for i, j in pairs))
+      raise ValueError(
+          f'Unknown type for reference image list: {ref_list}')
+  elif ref_features is not None:
+    names_ref = list_h5_names(ref_features)
+  else:
+    self_matching = True
+    names_ref = names_q
+
+  pairs = []
+  for i, n1 in enumerate(names_q):
+    for j, n2 in enumerate(names_ref):
+      if self_matching and j <= i:
+        continue
+      pairs.append((n1, n2))
+
+  logger.info(f'Found {len(pairs)} pairs.')
+  with open(output, 'w') as f:
+    f.write('\n'.join(' '.join([i, j]) for i, j in pairs))
 
 
 if __name__ == "__main__":
-    parser = argparse.ArgumentParser()
-    parser.add_argument("--output", required=True, type=Path)
-    parser.add_argument("--image_list", type=Path)
-    parser.add_argument("--features", type=Path)
-    parser.add_argument("--ref_list", type=Path)
-    parser.add_argument("--ref_features", type=Path)
-    args = parser.parse_args()
-    main(**args.__dict__)
+  parser = argparse.ArgumentParser()
+  parser.add_argument('--output', required=True, type=Path)
+  parser.add_argument('--image_list', type=Path)
+  parser.add_argument('--features', type=Path)
+  parser.add_argument('--ref_list', type=Path)
+  parser.add_argument('--ref_features', type=Path)
+  args = parser.parse_args()
+  main(**args.__dict__)
diff -Naur '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=pipelines' '--exclude=extractors' '--exclude=matchers' /tmp/hloc-latest/hloc/pairs_from_poses.py hloc/pairs_from_poses.py
--- /tmp/hloc-latest/hloc/pairs_from_poses.py	2026-01-16 16:13:56.814320260 -0700
+++ hloc/pairs_from_poses.py	2026-01-16 16:16:08.545521437 -0700
@@ -1,68 +1,72 @@
+# SPDX-FileCopyrightText: (C) 2025 Intel Corporation
+# SPDX-License-Identifier: Apache-2.0
+
 import argparse
 from pathlib import Path
-
 import numpy as np
 import scipy.spatial
 
 from . import logger
-from .pairs_from_retrieval import pairs_from_score_matrix
 from .utils.read_write_model import read_images_binary
+from .pairs_from_retrieval import pairs_from_score_matrix
 
 DEFAULT_ROT_THRESH = 30  # in degrees
 
 
 def get_pairwise_distances(images):
-    ids = np.array(list(images.keys()))
-    Rs = []
-    ts = []
-    for id_ in ids:
-        image = images[id_]
-        R = image.qvec2rotmat()
-        t = image.tvec
-        Rs.append(R)
-        ts.append(t)
-    Rs = np.stack(Rs, 0)
-    ts = np.stack(ts, 0)
-
-    # Invert the poses from world-to-camera to camera-to-world.
-    Rs = Rs.transpose(0, 2, 1)
-    ts = -(Rs @ ts[:, :, None])[:, :, 0]
-
-    dist = scipy.spatial.distance.squareform(scipy.spatial.distance.pdist(ts))
-
-    # Instead of computing the angle between two camera orientations,
-    # we compute the angle between the principal axes, as two images rotated
-    # around their principal axis still observe the same scene.
-    axes = Rs[:, :, -1]
-    dots = np.einsum("mi,ni->mn", axes, axes, optimize=True)
-    dR = np.rad2deg(np.arccos(np.clip(dots, -1.0, 1.0)))
+  ids = np.array(list(images.keys()))
+  Rs = []
+  ts = []
+  for id_ in ids:
+    image = images[id_]
+    R = image.qvec2rotmat()
+    t = image.tvec
+    Rs.append(R)
+    ts.append(t)
+  Rs = np.stack(Rs, 0)
+  ts = np.stack(ts, 0)
+
+  # Invert the poses from world-to-camera to camera-to-world.
+  Rs = Rs.transpose(0, 2, 1)
+  ts = -(Rs @ ts[:, :, None])[:, :, 0]
+
+  dist = scipy.spatial.distance.squareform(scipy.spatial.distance.pdist(ts))
+
+  # Instead of computing the angle between two camera orientations,
+  # we compute the angle between the principal axes, as two images rotated
+  # around their principal axis still observe the same scene.
+  axes = Rs[:, :, -1]
+  dots = np.einsum('mi,ni->mn', axes, axes, optimize=True)
+  dR = np.rad2deg(np.arccos(np.clip(dots, -1., 1.)))
 
-    return ids, dist, dR
+  return ids, dist, dR
 
 
 def main(model, output, num_matched, rotation_threshold=DEFAULT_ROT_THRESH):
-    logger.info("Reading the COLMAP model...")
-    images = read_images_binary(model / "images.bin")
+  logger.info('Reading the COLMAP model...')
+  images = read_images_binary(model / 'images.bin')
 
-    logger.info(f"Obtaining pairwise distances between {len(images)} images...")
-    ids, dist, dR = get_pairwise_distances(images)
-    scores = -dist
-
-    invalid = dR >= rotation_threshold
-    np.fill_diagonal(invalid, True)
-    pairs = pairs_from_score_matrix(scores, invalid, num_matched)
-    pairs = [(images[ids[i]].name, images[ids[j]].name) for i, j in pairs]
-
-    logger.info(f"Found {len(pairs)} pairs.")
-    with open(output, "w") as f:
-        f.write("\n".join(" ".join(p) for p in pairs))
+  logger.info(
+      f'Obtaining pairwise distances between {len(images)} images...')
+  ids, dist, dR = get_pairwise_distances(images)
+  scores = -dist
+
+  invalid = (dR >= rotation_threshold)
+  np.fill_diagonal(invalid, True)
+  pairs = pairs_from_score_matrix(scores, invalid, num_matched)
+  pairs = [(images[ids[i]].name, images[ids[j]].name) for i, j in pairs]
+
+  logger.info(f'Found {len(pairs)} pairs.')
+  with open(output, 'w') as f:
+    f.write('\n'.join(' '.join(p) for p in pairs))
 
 
 if __name__ == "__main__":
-    parser = argparse.ArgumentParser()
-    parser.add_argument("--model", required=True, type=Path)
-    parser.add_argument("--output", required=True, type=Path)
-    parser.add_argument("--num_matched", required=True, type=int)
-    parser.add_argument("--rotation_threshold", default=DEFAULT_ROT_THRESH, type=float)
-    args = parser.parse_args()
-    main(**args.__dict__)
+  parser = argparse.ArgumentParser()
+  parser.add_argument('--model', required=True, type=Path)
+  parser.add_argument('--output', required=True, type=Path)
+  parser.add_argument('--num_matched', required=True, type=int)
+  parser.add_argument('--rotation_threshold',
+                      default=DEFAULT_ROT_THRESH, type=float)
+  args = parser.parse_args()
+  main(**args.__dict__)
diff -Naur '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=pipelines' '--exclude=extractors' '--exclude=matchers' /tmp/hloc-latest/hloc/pairs_from_retrieval.py hloc/pairs_from_retrieval.py
--- /tmp/hloc-latest/hloc/pairs_from_retrieval.py	2026-01-16 16:13:56.814320260 -0700
+++ hloc/pairs_from_retrieval.py	2026-01-16 16:16:08.545679224 -0700
@@ -1,133 +1,122 @@
+# SPDX-FileCopyrightText: (C) 2025 Intel Corporation
+# SPDX-License-Identifier: Apache-2.0
+
 import argparse
-import collections.abc as collections
 from pathlib import Path
 from typing import Optional
-
 import h5py
 import numpy as np
 import torch
+import collections.abc as collections
 
 from . import logger
-from .utils.io import list_h5_names
 from .utils.parsers import parse_image_lists
 from .utils.read_write_model import read_images_binary
+from .utils.io import list_h5_names
 
 
 def parse_names(prefix, names, names_all):
-    if prefix is not None:
-        if not isinstance(prefix, str):
-            prefix = tuple(prefix)
-        names = [n for n in names_all if n.startswith(prefix)]
-        if len(names) == 0:
-            raise ValueError(f"Could not find any image with the prefix `{prefix}`.")
-    elif names is not None:
-        if isinstance(names, (str, Path)):
-            names = parse_image_lists(names)
-        elif isinstance(names, collections.Iterable):
-            names = list(names)
-        else:
-            raise ValueError(
-                f"Unknown type of image list: {names}."
-                "Provide either a list or a path to a list file."
-            )
-    else:
-        names = names_all
-    return names
-
-
-def get_descriptors(names, path, name2idx=None, key="global_descriptor"):
-    if name2idx is None:
-        with h5py.File(str(path), "r", libver="latest") as fd:
-            desc = [fd[n][key].__array__() for n in names]
-    else:
-        desc = []
-        for n in names:
-            with h5py.File(str(path[name2idx[n]]), "r", libver="latest") as fd:
-                desc.append(fd[n][key].__array__())
-    return torch.from_numpy(np.stack(desc, 0)).float()
-
-
-def pairs_from_score_matrix(
-    scores: torch.Tensor,
-    invalid: np.array,
-    num_select: int,
-    min_score: Optional[float] = None,
-):
-    assert scores.shape == invalid.shape
-    if isinstance(scores, np.ndarray):
-        scores = torch.from_numpy(scores)
-    invalid = torch.from_numpy(invalid).to(scores.device)
-    if min_score is not None:
-        invalid |= scores < min_score
-    scores.masked_fill_(invalid, float("-inf"))
-
-    topk = torch.topk(scores, num_select, dim=1)
-    indices = topk.indices.cpu().numpy()
-    valid = topk.values.isfinite().cpu().numpy()
-
-    pairs = []
-    for i, j in zip(*np.where(valid)):
-        pairs.append((i, indices[i, j]))
-    return pairs
-
-
-def main(
-    descriptors,
-    output,
-    num_matched,
-    query_prefix=None,
-    query_list=None,
-    db_prefix=None,
-    db_list=None,
-    db_model=None,
-    db_descriptors=None,
-):
-    logger.info("Extracting image pairs from a retrieval database.")
-
-    # We handle multiple reference feature files.
-    # We only assume that names are unique among them and map names to files.
-    if db_descriptors is None:
-        db_descriptors = descriptors
-    if isinstance(db_descriptors, (Path, str)):
-        db_descriptors = [db_descriptors]
-    name2db = {n: i for i, p in enumerate(db_descriptors) for n in list_h5_names(p)}
-    db_names_h5 = list(name2db.keys())
-    query_names_h5 = list_h5_names(descriptors)
-
-    if db_model:
-        images = read_images_binary(db_model / "images.bin")
-        db_names = [i.name for i in images.values()]
+  if prefix is not None:
+    if not isinstance(prefix, str):
+      prefix = tuple(prefix)
+    names = [n for n in names_all if n.startswith(prefix)]
+  elif names is not None:
+    if isinstance(names, (str, Path)):
+      names = parse_image_lists(names)
+    elif isinstance(names, collections.Iterable):
+      names = list(names)
     else:
-        db_names = parse_names(db_prefix, db_list, db_names_h5)
-    if len(db_names) == 0:
-        raise ValueError("Could not find any database image.")
-    query_names = parse_names(query_prefix, query_list, query_names_h5)
-
-    device = "cuda" if torch.cuda.is_available() else "cpu"
-    db_desc = get_descriptors(db_names, db_descriptors, name2db)
-    query_desc = get_descriptors(query_names, descriptors)
-    sim = torch.einsum("id,jd->ij", query_desc.to(device), db_desc.to(device))
-
-    # Avoid self-matching
-    self = np.array(query_names)[:, None] == np.array(db_names)[None]
-    pairs = pairs_from_score_matrix(sim, self, num_matched, min_score=0)
-    pairs = [(query_names[i], db_names[j]) for i, j in pairs]
-
-    logger.info(f"Found {len(pairs)} pairs.")
-    with open(output, "w") as f:
-        f.write("\n".join(" ".join([i, j]) for i, j in pairs))
+      raise ValueError(f'Unknown type of image list: {names}.'
+                       'Provide either a list or a path to a list file.')
+  else:
+    names = names_all
+  return names
+
+
+def get_descriptors(names, path, name2idx=None, key='global_descriptor'):
+  if name2idx is None:
+    with h5py.File(str(path), 'r', libver='latest') as fd:
+      desc = [fd[n][key].__array__() for n in names]
+  else:
+    desc = []
+    for n in names:
+      with h5py.File(str(path[name2idx[n]]), 'r', libver='latest') as fd:
+        desc.append(fd[n][key].__array__())
+  return torch.from_numpy(np.stack(desc, 0)).float()
+
+
+def pairs_from_score_matrix(scores: torch.Tensor,
+                            invalid: np.array,
+                            num_select: int,
+                            min_score: Optional[float] = None):
+  assert scores.shape == invalid.shape
+  if isinstance(scores, np.ndarray):
+    scores = torch.from_numpy(scores)
+  invalid = torch.from_numpy(invalid).to(scores.device)
+  if min_score is not None:
+    invalid |= scores < min_score
+  scores.masked_fill_(invalid, float('-inf'))
+
+  topk = torch.topk(scores, num_select, dim=1)
+  indices = topk.indices.cpu().numpy()
+  valid = topk.values.isfinite().cpu().numpy()
+
+  pairs = []
+  for i, j in zip(*np.where(valid)):
+    pairs.append((i, indices[i, j]))
+  return pairs
+
+
+def main(descriptors, output, num_matched,
+         query_prefix=None, query_list=None,
+         db_prefix=None, db_list=None, db_model=None, db_descriptors=None):
+  logger.info('Extracting image pairs from a retrieval database.')
+
+  # We handle multiple reference feature files.
+  # We only assume that names are unique among them and map names to files.
+  if db_descriptors is None:
+    db_descriptors = descriptors
+  if isinstance(db_descriptors, (Path, str)):
+    db_descriptors = [db_descriptors]
+  name2db = {n: i for i, p in enumerate(db_descriptors)
+             for n in list_h5_names(p)}
+  db_names_h5 = list(name2db.keys())
+  query_names_h5 = list_h5_names(descriptors)
+
+  if db_model:
+    images = read_images_binary(db_model / 'images.bin')
+    db_names = [i.name for i in images.values()]
+  else:
+    db_names = parse_names(db_prefix, db_list, db_names_h5)
+  if len(db_names) == 0:
+    raise ValueError('Could not find any database image.')
+  query_names = parse_names(query_prefix, query_list, query_names_h5)
+
+  device = 'cuda' if torch.cuda.is_available() else 'cpu'
+  db_desc = get_descriptors(db_names, db_descriptors, name2db)
+  query_desc = get_descriptors(query_names, descriptors)
+  sim = torch.einsum('id,jd->ij', query_desc.to(device), db_desc.to(device))
+
+  # Avoid self-matching
+  self = np.array(query_names)[:, None] == np.array(db_names)[None]
+  pairs = pairs_from_score_matrix(sim, self, num_matched, min_score=0)
+  pairs = [(query_names[i], db_names[j]) for i, j in pairs]
+
+  logger.info(f'Found {len(pairs)} pairs.')
+  with open(output, 'w') as f:
+    f.write('\n'.join(' '.join([i, j]) for i, j in pairs))
 
 
 if __name__ == "__main__":
-    parser = argparse.ArgumentParser()
-    parser.add_argument("--descriptors", type=Path, required=True)
-    parser.add_argument("--output", type=Path, required=True)
-    parser.add_argument("--num_matched", type=int, required=True)
-    parser.add_argument("--query_prefix", type=str, nargs="+")
-    parser.add_argument("--query_list", type=Path)
-    parser.add_argument("--db_prefix", type=str, nargs="+")
-    parser.add_argument("--db_list", type=Path)
-    parser.add_argument("--db_model", type=Path)
-    parser.add_argument("--db_descriptors", type=Path)
-    args = parser.parse_args()
-    main(**args.__dict__)
+  parser = argparse.ArgumentParser()
+  parser.add_argument('--descriptors', type=Path, required=True)
+  parser.add_argument('--output', type=Path, required=True)
+  parser.add_argument('--num_matched', type=int, required=True)
+  parser.add_argument('--query_prefix', type=str, nargs='+')
+  parser.add_argument('--query_list', type=Path)
+  parser.add_argument('--db_prefix', type=str, nargs='+')
+  parser.add_argument('--db_list', type=Path)
+  parser.add_argument('--db_model', type=Path)
+  parser.add_argument('--db_descriptors', type=Path)
+  args = parser.parse_args()
+  main(**args.__dict__)
diff -Naur '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=pipelines' '--exclude=extractors' '--exclude=matchers' /tmp/hloc-latest/hloc/reconstruction.py hloc/reconstruction.py
--- /tmp/hloc-latest/hloc/reconstruction.py	2026-01-16 16:13:56.815082664 -0700
+++ hloc/reconstruction.py	2026-01-16 16:16:08.545856444 -0700
@@ -1,234 +1,169 @@
+# SPDX-FileCopyrightText: (C) 2025 Intel Corporation
+# SPDX-License-Identifier: Apache-2.0
+
 import argparse
-import multiprocessing
 import shutil
+from typing import Optional, List, Dict, Any
+import multiprocessing
 from pathlib import Path
-from typing import Any, Dict, List, Optional
-
 import pycolmap
-import tqdm
 
 from . import logger
+from .utils.database import COLMAPDatabase
 from .triangulation import (
-    OutputCapture,
-    estimation_and_geometric_verification,
-    import_features,
-    import_matches,
-    parse_option_args,
-)
+    import_features, import_matches, estimation_and_geometric_verification,
+    OutputCapture, parse_option_args)
 
 
 def create_empty_db(database_path: Path):
-    if database_path.exists():
-        logger.warning("The database already exists, deleting it.")
-        database_path.unlink()
-    logger.info("Creating an empty database...")
-    with pycolmap.Database.open(database_path) as _:
-        pass
-
-
-def import_images(
-    image_dir: Path,
-    database_path: Path,
-    camera_mode: pycolmap.CameraMode,
-    image_list: Optional[List[str]] = None,
-    options: Optional[Dict[str, Any]] = None,
-):
-    logger.info("Importing images into the database...")
-    if options is None:
-        options = {}
-    images = list(image_dir.iterdir())
-    if len(images) == 0:
-        raise IOError(f"No images found in {image_dir}.")
-    with pycolmap.ostream():
-        pycolmap.import_images(
-            database_path,
-            image_dir,
-            camera_mode,
-            image_names=image_list or [],
-            options=options,
-        )
+  if database_path.exists():
+    logger.warning("The database already exists, deleting it.")
+    database_path.unlink()
+  logger.info("Creating an empty database...")
+  db = COLMAPDatabase.connect(database_path)
+  db.create_tables()
+  db.commit()
+  db.close()
+
+
+def import_images(image_dir: Path,
+                  database_path: Path,
+                  camera_mode: pycolmap.CameraMode,
+                  image_list: Optional[List[str]] = None,
+                  options: Optional[Dict[str, Any]] = None):
+  logger.info('Importing images into the database...')
+  if options is None:
+    options = {}
+  images = list(image_dir.iterdir())
+  if len(images) == 0:
+    raise IOError(f"No images found in {image_dir}.")
+  with pycolmap.ostream():
+    pycolmap.import_images(database_path, image_dir, camera_mode,
+                           image_list=image_list or [],
+                           options=options)
 
 
 def get_image_ids(database_path: Path) -> Dict[str, int]:
-    images = {}
-    with pycolmap.Database.open(database_path) as db:
-        images = {image.name: image.image_id for image in db.read_all_images()}
-    return images
-
-
-def incremental_mapping(
-    database_path: Path,
-    image_dir: Path,
-    sfm_path: Path,
-    options: Optional[Dict[str, Any]] = None,
-) -> dict[int, pycolmap.Reconstruction]:
-    num_images = pycolmap.Database.open(database_path).num_images()
-    pbars = []
-
-    def restart_progress_bar():
-        if len(pbars) > 0:
-            pbars[-1].close()
-        pbars.append(
-            tqdm.tqdm(
-                total=num_images,
-                desc=f"Reconstruction {len(pbars)}",
-                unit="images",
-                postfix="registered",
-            )
-        )
-        pbars[-1].update(2)
-
-    reconstructions = pycolmap.incremental_mapping(
-        database_path,
-        image_dir,
-        sfm_path,
-        options=options or {},
-        initial_image_pair_callback=restart_progress_bar,
-        next_image_callback=lambda: pbars[-1].update(1),
-    )
-
-    return reconstructions
-
+  db = COLMAPDatabase.connect(database_path)
+  images = {}
+  for name, image_id in db.execute("SELECT name, image_id FROM images;"):
+    images[name] = image_id
+  db.close()
+  return images
+
+
+def run_reconstruction(sfm_dir: Path,
+                       database_path: Path,
+                       image_dir: Path,
+                       verbose: bool = False,
+                       options: Optional[Dict[str, Any]] = None,
+                       ) -> pycolmap.Reconstruction:
+  models_path = sfm_dir / 'models'
+  models_path.mkdir(exist_ok=True, parents=True)
+  logger.info('Running 3D reconstruction...')
+  if options is None:
+    options = {}
+  options = {'num_threads': min(multiprocessing.cpu_count(), 16), **options}
+  with OutputCapture(verbose):
+    with pycolmap.ostream():
+      reconstructions = pycolmap.incremental_mapping(
+          database_path, image_dir, models_path, options=options)
 
-def run_reconstruction(
-    sfm_dir: Path,
-    database_path: Path,
-    image_dir: Path,
-    verbose: bool = False,
-    options: Optional[Dict[str, Any]] = None,
-) -> pycolmap.Reconstruction:
-    models_path = sfm_dir / "models"
-    models_path.mkdir(exist_ok=True, parents=True)
-    logger.info("Running 3D reconstruction...")
-    if options is None:
-        options = {}
-    options = {"num_threads": min(multiprocessing.cpu_count(), 16), **options}
-
-    with OutputCapture(verbose):
-        reconstructions = incremental_mapping(
-            database_path, image_dir, models_path, options=options
-        )
-
-    if len(reconstructions) == 0:
-        logger.error("Could not reconstruct any model!")
-        return None
-    logger.info(f"Reconstructed {len(reconstructions)} model(s).")
-
-    largest_index = None
-    largest_num_images = 0
-    for index, rec in reconstructions.items():
-        num_images = rec.num_reg_images()
-        if num_images > largest_num_images:
-            largest_index = index
-            largest_num_images = num_images
-    assert largest_index is not None
+  if len(reconstructions) == 0:
+    logger.error("Could not reconstruct any model!")
+    return None
+  logger.info(f"Reconstructed {len(reconstructions)} model(s).")
+
+  largest_index = None
+  largest_num_images = 0
+  for index, rec in reconstructions.items():
+    num_images = rec.num_reg_images()
+    if num_images > largest_num_images:
+      largest_index = index
+      largest_num_images = num_images
+  assert largest_index is not None
+  logger.info(
+      f"Largest model is #{largest_index} " f"with {largest_num_images} images."
+  )
+
+  for filename in ["images.bin", "cameras.bin", "points3D.bin"]:
+    if (sfm_dir / filename).exists():
+      (sfm_dir / filename).unlink()
+    shutil.move(str(models_path / str(largest_index) / filename), str(sfm_dir))
+  return reconstructions[largest_index]
+
+
+def main(sfm_dir: Path,
+         image_dir: Path,
+         pairs: Path,
+         features: Path,
+         matches: Path,
+         camera_mode: pycolmap.CameraMode = pycolmap.CameraMode.AUTO,
+         verbose: bool = False,
+         skip_geometric_verification: bool = False,
+         min_match_score: Optional[float] = None,
+         image_list: Optional[List[str]] = None,
+         image_options: Optional[Dict[str, Any]] = None,
+         mapper_options: Optional[Dict[str, Any]] = None,
+         ) -> pycolmap.Reconstruction:
+
+  assert features.exists(), features
+  assert pairs.exists(), pairs
+  assert matches.exists(), matches
+
+  sfm_dir.mkdir(parents=True, exist_ok=True)
+  database = sfm_dir / "database.db"
+
+  create_empty_db(database)
+  import_images(image_dir, database, camera_mode, image_list, image_options)
+  image_ids = get_image_ids(database)
+  import_features(image_ids, database, features)
+  import_matches(
+      image_ids,
+      database,
+      pairs,
+      matches,
+      min_match_score,
+      skip_geometric_verification,
+  )
+  if not skip_geometric_verification:
+    estimation_and_geometric_verification(database, pairs, verbose)
+  reconstruction = run_reconstruction(
+      sfm_dir, database, image_dir, verbose, mapper_options)
+  if reconstruction is not None:
     logger.info(
-        f"Largest model is #{largest_index} " f"with {largest_num_images} images."
+        f"Reconstruction statistics:\n{reconstruction.summary()}"
+        + f"\n\tnum_input_images = {len(image_ids)}"
     )
-
-    for filename in [
-        "images.bin",
-        "cameras.bin",
-        "points3D.bin",
-        "frames.bin",
-        "rigs.bin",
-    ]:
-        if (sfm_dir / filename).exists():
-            (sfm_dir / filename).unlink()
-        shutil.move(str(models_path / str(largest_index) / filename), str(sfm_dir))
-    return reconstructions[largest_index]
-
-
-def main(
-    sfm_dir: Path,
-    image_dir: Path,
-    pairs: Path,
-    features: Path,
-    matches: Path,
-    camera_mode: pycolmap.CameraMode = pycolmap.CameraMode.AUTO,
-    verbose: bool = False,
-    skip_geometric_verification: bool = False,
-    min_match_score: Optional[float] = None,
-    image_list: Optional[List[str]] = None,
-    image_options: Optional[Dict[str, Any]] = None,
-    mapper_options: Optional[Dict[str, Any]] = None,
-) -> pycolmap.Reconstruction:
-    assert features.exists(), features
-    assert pairs.exists(), pairs
-    assert matches.exists(), matches
-
-    sfm_dir.mkdir(parents=True, exist_ok=True)
-    database = sfm_dir / "database.db"
-
-    logger.info(f"Writing COLMAP logs to {sfm_dir / 'colmap.LOG.*'}")
-    pycolmap.logging.set_log_destination(pycolmap.logging.INFO, sfm_dir / "colmap.LOG.")
-
-    create_empty_db(database)
-    import_images(image_dir, database, camera_mode, image_list, image_options)
-    image_ids = get_image_ids(database)
-    with pycolmap.Database.open(database) as db:
-        import_features(image_ids, db, features)
-        import_matches(
-            image_ids,
-            db,
-            pairs,
-            matches,
-            min_match_score,
-            skip_geometric_verification,
-        )
-    if not skip_geometric_verification:
-        estimation_and_geometric_verification(database, pairs, verbose)
-    reconstruction = run_reconstruction(
-        sfm_dir, database, image_dir, verbose, mapper_options
-    )
-    if reconstruction is not None:
-        logger.info(
-            f"Reconstruction statistics:\n{reconstruction.summary()}"
-            + f"\n\tnum_input_images = {len(image_ids)}"
-        )
-    return reconstruction
+  return reconstruction
 
 
 if __name__ == "__main__":
-    parser = argparse.ArgumentParser()
-    parser.add_argument("--sfm_dir", type=Path, required=True)
-    parser.add_argument("--image_dir", type=Path, required=True)
-
-    parser.add_argument("--pairs", type=Path, required=True)
-    parser.add_argument("--features", type=Path, required=True)
-    parser.add_argument("--matches", type=Path, required=True)
-
-    parser.add_argument(
-        "--camera_mode",
-        type=str,
-        default="AUTO",
-        choices=list(pycolmap.CameraMode.__members__.keys()),
-    )
-    parser.add_argument("--skip_geometric_verification", action="store_true")
-    parser.add_argument("--min_match_score", type=float)
-    parser.add_argument("--verbose", action="store_true")
-
-    parser.add_argument(
-        "--image_options",
-        nargs="+",
-        default=[],
-        help="List of key=value from {}".format(pycolmap.ImageReaderOptions().todict()),
-    )
-    parser.add_argument(
-        "--mapper_options",
-        nargs="+",
-        default=[],
-        help="List of key=value from {}".format(
-            pycolmap.IncrementalMapperOptions().todict()
-        ),
-    )
-    args = parser.parse_args().__dict__
-
-    image_options = parse_option_args(
-        args.pop("image_options"), pycolmap.ImageReaderOptions()
-    )
-    mapper_options = parse_option_args(
-        args.pop("mapper_options"), pycolmap.IncrementalMapperOptions()
-    )
+  parser = argparse.ArgumentParser()
+  parser.add_argument("--sfm_dir", type=Path, required=True)
+  parser.add_argument("--image_dir", type=Path, required=True)
+
+  parser.add_argument("--pairs", type=Path, required=True)
+  parser.add_argument("--features", type=Path, required=True)
+  parser.add_argument("--matches", type=Path, required=True)
+
+  parser.add_argument('--camera_mode', type=str, default="AUTO",
+                      choices=list(pycolmap.CameraMode.__members__.keys()))
+  parser.add_argument('--skip_geometric_verification', action='store_true')
+  parser.add_argument('--min_match_score', type=float)
+  parser.add_argument('--verbose', action='store_true')
+
+  parser.add_argument('--image_options', nargs='+', default=[],
+                      help='List of key=value from {}'.format(
+                          pycolmap.ImageReaderOptions().todict()))
+  parser.add_argument('--mapper_options', nargs='+', default=[],
+                      help='List of key=value from {}'.format(
+                          pycolmap.IncrementalMapperOptions().todict()))
+  args = parser.parse_args().__dict__
+
+  image_options = parse_option_args(
+      args.pop("image_options"), pycolmap.ImageReaderOptions())
+  mapper_options = parse_option_args(
+      args.pop("mapper_options"), pycolmap.IncrementalMapperOptions())
 
-    main(**args, image_options=image_options, mapper_options=mapper_options)
+  main(**args, image_options=image_options, mapper_options=mapper_options)
diff -Naur '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=pipelines' '--exclude=extractors' '--exclude=matchers' /tmp/hloc-latest/hloc/triangulation.py hloc/triangulation.py
--- /tmp/hloc-latest/hloc/triangulation.py	2026-01-16 16:13:56.815082664 -0700
+++ hloc/triangulation.py	2026-01-16 16:16:08.546033935 -0700
@@ -1,281 +1,274 @@
+# SPDX-FileCopyrightText: (C) 2025 Intel Corporation
+# SPDX-License-Identifier: Apache-2.0
+
 import argparse
+import contextlib
+from typing import Optional, List, Dict, Any
+import io
+import sys
 from pathlib import Path
-from typing import Any, Dict, List, Optional
-
 import numpy as np
-import pycolmap
 from tqdm import tqdm
+import pycolmap
 
 from . import logger
-from .utils.geometry import compute_epipolar_errors
+from .utils.database import COLMAPDatabase
 from .utils.io import get_keypoints, get_matches
 from .utils.parsers import parse_retrieval
+from .utils.geometry import compute_epipolar_errors
 
 
 class OutputCapture:
-    def __init__(self, verbose: bool):
-        self.verbose = verbose
+  def __init__(self, verbose: bool):
+    self.verbose = verbose
 
-    def __enter__(self):
-        if not self.verbose:
-            pycolmap.logging.alsologtostderr = False
-
-    def __exit__(self, exc_type, *args):
-        if not self.verbose:
-            pycolmap.logging.alsologtostderr = True
-
-
-def create_db_from_model(
-    reconstruction: pycolmap.Reconstruction, database_path: Path
-) -> Dict[str, int]:
-    if database_path.exists():
-        logger.warning("The database already exists, deleting it.")
-        database_path.unlink()
-
-    with pycolmap.Database.open(database_path) as db:
-        for camera_id, camera in reconstruction.cameras.items():
-            db.write_camera(camera, use_camera_id=True)
-        for rig_id, rig in reconstruction.rigs.items():
-            db.write_rig(rig, use_rig_id=True)
-        for frame_id, frame in reconstruction.frames.items():
-            db.write_frame(frame, use_frame_id=True)
-        for image_id, image in reconstruction.images.items():
-            db.write_image(image, use_image_id=True)
-    return {image.name: image_id for image_id, image in reconstruction.images.items()}
-
-
-def import_features(
-    image_ids: Dict[str, int], db: pycolmap.Database, features_path: Path
-):
-    logger.info("Importing features into the database...")
-    for image_name, image_id in tqdm(image_ids.items()):
-        keypoints = get_keypoints(features_path, image_name)
-        keypoints += 0.5  # COLMAP origin
-        db.write_keypoints(image_id, keypoints)
-
-
-def import_matches(
-    image_ids: Dict[str, int],
-    db: pycolmap.Database,
-    pairs_path: Path,
-    matches_path: Path,
-    min_match_score: Optional[float] = None,
-    skip_geometric_verification: bool = False,
-):
-    logger.info("Importing matches into the database...")
-
-    with open(str(pairs_path), "r") as f:
-        pairs = [p.split() for p in f.readlines()]
-
-    matched = set()
-    for name0, name1 in tqdm(pairs):
-        id0, id1 = image_ids[name0], image_ids[name1]
-        if len({(id0, id1), (id1, id0)} & matched) > 0:
-            continue
-        matches, scores = get_matches(matches_path, name0, name1)
-        if min_match_score:
-            matches = matches[scores > min_match_score]
-        db.write_matches(id0, id1, matches)
-        matched |= {(id0, id1), (id1, id0)}
-
-        if skip_geometric_verification:
-            db.write_two_view_geometry(
-                id0, id1, pycolmap.TwoViewGeometry(inlier_matches=matches)
-            )
-
-
-def estimation_and_geometric_verification(
-    database_path: Path, pairs_path: Path, verbose: bool = False
-):
-    logger.info("Performing geometric verification of the matches...")
-    with OutputCapture(verbose):
-        pycolmap.verify_matches(
-            database_path,
-            pairs_path,
-            options=dict(ransac=dict(max_num_trials=20000, min_inlier_ratio=0.1)),
-        )
-
-
-def geometric_verification(
-    image_ids: Dict[str, int],
-    reference: pycolmap.Reconstruction,
-    db: pycolmap.Database,
-    features_path: Path,
-    pairs_path: Path,
-    matches_path: Path,
-    max_error: float = 4.0,
-):
-    logger.info("Performing geometric verification of the matches...")
-
-    pairs = parse_retrieval(pairs_path)
-
-    inlier_ratios = []
-    matched = set()
-    for name0 in tqdm(pairs):
-        id0 = image_ids[name0]
-        image0 = reference.images[id0]
-        cam0 = reference.cameras[image0.camera_id]
-        kps0, noise0 = get_keypoints(features_path, name0, return_uncertainty=True)
-        noise0 = 1.0 if noise0 is None else noise0
-        if len(kps0) > 0:
-            kps0 = np.stack(cam0.cam_from_img(kps0))
-        else:
-            kps0 = np.zeros((0, 2))
-
-        for name1 in pairs[name0]:
-            id1 = image_ids[name1]
-            image1 = reference.images[id1]
-            cam1 = reference.cameras[image1.camera_id]
-            kps1, noise1 = get_keypoints(features_path, name1, return_uncertainty=True)
-            noise1 = 1.0 if noise1 is None else noise1
-            if len(kps1) > 0:
-                kps1 = np.stack(cam1.cam_from_img(kps1))
-            else:
-                kps1 = np.zeros((0, 2))
-
-            matches = get_matches(matches_path, name0, name1)[0]
-
-            if len({(id0, id1), (id1, id0)} & matched) > 0:
-                continue
-            matched |= {(id0, id1), (id1, id0)}
-
-            if matches.shape[0] == 0:
-                db.write_two_view_geometry(id0, id1, pycolmap.TwoViewGeometry())
-                continue
-
-            cam1_from_cam0 = image1.cam_from_world() * image0.cam_from_world().inverse()
-            errors0, errors1 = compute_epipolar_errors(
-                cam1_from_cam0, kps0[matches[:, 0]], kps1[matches[:, 1]]
-            )
-            valid_matches = np.logical_and(
-                errors0 <= cam0.cam_from_img_threshold(noise0 * max_error),
-                errors1 <= cam1.cam_from_img_threshold(noise1 * max_error),
-            )
-            # TODO: We could also add E to the database, but we need
-            # to reverse the transformations if id0 > id1 in utils/database.py.
-            db.write_two_view_geometry(
-                id0,
-                id1,
-                pycolmap.TwoViewGeometry(inlier_matches=matches[valid_matches, :]),
-            )
-            inlier_ratios.append(np.mean(valid_matches))
-    logger.info(
-        "mean/med/min/max valid matches %.2f/%.2f/%.2f/%.2f%%.",
-        np.mean(inlier_ratios) * 100,
-        np.median(inlier_ratios) * 100,
-        np.min(inlier_ratios) * 100,
-        np.max(inlier_ratios) * 100,
-    )
-
-
-def run_triangulation(
-    model_path: Path,
-    database_path: Path,
-    image_dir: Path,
-    reference_model: pycolmap.Reconstruction,
-    verbose: bool = False,
-    options: Optional[Dict[str, Any]] = None,
-) -> pycolmap.Reconstruction:
-    model_path.mkdir(parents=True, exist_ok=True)
-    logger.info("Running 3D triangulation...")
-    if options is None:
-        options = {}
-    with OutputCapture(verbose):
-        reconstruction = pycolmap.triangulate_points(
-            reference_model, database_path, image_dir, model_path, options=options
-        )
-    return reconstruction
-
-
-def main(
-    sfm_dir: Path,
-    reference_model: Path,
-    image_dir: Path,
-    pairs: Path,
-    features: Path,
-    matches: Path,
-    skip_geometric_verification: bool = False,
-    estimate_two_view_geometries: bool = False,
-    min_match_score: Optional[float] = None,
-    verbose: bool = False,
-    mapper_options: Optional[Dict[str, Any]] = None,
-) -> pycolmap.Reconstruction:
-    assert reference_model.exists(), reference_model
-    assert features.exists(), features
-    assert pairs.exists(), pairs
-    assert matches.exists(), matches
-
-    sfm_dir.mkdir(parents=True, exist_ok=True)
-    database = sfm_dir / "database.db"
-    reference = pycolmap.Reconstruction(reference_model)
-
-    image_ids = create_db_from_model(reference, database)
-    with pycolmap.Database.open(database) as db:
-        import_features(image_ids, db, features)
-        import_matches(
-            image_ids,
-            db,
-            pairs,
-            matches,
-            min_match_score,
-            skip_geometric_verification,
-        )
-    if not skip_geometric_verification:
-        if estimate_two_view_geometries:
-            estimation_and_geometric_verification(database, pairs, verbose)
-        else:
-            with pycolmap.Database.open(database) as db:
-                geometric_verification(
-                    image_ids, reference, db, features, pairs, matches
-                )
-    reconstruction = run_triangulation(
-        sfm_dir, database, image_dir, reference, verbose, mapper_options
-    )
-    logger.info(
-        "Finished the triangulation with statistics:\n%s", reconstruction.summary()
-    )
-    return reconstruction
+  def __enter__(self):
+    if not self.verbose:
+      self.capture = contextlib.redirect_stdout(io.StringIO())
+      self.out = self.capture.__enter__()
+
+  def __exit__(self, exc_type, *args):
+    if not self.verbose:
+      self.capture.__exit__(exc_type, *args)
+      if exc_type is not None:
+        logger.error('Failed with output:\n%s', self.out.getvalue())
+    sys.stdout.flush()
+
+
+def create_db_from_model(reconstruction: pycolmap.Reconstruction,
+                         database_path: Path) -> Dict[str, int]:
+  if database_path.exists():
+    logger.warning('The database already exists, deleting it.')
+    database_path.unlink()
+
+  db = COLMAPDatabase.connect(database_path)
+  db.create_tables()
+
+  for i, camera in reconstruction.cameras.items():
+    db.add_camera(
+        camera.model_id, camera.width, camera.height, camera.params,
+        camera_id=i, prior_focal_length=True)
+
+  for i, image in reconstruction.images.items():
+    db.add_image(image.name, image.camera_id, image_id=i)
+
+  db.commit()
+  db.close()
+  return {image.name: i for i, image in reconstruction.images.items()}
+
+
+def import_features(image_ids: Dict[str, int],
+                    database_path: Path,
+                    features_path: Path):
+  logger.info('Importing features into the database...')
+  db = COLMAPDatabase.connect(database_path)
+
+  for image_name, image_id in tqdm(image_ids.items()):
+    keypoints = get_keypoints(features_path, image_name)
+    keypoints += 0.5  # COLMAP origin
+    db.add_keypoints(image_id, keypoints)
+
+  db.commit()
+  db.close()
+
+
+def import_matches(image_ids: Dict[str, int],
+                   database_path: Path,
+                   pairs_path: Path,
+                   matches_path: Path,
+                   min_match_score: Optional[float] = None,
+                   skip_geometric_verification: bool = False):
+  logger.info('Importing matches into the database...')
+
+  with open(str(pairs_path), 'r') as f:
+    pairs = [p.split() for p in f.readlines()]
+
+  db = COLMAPDatabase.connect(database_path)
+
+  matched = set()
+  for name0, name1 in tqdm(pairs):
+    id0, id1 = image_ids[name0], image_ids[name1]
+    if len({(id0, id1), (id1, id0)} & matched) > 0:
+      continue
+    matches, scores = get_matches(matches_path, name0, name1)
+    if min_match_score:
+      matches = matches[scores > min_match_score]
+    db.add_matches(id0, id1, matches)
+    matched |= {(id0, id1), (id1, id0)}
+
+    if skip_geometric_verification:
+      db.add_two_view_geometry(id0, id1, matches)
+
+  db.commit()
+  db.close()
+
+
+def estimation_and_geometric_verification(database_path: Path,
+                                          pairs_path: Path,
+                                          verbose: bool = False):
+  logger.info('Performing geometric verification of the matches...')
+  with OutputCapture(verbose):
+    with pycolmap.ostream():
+      pycolmap.verify_matches(
+          database_path, pairs_path,
+          max_num_trials=20000, min_inlier_ratio=0.1)
+
+
+def geometric_verification(image_ids: Dict[str, int],
+                           reference: pycolmap.Reconstruction,
+                           database_path: Path,
+                           features_path: Path,
+                           pairs_path: Path,
+                           matches_path: Path,
+                           max_error: float = 4.0):
+  logger.info('Performing geometric verification of the matches...')
+
+  pairs = parse_retrieval(pairs_path)
+  db = COLMAPDatabase.connect(database_path)
+
+  inlier_ratios = []
+  matched = set()
+  for name0 in tqdm(pairs):
+    id0 = image_ids[name0]
+    image0 = reference.images[id0]
+    cam0 = reference.cameras[image0.camera_id]
+    kps0, noise0 = get_keypoints(
+        features_path, name0, return_uncertainty=True)
+    noise0 = 1.0 if noise0 is None else noise0
+    kps0 = np.stack(cam0.image_to_world(kps0))
+
+    for name1 in pairs[name0]:
+      id1 = image_ids[name1]
+      image1 = reference.images[id1]
+      cam1 = reference.cameras[image1.camera_id]
+      kps1, noise1 = get_keypoints(
+          features_path, name1, return_uncertainty=True)
+      noise1 = 1.0 if noise1 is None else noise1
+      kps1 = np.stack(cam1.image_to_world(kps1))
+
+      matches = get_matches(matches_path, name0, name1)[0]
+
+      if len({(id0, id1), (id1, id0)} & matched) > 0:
+        continue
+      matched |= {(id0, id1), (id1, id0)}
+
+      if matches.shape[0] == 0:
+        db.add_two_view_geometry(id0, id1, matches)
+        continue
+
+      qvec_01, tvec_01 = pycolmap.relative_pose(
+          image0.qvec, image0.tvec, image1.qvec, image1.tvec)
+      _, errors0, errors1 = compute_epipolar_errors(
+          qvec_01, tvec_01, kps0[matches[:, 0]], kps1[matches[:, 1]])
+      valid_matches = np.logical_and(
+          errors0 <= max_error * noise0 / cam0.mean_focal_length(),
+          errors1 <= max_error * noise1 / cam1.mean_focal_length())
+      # TODO: We could also add E to the database, but we need
+      # to reverse the transformations if id0 > id1 in utils/database.py.
+      db.add_two_view_geometry(id0, id1, matches[valid_matches, :])
+      inlier_ratios.append(np.mean(valid_matches))
+  logger.info('mean/med/min/max valid matches %.2f/%.2f/%.2f/%.2f%%.',
+              np.mean(inlier_ratios) * 100, np.median(inlier_ratios) * 100,
+              np.min(inlier_ratios) * 100, np.max(inlier_ratios) * 100)
+
+  db.commit()
+  db.close()
+
+
+def run_triangulation(model_path: Path,
+                      database_path: Path,
+                      image_dir: Path,
+                      reference_model: pycolmap.Reconstruction,
+                      verbose: bool = False,
+                      options: Optional[Dict[str, Any]] = None,
+                      ) -> pycolmap.Reconstruction:
+  model_path.mkdir(parents=True, exist_ok=True)
+  logger.info('Running 3D triangulation...')
+  if options is None:
+    options = {}
+  with OutputCapture(verbose):
+    with pycolmap.ostream():
+      reconstruction = pycolmap.triangulate_points(
+          reference_model, database_path, image_dir, model_path,
+          options=options)
+  return reconstruction
+
+
+def main(sfm_dir: Path,
+         reference_model: Path,
+         image_dir: Path,
+         pairs: Path,
+         features: Path,
+         matches: Path,
+         skip_geometric_verification: bool = False,
+         estimate_two_view_geometries: bool = False,
+         min_match_score: Optional[float] = None,
+         verbose: bool = False,
+         mapper_options: Optional[Dict[str, Any]] = None,
+         ) -> pycolmap.Reconstruction:
+
+  assert reference_model.exists(), reference_model
+  assert features.exists(), features
+  assert pairs.exists(), pairs
+  assert matches.exists(), matches
+
+  sfm_dir.mkdir(parents=True, exist_ok=True)
+  database = sfm_dir / 'database.db'
+  reference = pycolmap.Reconstruction(reference_model)
+
+  image_ids = create_db_from_model(reference, database)
+  import_features(image_ids, database, features)
+  import_matches(image_ids, database, pairs, matches,
+                 min_match_score, skip_geometric_verification)
+  if not skip_geometric_verification:
+    if estimate_two_view_geometries:
+      estimation_and_geometric_verification(database, pairs, verbose)
+    else:
+      geometric_verification(
+          image_ids, reference, database, features, pairs, matches)
+  reconstruction = run_triangulation(sfm_dir, database, image_dir, reference,
+                                     verbose, mapper_options)
+  logger.info('Finished the triangulation with statistics:\n%s',
+              reconstruction.summary())
+  return reconstruction
 
 
 def parse_option_args(args: List[str], default_options) -> Dict[str, Any]:
-    options = {}
-    for arg in args:
-        idx = arg.find("=")
-        if idx == -1:
-            raise ValueError("Options format: key1=value1 key2=value2 etc.")
-        key, value = arg[:idx], arg[idx + 1 :]
-        if not hasattr(default_options, key):
-            raise ValueError(
-                f'Unknown option "{key}", allowed options and default values'
-                f" for {default_options.summary()}"
-            )
-        value = eval(value)
-        target_type = type(getattr(default_options, key))
-        if not isinstance(value, target_type):
-            raise ValueError(
-                f'Incorrect type for option "{key}":' f" {type(value)} vs {target_type}"
-            )
-        options[key] = value
-    return options
-
-
-if __name__ == "__main__":
-    parser = argparse.ArgumentParser()
-    parser.add_argument("--sfm_dir", type=Path, required=True)
-    parser.add_argument("--reference_sfm_model", type=Path, required=True)
-    parser.add_argument("--image_dir", type=Path, required=True)
-
-    parser.add_argument("--pairs", type=Path, required=True)
-    parser.add_argument("--features", type=Path, required=True)
-    parser.add_argument("--matches", type=Path, required=True)
-
-    parser.add_argument("--skip_geometric_verification", action="store_true")
-    parser.add_argument("--min_match_score", type=float)
-    parser.add_argument("--verbose", action="store_true")
-    args = parser.parse_args().__dict__
-
-    mapper_options = parse_option_args(
-        args.pop("mapper_options"), pycolmap.IncrementalMapperOptions()
-    )
+  options = {}
+  for arg in args:
+    idx = arg.find('=')
+    if idx == -1:
+      raise ValueError('Options format: key1=value1 key2=value2 etc.')
+    key, value = arg[:idx], arg[idx+1:]
+    if not hasattr(default_options, key):
+      raise ValueError(
+          f'Unknown option "{key}", allowed options and default values'
+          f' for {default_options.summary()}')
+    value = eval(value)
+    target_type = type(getattr(default_options, key))
+    if not isinstance(value, target_type):
+      raise ValueError(f'Incorrect type for option "{key}":'
+                       f' {type(value)} vs {target_type}')
+    options[key] = value
+  return options
+
+
+if __name__ == '__main__':
+  parser = argparse.ArgumentParser()
+  parser.add_argument('--sfm_dir', type=Path, required=True)
+  parser.add_argument('--reference_sfm_model', type=Path, required=True)
+  parser.add_argument('--image_dir', type=Path, required=True)
+
+  parser.add_argument('--pairs', type=Path, required=True)
+  parser.add_argument('--features', type=Path, required=True)
+  parser.add_argument('--matches', type=Path, required=True)
+
+  parser.add_argument('--skip_geometric_verification', action='store_true')
+  parser.add_argument('--min_match_score', type=float)
+  parser.add_argument('--verbose', action='store_true')
+  args = parser.parse_args().__dict__
+
+  mapper_options = parse_option_args(
+      args.pop("mapper_options"), pycolmap.IncrementalMapperOptions())
 
-    main(**args, mapper_options=mapper_options)
+  main(**args, mapper_options=mapper_options)
diff -Naur '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=pipelines' '--exclude=extractors' '--exclude=matchers' /tmp/hloc-latest/hloc/utils/base_model.py hloc/utils/base_model.py
--- /tmp/hloc-latest/hloc/utils/base_model.py	2026-01-16 16:13:56.815239491 -0700
+++ hloc/utils/base_model.py	2026-01-16 16:16:08.546224078 -0700
@@ -1,48 +1,89 @@
-import inspect
+# SPDX-FileCopyrightText: (C) 2025 Intel Corporation
+# SPDX-License-Identifier: Apache-2.0
+
 import sys
 from abc import ABCMeta, abstractmethod
 from copy import copy
-
+import inspect
+from enum import Flag, auto
+import numpy as np
+import torch
+from types import SimpleNamespace as SN
 from torch import nn
 
+torch.set_flush_denormal(True)
+torch.jit.enable_onednn_fusion(True)
+
 
 class BaseModel(nn.Module, metaclass=ABCMeta):
-    default_conf = {}
-    required_inputs = []
+  default_conf = {}
+  required_inputs = []
 
-    def __init__(self, conf):
-        """Perform some logic and call the _init method of the child model."""
-        super().__init__()
-        self.conf = conf = {**self.default_conf, **conf}
-        self.required_inputs = copy(self.required_inputs)
-        self._init(conf)
-        sys.stdout.flush()
-
-    def forward(self, data):
-        """Check the data and call the _forward method of the child model."""
-        for key in self.required_inputs:
-            assert key in data, "Missing key {} in data".format(key)
-        return self._forward(data)
-
-    @abstractmethod
-    def _init(self, conf):
-        """To be implemented by the child class."""
-        raise NotImplementedError
-
-    @abstractmethod
-    def _forward(self, data):
-        """To be implemented by the child class."""
-        raise NotImplementedError
+  def __init__(self, conf):
+    """Perform some logic and call the _init method of the child model."""
+    super().__init__()
+    self.conf = conf = {**self.default_conf, **conf}
+    self.required_inputs = copy(self.required_inputs)
+    self._init(conf)
+    sys.stdout.flush()
+
+  def forward(self, data):
+    """Check the data and call the _forward method of the child model."""
+    for key in self.required_inputs:
+      assert key in data, "Missing key {} in data".format(key)
+    return self._forward(data)
+
+  @abstractmethod
+  def _init(self, conf):
+    """To be implemented by the child class."""
+    raise NotImplementedError
+
+  @abstractmethod
+  def _forward(self, data):
+    """To be implemented by the child class."""
+    raise NotImplementedError
 
 
 def dynamic_load(root, model):
-    module_path = f"{root.__name__}.{model}"
-    module = __import__(module_path, fromlist=[""])
-    classes = inspect.getmembers(module, inspect.isclass)
-    # Filter classes defined in the module
-    classes = [c for c in classes if c[1].__module__ == module_path]
-    # Filter classes inherited from BaseModel
-    classes = [c for c in classes if issubclass(c[1], BaseModel)]
-    assert len(classes) == 1, classes
-    return classes[0][1]
-    # return getattr(module, 'Model')
+  module_path = f"{root.__name__}.{model}"
+  module = __import__(module_path, fromlist=[""])
+  classes = inspect.getmembers(module, inspect.isclass)
+  # Filter classes defined in the module
+  classes = [c for c in classes if c[1].__module__ == module_path]
+  # Filter classes inherited from BaseModel
+  classes = [c for c in classes if issubclass(c[1], BaseModel)]
+  assert len(classes) == 1, classes
+  return classes[0][1]
+  # return getattr(module, 'Model')
+
+
+def cached_load(root, model_conf):
+  """Reuse optimized model in repeat calls. Only maintains one model per name, so
+  using a different configuration will overwrite a model."""
+  name = model_conf["name"]
+  if (
+      name in cached_load.model_cache
+      and cached_load.model_cache[name].conf == model_conf
+  ):
+    return cached_load.model_cache[name].model
+  Model = dynamic_load(root, model_conf["name"])
+  model = Model(model_conf).eval().to(cached_load.device)
+  if "optimize" in model_conf and "script" in model_conf["optimize"]:
+    model = torch.jit.freeze(torch.jit.script(model))
+    model = torch.jit.optimize_for_inference(model)
+  if cached_load.device == "cpu":
+    try:
+      import intel_extension_for_pytorch as ipex
+
+      ipex.enable_onednn_fusion(True)
+
+      model = ipex.optimize(model)
+    except ImportError:
+      pass
+  cached_load.model_cache[name] = SN(model=model, conf=model_conf)
+  return model
+
+
+cached_load.device = "cuda" if torch.cuda.is_available() else "cpu"
+cached_load.model_cache = {}
+"""Cache loaded, optimized models by name"""
diff -Naur '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=pipelines' '--exclude=extractors' '--exclude=matchers' /tmp/hloc-latest/hloc/utils/database.py hloc/utils/database.py
--- /tmp/hloc-latest/hloc/utils/database.py	1969-12-31 17:00:00.000000000 -0700
+++ hloc/utils/database.py	2026-01-16 16:16:08.546445879 -0700
@@ -0,0 +1,333 @@
+# SPDX-FileCopyrightText: (C) 2025 Intel Corporation
+# SPDX-FileCopyrightText: 2018 ETH Zurich and UNC Chapel Hill.
+# SPDX-License-Identifier: Apache-2.0
+
+# This script is based on an original implementation by True Price.
+
+import sys
+import sqlite3
+import numpy as np
+
+
+IS_PYTHON3 = sys.version_info[0] >= 3
+
+MAX_IMAGE_ID = 2**31 - 1
+
+CREATE_CAMERAS_TABLE = """CREATE TABLE IF NOT EXISTS cameras (
+    camera_id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,
+    model INTEGER NOT NULL,
+    width INTEGER NOT NULL,
+    height INTEGER NOT NULL,
+    params BLOB,
+    prior_focal_length INTEGER NOT NULL)"""
+
+CREATE_DESCRIPTORS_TABLE = """CREATE TABLE IF NOT EXISTS descriptors (
+    image_id INTEGER PRIMARY KEY NOT NULL,
+    rows INTEGER NOT NULL,
+    cols INTEGER NOT NULL,
+    data BLOB,
+    FOREIGN KEY(image_id) REFERENCES images(image_id) ON DELETE CASCADE)"""
+
+CREATE_IMAGES_TABLE = """CREATE TABLE IF NOT EXISTS images (
+    image_id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,
+    name TEXT NOT NULL UNIQUE,
+    camera_id INTEGER NOT NULL,
+    prior_qw REAL,
+    prior_qx REAL,
+    prior_qy REAL,
+    prior_qz REAL,
+    prior_tx REAL,
+    prior_ty REAL,
+    prior_tz REAL,
+    CONSTRAINT image_id_check CHECK(image_id >= 0 and image_id < {}),
+    FOREIGN KEY(camera_id) REFERENCES cameras(camera_id))
+""".format(MAX_IMAGE_ID)
+
+CREATE_TWO_VIEW_GEOMETRIES_TABLE = """
+CREATE TABLE IF NOT EXISTS two_view_geometries (
+    pair_id INTEGER PRIMARY KEY NOT NULL,
+    rows INTEGER NOT NULL,
+    cols INTEGER NOT NULL,
+    data BLOB,
+    config INTEGER NOT NULL,
+    F BLOB,
+    E BLOB,
+    H BLOB,
+    qvec BLOB,
+    tvec BLOB)
+"""
+
+CREATE_KEYPOINTS_TABLE = """CREATE TABLE IF NOT EXISTS keypoints (
+    image_id INTEGER PRIMARY KEY NOT NULL,
+    rows INTEGER NOT NULL,
+    cols INTEGER NOT NULL,
+    data BLOB,
+    FOREIGN KEY(image_id) REFERENCES images(image_id) ON DELETE CASCADE)
+"""
+
+CREATE_MATCHES_TABLE = """CREATE TABLE IF NOT EXISTS matches (
+    pair_id INTEGER PRIMARY KEY NOT NULL,
+    rows INTEGER NOT NULL,
+    cols INTEGER NOT NULL,
+    data BLOB)"""
+
+CREATE_NAME_INDEX = \
+    "CREATE UNIQUE INDEX IF NOT EXISTS index_name ON images(name)"
+
+CREATE_ALL = "; ".join([
+    CREATE_CAMERAS_TABLE,
+    CREATE_IMAGES_TABLE,
+    CREATE_KEYPOINTS_TABLE,
+    CREATE_DESCRIPTORS_TABLE,
+    CREATE_MATCHES_TABLE,
+    CREATE_TWO_VIEW_GEOMETRIES_TABLE,
+    CREATE_NAME_INDEX
+])
+
+
+def image_ids_to_pair_id(image_id1, image_id2):
+  if image_id1 > image_id2:
+    image_id1, image_id2 = image_id2, image_id1
+  return image_id1 * MAX_IMAGE_ID + image_id2
+
+
+def pair_id_to_image_ids(pair_id):
+  image_id2 = pair_id % MAX_IMAGE_ID
+  image_id1 = (pair_id - image_id2) / MAX_IMAGE_ID
+  return image_id1, image_id2
+
+
+def array_to_blob(array):
+  if IS_PYTHON3:
+    return array.tobytes()
+  else:
+    return np.getbuffer(array)
+
+
+def blob_to_array(blob, dtype, shape=(-1,)):
+  if IS_PYTHON3:
+    return np.fromstring(blob, dtype=dtype).reshape(*shape)
+  else:
+    return np.frombuffer(blob, dtype=dtype).reshape(*shape)
+
+
+class COLMAPDatabase(sqlite3.Connection):
+
+  @staticmethod
+  def connect(database_path):
+    return sqlite3.connect(str(database_path), factory=COLMAPDatabase)
+
+  def __init__(self, *args, **kwargs):
+    super(COLMAPDatabase, self).__init__(*args, **kwargs)
+
+    self.create_tables = lambda: self.executescript(CREATE_ALL)
+    self.create_cameras_table = \
+        lambda: self.executescript(CREATE_CAMERAS_TABLE)
+    self.create_descriptors_table = \
+        lambda: self.executescript(CREATE_DESCRIPTORS_TABLE)
+    self.create_images_table = \
+        lambda: self.executescript(CREATE_IMAGES_TABLE)
+    self.create_two_view_geometries_table = \
+        lambda: self.executescript(CREATE_TWO_VIEW_GEOMETRIES_TABLE)
+    self.create_keypoints_table = \
+        lambda: self.executescript(CREATE_KEYPOINTS_TABLE)
+    self.create_matches_table = \
+        lambda: self.executescript(CREATE_MATCHES_TABLE)
+    self.create_name_index = lambda: self.executescript(CREATE_NAME_INDEX)
+
+  def add_camera(self, model, width, height, params,
+                 prior_focal_length=False, camera_id=None):
+    params = np.asarray(params, np.float64)
+    cursor = self.execute(
+        "INSERT INTO cameras VALUES (?, ?, ?, ?, ?, ?)",
+        (camera_id, model, width, height, array_to_blob(params),
+         prior_focal_length))
+    return cursor.lastrowid
+
+  def add_image(self, name, camera_id,
+                prior_q=np.full(4, np.NaN), prior_t=np.full(3, np.NaN),
+                image_id=None):
+    cursor = self.execute(
+        "INSERT INTO images VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)",
+        (image_id, name, camera_id, prior_q[0], prior_q[1], prior_q[2],
+         prior_q[3], prior_t[0], prior_t[1], prior_t[2]))
+    return cursor.lastrowid
+
+  def add_keypoints(self, image_id, keypoints):
+    assert (len(keypoints.shape) == 2)
+    assert (keypoints.shape[1] in [2, 4, 6])
+
+    keypoints = np.asarray(keypoints, np.float32)
+    self.execute(
+        "INSERT INTO keypoints VALUES (?, ?, ?, ?)",
+        (image_id,) + keypoints.shape + (array_to_blob(keypoints),))
+
+  def add_descriptors(self, image_id, descriptors):
+    descriptors = np.ascontiguousarray(descriptors, np.uint8)
+    self.execute(
+        "INSERT INTO descriptors VALUES (?, ?, ?, ?)",
+        (image_id,) + descriptors.shape + (array_to_blob(descriptors),))
+
+  def add_matches(self, image_id1, image_id2, matches):
+    assert (len(matches.shape) == 2)
+    assert (matches.shape[1] == 2)
+
+    if image_id1 > image_id2:
+      matches = matches[:, ::-1]
+
+    pair_id = image_ids_to_pair_id(image_id1, image_id2)
+    matches = np.asarray(matches, np.uint32)
+    self.execute(
+        "INSERT INTO matches VALUES (?, ?, ?, ?)",
+        (pair_id,) + matches.shape + (array_to_blob(matches),))
+
+  def add_two_view_geometry(self, image_id1, image_id2, matches,
+                            F=np.eye(3), E=np.eye(3), H=np.eye(3),
+                            qvec=np.array([1.0, 0.0, 0.0, 0.0]),
+                            tvec=np.zeros(3), config=2):
+    assert (len(matches.shape) == 2)
+    assert (matches.shape[1] == 2)
+
+    if image_id1 > image_id2:
+      matches = matches[:, ::-1]
+
+    pair_id = image_ids_to_pair_id(image_id1, image_id2)
+    matches = np.asarray(matches, np.uint32)
+    F = np.asarray(F, dtype=np.float64)
+    E = np.asarray(E, dtype=np.float64)
+    H = np.asarray(H, dtype=np.float64)
+    qvec = np.asarray(qvec, dtype=np.float64)
+    tvec = np.asarray(tvec, dtype=np.float64)
+    self.execute(
+        "INSERT INTO two_view_geometries VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)",
+        (pair_id,) + matches.shape + (array_to_blob(matches), config,
+                                      array_to_blob(F), array_to_blob(
+            E), array_to_blob(H),
+            array_to_blob(qvec), array_to_blob(tvec)))
+
+
+def example_usage():
+  import os
+  import argparse
+
+  parser = argparse.ArgumentParser()
+  parser.add_argument("--database_path", default="database.db")
+  args = parser.parse_args()
+
+  if os.path.exists(args.database_path):
+    print("ERROR: database path already exists -- will not modify it.")
+    return
+
+  # Open the database.
+
+  db = COLMAPDatabase.connect(args.database_path)
+
+  # For convenience, try creating all the tables upfront.
+
+  db.create_tables()
+
+  # Create dummy cameras.
+
+  model1, width1, height1, params1 = \
+      0, 1024, 768, np.array((1024., 512., 384.))
+  model2, width2, height2, params2 = \
+      2, 1024, 768, np.array((1024., 512., 384., 0.1))
+
+  camera_id1 = db.add_camera(model1, width1, height1, params1)
+  camera_id2 = db.add_camera(model2, width2, height2, params2)
+
+  # Create dummy images.
+
+  image_id1 = db.add_image("image1.png", camera_id1)
+  image_id2 = db.add_image("image2.png", camera_id1)
+  image_id3 = db.add_image("image3.png", camera_id2)
+  image_id4 = db.add_image("image4.png", camera_id2)
+
+  # Create dummy keypoints.
+  #
+  # Note that COLMAP supports:
+  #      - 2D keypoints: (x, y)
+  #      - 4D keypoints: (x, y, theta, scale)
+  #      - 6D affine keypoints: (x, y, a_11, a_12, a_21, a_22)
+
+  num_keypoints = 1000
+  keypoints1 = np.random.rand(num_keypoints, 2) * (width1, height1)
+  keypoints2 = np.random.rand(num_keypoints, 2) * (width1, height1)
+  keypoints3 = np.random.rand(num_keypoints, 2) * (width2, height2)
+  keypoints4 = np.random.rand(num_keypoints, 2) * (width2, height2)
+
+  db.add_keypoints(image_id1, keypoints1)
+  db.add_keypoints(image_id2, keypoints2)
+  db.add_keypoints(image_id3, keypoints3)
+  db.add_keypoints(image_id4, keypoints4)
+
+  # Create dummy matches.
+
+  M = 50
+  matches12 = np.random.randint(num_keypoints, size=(M, 2))
+  matches23 = np.random.randint(num_keypoints, size=(M, 2))
+  matches34 = np.random.randint(num_keypoints, size=(M, 2))
+
+  db.add_matches(image_id1, image_id2, matches12)
+  db.add_matches(image_id2, image_id3, matches23)
+  db.add_matches(image_id3, image_id4, matches34)
+
+  # Commit the data to the file.
+
+  db.commit()
+
+  # Read and check cameras.
+
+  rows = db.execute("SELECT * FROM cameras")
+
+  camera_id, model, width, height, params, prior = next(rows)
+  params = blob_to_array(params, np.float64)
+  assert camera_id == camera_id1
+  assert model == model1 and width == width1 and height == height1
+  assert np.allclose(params, params1)
+
+  camera_id, model, width, height, params, prior = next(rows)
+  params = blob_to_array(params, np.float64)
+  assert camera_id == camera_id2
+  assert model == model2 and width == width2 and height == height2
+  assert np.allclose(params, params2)
+
+  # Read and check keypoints.
+
+  keypoints = dict(
+      (image_id, blob_to_array(data, np.float32, (-1, 2)))
+      for image_id, data in db.execute(
+          "SELECT image_id, data FROM keypoints"))
+
+  assert np.allclose(keypoints[image_id1], keypoints1)
+  assert np.allclose(keypoints[image_id2], keypoints2)
+  assert np.allclose(keypoints[image_id3], keypoints3)
+  assert np.allclose(keypoints[image_id4], keypoints4)
+
+  # Read and check matches.
+
+  pair_ids = [image_ids_to_pair_id(*pair) for pair in
+              ((image_id1, image_id2),
+               (image_id2, image_id3),
+               (image_id3, image_id4))]
+
+  matches = dict(
+      (pair_id_to_image_ids(pair_id),
+       blob_to_array(data, np.uint32, (-1, 2)))
+      for pair_id, data in db.execute("SELECT pair_id, data FROM matches")
+  )
+
+  assert np.all(matches[(image_id1, image_id2)] == matches12)
+  assert np.all(matches[(image_id2, image_id3)] == matches23)
+  assert np.all(matches[(image_id3, image_id4)] == matches34)
+
+  # Clean up.
+
+  db.close()
+
+  if os.path.exists(args.database_path):
+    os.remove(args.database_path)
+
+
+if __name__ == "__main__":
+  example_usage()
diff -Naur '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=pipelines' '--exclude=extractors' '--exclude=matchers' /tmp/hloc-latest/hloc/utils/dataset.py hloc/utils/dataset.py
--- /tmp/hloc-latest/hloc/utils/dataset.py	1969-12-31 17:00:00.000000000 -0700
+++ hloc/utils/dataset.py	2026-01-16 16:16:08.558886878 -0700
@@ -0,0 +1,41 @@
+# SPDX-FileCopyrightText: (C) 2025 Intel Corporation
+# SPDX-License-Identifier: Apache-2.0
+
+import json
+import open3d as o3d
+import os
+from argparse import ArgumentParser
+
+
+def build_argparser():
+  parser = ArgumentParser()
+  parser.add_argument("--bag_file", help="path to bag file")
+  parser.add_argument("--bag_frames", help="path to output folder")
+  return parser
+
+
+def convert_json(bag_frames):
+  with open(os.path.join(bag_frames, "intrinsic.json")) as json_str:
+    data = json.load(json_str)
+    txt = open(os.path.join(bag_frames, "cameras.txt"), "w")
+    txt.write("#camera_id model width height params\n")
+    intrinsic = data['intrinsic_matrix']
+    txt.write(f"0 OPENCV {data['width']} {data['height']} {intrinsic[0]} {
+              intrinsic[4]} {intrinsic[6]} {intrinsic[7]} 0 0 0 0")
+    txt.close()
+
+
+def main():
+  args = build_argparser().parse_args()
+  if args.bag_file != None:
+    bag_reader = o3d.t.io.RSBagReader()
+    opened = bag_reader.open(args.bag_file)
+    if opened:
+      bag_reader.save_frames(args.bag_frames)
+      bag_reader.close()
+  convert_json(args.bag_frames)
+  os.rename(args.bag_frames+"/color", args.bag_frames+"/rgb")
+
+
+if __name__ == '__main__':
+  exit(main() or 0)
diff -Naur '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=pipelines' '--exclude=extractors' '--exclude=matchers' /tmp/hloc-latest/hloc/utils/evaluate.py hloc/utils/evaluate.py
--- /tmp/hloc-latest/hloc/utils/evaluate.py	1969-12-31 17:00:00.000000000 -0700
+++ hloc/utils/evaluate.py	2026-01-16 16:16:08.558932560 -0700
@@ -0,0 +1,119 @@
+# SPDX-FileCopyrightText: (C) 2025 Intel Corporation
+# SPDX-License-Identifier: Apache-2.0
+
+import numpy as np
+from scipy.spatial.transform import Rotation as R
+from .read_write_model import qvec2rotmat
+from typing import Dict, Sequence
+
+
+def to_sp_rotation(qwxyz):
+  return R.from_quat((*qwxyz[1:], qwxyz[0]))
+
+
+def pose_distance(pose1, pose2):
+  if isinstance(pose1, tuple):  # (R,t) or (qwxyz, tvec)
+    R1, t1 = pose1[0], pose1[1]
+    R2, t2 = pose2[0], pose2[1]
+    if R1.size == 4:  # quaternion qwxyz
+      R1 = to_sp_rotation(R1)
+      R2 = to_sp_rotation(R2)
+    distance = np.linalg.norm(-R1.inv().apply(t1) + R2.inv().apply(t2))
+    angle = (R1.inv() * R2).magnitude()
+    return distance, angle
+  if isinstance(pose1, np.ndarray):  # 4x4 homogenous matrix
+    return pose_distance(
+        (pose1[:3, :3], pose1[:3, 3:]), (pose2[:3, :3], pose2[:3, 3:])
+    )
+  return pose_distance((pose1.qvec, pose1.tvec), (pose2.qvec, pose2.tvec))
+
+
+def find_nearest_pose(query_poses: Dict, dataset_poses: Dict):
+  """Get nearest pose from a dataset to a query pose.
+
+  Args:
+      query_poses (dict): List of query poses. Pose is the camera extrinsic tuple
+      (q_xyzw, tvec).
+      dataset_poses (dict): List of dataset poses.
+
+  Returns:
+      (dict[str, str], dict[str, (float, float)]): dataset_poses that are
+      closest to the query_poses and min distances.
+  """
+  min_dset = {}
+  min_d_a = {}
+  for qpath, qpose in query_poses.items():
+    min_d_a[qpath] = (np.inf, np.inf)
+    for dpath, dpose in dataset_poses.items():
+      # prevent sorting order errors due to rounding errors
+      d_a = tuple(round(x, 6) for x in pose_distance(qpose, dpose))
+      if d_a < min_d_a[qpath]:
+        min_dset[qpath] = dpath
+        min_d_a[qpath] = d_a
+  return min_dset, min_d_a
+
+
+def evaluate(
+    poses_predicted: Dict[str, Dict],
+    poses_gt: Dict[str, Dict],
+    thresholds_tRdeg: Sequence = ((0.01, 1.0), (0.02, 2.0), (0.05, 5.0), (0.1, 10.0)),
+    error_basis=None,
+    only_localized: bool = False,
+    logs=None,
+):
+  """Compute median translation and rotation errors, as well as ratio of
+  images localizaed to within the given thresholds.
+
+  Args:
+      poses_predicted (Dict[str, Dict]): Dictionary of estimated camera
+          extrinsic parameters in terms of pairs of quaternion rotation and
+          translation for query images.
+      poses_gt (Dict[str, Dict]): Dictionary with ground truth poses, similar
+          to poses_predicted.
+      thresholds_tRdeg [Sequence]: Sequence of pairs of (distance,
+          angle_degrees) thresholds.
+      only_localized (bool): Ignore localization failures.
+      logs [Dict]: If provided, computed errors are aded to the logs.
+
+  Returns:
+      median translation error, median rotation (degrees) error, sequence of
+          ratios of images with error less than thresholds.
+  """
+  if error_basis is not None:
+    assert len(error_basis) == len(poses_predicted)
+  errors = np.empty((len(poses_gt), 3))
+  for idx, name in enumerate(poses_gt):
+    if name not in poses_predicted:
+      if only_localized:
+        continue
+      e_t = np.inf
+      e_R = 180.0
+    else:
+      R_gt, t_gt = qvec2rotmat(poses_gt[name][0]), poses_gt[name][1]
+      R, t = qvec2rotmat(poses_predicted[name][0]), poses_gt[name][1]
+      e_t = np.linalg.norm(-R_gt.T @ t_gt + R.T @ t, axis=0)
+      cos = np.clip((np.trace(np.dot(R_gt.T, R)) - 1) / 2, -1.0, 1.0)
+      e_R = np.rad2deg(np.abs(np.arccos(cos)))
+
+    errors[idx] = (
+        e_t,
+        1.0 if error_basis is None else e_t / error_basis[name][0],
+        e_R,
+    )
+    if logs is not None:
+      logs["loc"][name]["e_t"] = e_t
+      logs["loc"][name]["e_t_rel"] = errors[idx, 1]
+      logs["loc"][name]["e_R"] = e_R
+
+  med_t = np.median(errors[:, 0])
+  med_R = np.median(errors[:, 2])
+
+  ratio_localized = np.fromiter(
+      (
+          np.mean((errors[:, 0] < th_t) & (errors[:, 2] < th_R))
+          for th_t, th_R in thresholds_tRdeg
+      ),
+      dtype=float,
+  )
+
+  return med_t, med_R, ratio_localized
diff -Naur '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=pipelines' '--exclude=extractors' '--exclude=matchers' /tmp/hloc-latest/hloc/utils/geometry.py hloc/utils/geometry.py
--- /tmp/hloc-latest/hloc/utils/geometry.py	2026-01-16 16:13:56.815239491 -0700
+++ hloc/utils/geometry.py	2026-01-16 16:16:08.546776038 -0700
@@ -1,16 +1,155 @@
+# SPDX-FileCopyrightText: (C) 2025 Intel Corporation
+# SPDX-License-Identifier: Apache-2.0
+
+from typing import Union
+from pathlib import Path
+import functools
 import numpy as np
 import pycolmap
+import open3d as o3d
+import torch
+from torch.utils.dlpack import from_dlpack as torch_from_dlpack
+from hloc.utils.read_write_model import Camera
+from hloc import logger
+
+Image = o3d.t.geometry.Image
+Tensor = o3d.core.Tensor
+
+
+def Tinv(Tr):
+  """Inverse transform for homogenous matrix"""
+  return np.vstack(
+      (np.hstack((Tr[:3, :3].T, -Tr[:3, :3].T @ Tr[:3, 3:])), [[0, 0, 0, 1]])
+  )
 
 
 def to_homogeneous(p):
-    return np.pad(p, ((0, 0),) * (p.ndim - 1) + ((0, 1),), constant_values=1)
+  return np.pad(p, ((0, 0),) * (p.ndim - 1) + ((0, 1),), constant_values=1)
+
+
+def vector_to_cross_product_matrix(v):
+  return np.array([[0, -v[2], v[1]], [v[2], 0, -v[0]], [-v[1], v[0], 0]])
+
+
+def compute_epipolar_errors(qvec_r2t, tvec_r2t, p2d_r, p2d_t):
+  T_r2t = pose_matrix_from_qvec_tvec(qvec_r2t, tvec_r2t)
+  # Compute errors in normalized plane to avoid distortion.
+  E = vector_to_cross_product_matrix(T_r2t[:3, -1]) @ T_r2t[:3, :3]
+  l2d_r2t = (E @ to_homogeneous(p2d_r).T).T
+  l2d_t2r = (E.T @ to_homogeneous(p2d_t).T).T
+  errors_r = np.abs(np.sum(to_homogeneous(p2d_r) * l2d_t2r, axis=1)) / np.linalg.norm(
+      l2d_t2r[:, :2], axis=1
+  )
+  errors_t = np.abs(np.sum(to_homogeneous(p2d_t) * l2d_r2t, axis=1)) / np.linalg.norm(
+      l2d_r2t[:, :2], axis=1
+  )
+  return E, errors_r, errors_t
+
+
+def pose_matrix_from_qvec_tvec(qvec, tvec):
+  pose = np.eye(4)
+  pose[:3, :3] = pycolmap.qvec_to_rotmat(qvec)
+  pose[:3, -1] = tvec
+  return pose
+
+
+def qvec_tvec_from_pose_matrix(pose):
+  qvec = pycolmap.rotmat_to_qvec(pose[:3, :3])
+  tvec = pose[:3, -1]
+  return qvec, tvec
+
+
+def get_intrinsic_mat(camera: Camera):
+  distortion = np.array([0.0])
+  if camera.model.upper() in ("SIMPLE_PINHOLE", "SIMPLE_RADIAL"):
+    fx, cx, cy, *distortion = camera.params
+    fy = fx
+  elif camera.model.upper() in ("PINHOLE", "RADIAL", "OPENCV"):
+    fx, fy, cx, cy, *distortion = camera.params
+  else:
+    raise ValueError(
+        f"Unsupported camera model {camera.model}. "
+        "Only SIMPLE_PINHOLE, PINHOLE, SIMPLE_RADIAL, RADIAL and OPENCV are supported."
+    )
+  if not np.allclose(distortion, 0):
+    logger.warning("Ignoring distortion in camera.")
+
+  intrinsic_mat = Tensor([[fx, 0, cx], [0, fy, cy], [0, 0, 1]])  #
+  return intrinsic_mat
+
+
+# @torch.no_grad
+def interpolate_scan(depth: Image, camera: Camera, kp: np.ndarray):
+
+  if (depth.rows, depth.columns) != (camera.height, camera.width):
+    raise ValueError(
+        f"Depth camera intrinsic shape ({camera.height},{camera.width})"
+        f"does not match depth image shape {depth.shape[:2]}."
+    )
+  intrinsic_mat = get_intrinsic_mat(camera)
+  scan = depth.create_vertex_map(
+      intrinsics=intrinsic_mat, invalid_fill=np.nan
+  ).as_tensor()
+
+  h, w, c = scan.shape
+  kp = kp / np.array([[w - 1, h - 1]], dtype=kp.dtype) * 2 - 1
+  assert np.all(kp > -1) and np.all(kp < 1)
+  scan = torch_from_dlpack(scan.to_dlpack()).permute(2, 0, 1)[None]
+  kp = torch.from_numpy(kp)[None, None]
+  grid_sample = torch.nn.functional.grid_sample
+
+  # To maximize the number of points that have depth:
+  # do bilinear interpolation first and then nearest for the remaining points
+  interp_lin = grid_sample(scan, kp, align_corners=True, mode="bilinear")[0, :, 0]
+  interp_nn = torch.nn.functional.grid_sample(
+      scan, kp, align_corners=True, mode="nearest"
+  )[0, :, 0]
+  interp = torch.where(torch.isnan(interp_lin), interp_nn, interp_lin)
+  valid = ~torch.any(torch.isnan(interp), 0)
+
+  kp3d = interp.T.numpy()
+  valid = valid.numpy()
+  return kp3d, valid
+
+
+def interpolate_mesh(
+    mesh_path: Union[Path, str],
+    pose: np.ndarray,  # extrinsics: world_to_camera
+    camera: Camera,  # intrinsics
+    kp: np.ndarray,  # (N, 2)
+):
+  """Create ray casting data structure for mesh interpolation and provide 3D
+  points for input 2D points and camera calibration (intrinsics + extrinics).
+
+  Args:
+
+      mesh_path (str or Path): path to mesh file.
+      pose (np.ndarray):  extrinsics- world_to_camera.
+      camera (Camera):  camera intrinsics.
+      kp (np.ndarray): 2D keypoints with shape (N, 2).
+
+  Returns:
+      tuple of 3D keypoints (shape (N,3)) and valid mask (shape (N,))
+  """
 
+  @functools.lru_cache(maxsize=8)
+  def get_raycasting_scene(mesh_path: str):
+    mesh = o3d.t.io.read_triangle_mesh(mesh_path)
+    rc_scene = o3d.t.geometry.RaycastingScene()
+    rc_scene.add_triangles(mesh)
+    return rc_scene
 
-def compute_epipolar_errors(j_from_i: pycolmap.Rigid3d, p2d_i, p2d_j):
-    j_E_i = pycolmap.essential_matrix_from_pose(j_from_i)
-    l2d_j = to_homogeneous(p2d_i) @ j_E_i.T
-    l2d_i = to_homogeneous(p2d_j) @ j_E_i
-    dist = np.abs(np.sum(to_homogeneous(p2d_i) * l2d_i, axis=1))
-    errors_i = dist / np.linalg.norm(l2d_i[:, :2], axis=1)
-    errors_j = dist / np.linalg.norm(l2d_j[:, :2], axis=1)
-    return errors_i, errors_j
+  K = get_intrinsic_mat(camera).numpy()
+  K_inv = np.linalg.inv(K)
+  camera_center = (-pose[:3, :3].T @ pose[:3, 3:]).astype(np.float32)
+  ray_dirn = (
+      pose[:3, :3].T @ K_inv @ np.vstack((kp.T, np.ones((1, kp.shape[0]))))
+  ).astype(np.float32)
+  rays = Tensor.from_numpy(
+      np.hstack((np.repeat(camera_center.T, kp.shape[0], axis=0), ray_dirn.T))
+  )
+  rc_scene = get_raycasting_scene(str(mesh_path))
+  result = rc_scene.cast_rays(rays)
+  valid = result["t_hit"].isfinite().numpy()
+  kp3d = camera_center + ray_dirn * result["t_hit"].reshape((1, -1)).numpy()
+  return kp3d.T, valid
diff -Naur '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=pipelines' '--exclude=extractors' '--exclude=matchers' /tmp/hloc-latest/hloc/utils/io.py hloc/utils/io.py
--- /tmp/hloc-latest/hloc/utils/io.py	2026-01-16 16:13:56.815239491 -0700
+++ hloc/utils/io.py	2026-01-16 16:16:08.546911868 -0700
@@ -1,92 +1,95 @@
-from pathlib import Path
-from typing import Mapping, Tuple
+# SPDX-FileCopyrightText: (C) 2025 Intel Corporation
+# SPDX-License-Identifier: Apache-2.0
 
+from typing import Tuple
+from pathlib import Path
+import base64
+import numpy as np
 import cv2
 import h5py
-import numpy as np
-import pycolmap
 
 from .parsers import names_to_pair, names_to_pair_old
 
 
 def read_image(path, grayscale=False):
-    if grayscale:
-        mode = cv2.IMREAD_GRAYSCALE
-    else:
-        mode = cv2.IMREAD_COLOR
-    image = cv2.imread(str(path), mode | cv2.IMREAD_IGNORE_ORIENTATION)
-    if image is None:
-        raise ValueError(f"Cannot read image {path}.")
-    if not grayscale and len(image.shape) == 3:
-        image = image[:, :, ::-1]  # BGR to RGB
-    return image
+  if grayscale:
+    mode = cv2.IMREAD_GRAYSCALE
+  else:
+    mode = cv2.IMREAD_COLOR
+  image = cv2.imread(str(path), mode)
+  if image is None:
+    raise ValueError(f"Cannot read image {path}.")
+  if not grayscale and len(image.shape) == 3:
+    image = image[:, :, ::-1]  # BGR to RGB
+  return image
+
+
+def base64_to_image(b64bytes: bytes, grayscale: bool = False, log_as=None):
+  if grayscale:
+    mode = cv2.IMREAD_GRAYSCALE
+  else:
+    mode = cv2.IMREAD_COLOR
+  image = np.fromstring(base64.b64decode(b64bytes), np.uint8)
+  image = cv2.imdecode(image, mode)
+  if image is None:
+    raise ValueError("Cannot decode image.")
+  if log_as is not None:
+    cv2.imwrite("logs/query_images/" + log_as + ".jpg", image)
+  if not grayscale and len(image.shape) == 3:
+    image = image[:, :, ::-1]  # BGR to RGB
+  return image
 
 
 def list_h5_names(path):
-    names = []
-    with h5py.File(str(path), "r", libver="latest") as fd:
-
-        def visit_fn(_, obj):
-            if isinstance(obj, h5py.Dataset):
-                names.append(obj.parent.name.strip("/"))
-
-        fd.visititems(visit_fn)
-    return list(set(names))
-
-
-def get_keypoints(
-    path: Path, name: str, return_uncertainty: bool = False
-) -> np.ndarray:
-    with h5py.File(str(path), "r", libver="latest") as hfile:
-        dset = hfile[name]["keypoints"]
-        p = dset.__array__()
-        uncertainty = dset.attrs.get("uncertainty")
-    if return_uncertainty:
-        return p, uncertainty
-    return p
+  names = []
+  with h5py.File(str(path), 'r', libver='latest') as fd:
+    def visit_fn(_, obj):
+      if isinstance(obj, h5py.Dataset):
+        names.append(obj.parent.name.strip("/"))
+
+    fd.visititems(visit_fn)
+  return list(set(names))
+
+
+def get_keypoints(path: Path, name: str,
+                  return_uncertainty: bool = False) -> np.ndarray:
+  with h5py.File(str(path), 'r', libver='latest') as hfile:
+    dset = hfile[name]['keypoints']
+    p = dset.__array__()
+    uncertainty = dset.attrs.get("uncertainty")
+  if return_uncertainty:
+    return p, uncertainty
+  return p
 
 
 def find_pair(hfile: h5py.File, name0: str, name1: str):
-    pair = names_to_pair(name0, name1)
-    if pair in hfile:
-        return pair, False
-    pair = names_to_pair(name1, name0)
-    if pair in hfile:
-        return pair, True
-    # older, less efficient format
-    pair = names_to_pair_old(name0, name1)
-    if pair in hfile:
-        return pair, False
-    pair = names_to_pair_old(name1, name0)
-    if pair in hfile:
-        return pair, True
-    raise ValueError(
-        f"Could not find pair {(name0, name1)}... "
-        "Maybe you matched with a different list of pairs? "
-    )
+  pair = names_to_pair(name0, name1)
+  if pair in hfile:
+    return pair, False
+  pair = names_to_pair(name1, name0)
+  if pair in hfile:
+    return pair, True
+  # older, less efficient format
+  pair = names_to_pair_old(name0, name1)
+  if pair in hfile:
+    return pair, False
+  pair = names_to_pair_old(name1, name0)
+  if pair in hfile:
+    return pair, True
+  raise ValueError(
+      f"Could not find pair {(name0, name1)}... "
+      "Maybe you matched with a different list of pairs? "
+  )
 
 
 def get_matches(path: Path, name0: str, name1: str) -> Tuple[np.ndarray]:
-    with h5py.File(str(path), "r", libver="latest") as hfile:
-        pair, reverse = find_pair(hfile, name0, name1)
-        matches = hfile[pair]["matches0"].__array__()
-        scores = hfile[pair]["matching_scores0"].__array__()
-    idx = np.where(matches != -1)[0]
-    matches = np.stack([idx, matches[idx]], -1)
-    if reverse:
-        matches = np.flip(matches, -1)
-    scores = scores[idx]
-    return matches, scores
-
-
-def write_poses(
-    poses: Mapping[str, pycolmap.Rigid3d], path: str, prepend_camera_name: bool
-):
-    with open(path, "w") as f:
-        for query, t in poses.items():
-            qvec = " ".join(map(str, t.rotation.quat[[3, 0, 1, 2]]))
-            tvec = " ".join(map(str, t.translation))
-            name = query.split("/")[-1]
-            if prepend_camera_name:
-                name = query.split("/")[-2] + "/" + name
-            f.write(f"{name} {qvec} {tvec}\n")
+  with h5py.File(str(path), 'r', libver='latest') as hfile:
+    pair, reverse = find_pair(hfile, name0, name1)
+    matches = hfile[pair]["matches0"].__array__()
+    scores = hfile[pair]["matching_scores0"].__array__()
+  idx = np.where(matches != -1)[0]
+  matches = np.stack([idx, matches[idx]], -1)
+  if reverse:
+    matches = np.flip(matches, -1)
+  scores = scores[idx]
+  return matches, scores
diff -Naur '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=pipelines' '--exclude=extractors' '--exclude=matchers' /tmp/hloc-latest/hloc/utils/parsers.py hloc/utils/parsers.py
--- /tmp/hloc-latest/hloc/utils/parsers.py	2026-01-16 16:13:56.815239491 -0700
+++ hloc/utils/parsers.py	2026-01-16 16:16:08.547029394 -0700
@@ -1,59 +1,59 @@
-import logging
-from collections import defaultdict
-from pathlib import Path
+# SPDX-FileCopyrightText: (C) 2025 Intel Corporation
+# SPDX-License-Identifier: Apache-2.0
 
+from pathlib import Path
+import logging
 import numpy as np
+from collections import defaultdict
 import pycolmap
 
 logger = logging.getLogger(__name__)
 
 
 def parse_image_list(path, with_intrinsics=False):
-    images = []
-    with open(path, "r") as f:
-        for line in f:
-            line = line.strip("\n")
-            if len(line) == 0 or line[0] == "#":
-                continue
-            name, *data = line.split()
-            if with_intrinsics:
-                model, width, height, *params = data
-                params = np.array(params, float)
-                cam = pycolmap.Camera(
-                    model=model, width=int(width), height=int(height), params=params
-                )
-                images.append((name, cam))
-            else:
-                images.append(name)
-
-    assert len(images) > 0
-    logger.info(f"Imported {len(images)} images from {path.name}")
-    return images
+  images = []
+  with open(path, 'r') as f:
+    for line in f:
+      line = line.strip('\n')
+      if len(line) == 0 or line[0] == '#':
+        continue
+      name, *data = line.split()
+      if with_intrinsics:
+        model, width, height, *params = data
+        params = np.array(params, float)
+        cam = pycolmap.Camera(model, int(width), int(height), params)
+        images.append((name, cam))
+      else:
+        images.append(name)
+
+  assert len(images) > 0
+  logger.info(f'Imported {len(images)} images from {path.name}')
+  return images
 
 
 def parse_image_lists(paths, with_intrinsics=False):
-    images = []
-    files = list(Path(paths.parent).glob(paths.name))
-    assert len(files) > 0
-    for lfile in files:
-        images += parse_image_list(lfile, with_intrinsics=with_intrinsics)
-    return images
+  images = []
+  files = list(Path(paths.parent).glob(paths.name))
+  assert len(files) > 0
+  for lfile in files:
+    images += parse_image_list(lfile, with_intrinsics=with_intrinsics)
+  return images
 
 
 def parse_retrieval(path):
-    retrieval = defaultdict(list)
-    with open(path, "r") as f:
-        for p in f.read().rstrip("\n").split("\n"):
-            if len(p) == 0:
-                continue
-            q, r = p.split()
-            retrieval[q].append(r)
-    return dict(retrieval)
+  retrieval = defaultdict(list)
+  with open(path, 'r') as f:
+    for p in f.read().rstrip('\n').split('\n'):
+      if len(p) == 0:
+        continue
+      q, r = p.split()
+      retrieval[q].append(r)
+  return dict(retrieval)
 
 
-def names_to_pair(name0, name1, separator="/"):
-    return separator.join((name0.replace("/", "-"), name1.replace("/", "-")))
+def names_to_pair(name0, name1, separator='/'):
+  return separator.join((name0.replace('/', '-'), name1.replace('/', '-')))
 
 
 def names_to_pair_old(name0, name1):
-    return names_to_pair(name0, name1, separator="_")
+  return names_to_pair(name0, name1, separator='_')
diff -Naur '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=pipelines' '--exclude=extractors' '--exclude=matchers' /tmp/hloc-latest/hloc/utils/read_write_model.py hloc/utils/read_write_model.py
--- /tmp/hloc-latest/hloc/utils/read_write_model.py	2026-01-16 16:13:56.815239491 -0700
+++ hloc/utils/read_write_model.py	2026-01-16 16:16:08.547284932 -0700
@@ -1,41 +1,13 @@
-# Copyright (c) 2018, ETH Zurich and UNC Chapel Hill.
-# All rights reserved.
-#
-# Redistribution and use in source and binary forms, with or without
-# modification, are permitted provided that the following conditions are met:
-#
-#     * Redistributions of source code must retain the above copyright
-#       notice, this list of conditions and the following disclaimer.
-#
-#     * Redistributions in binary form must reproduce the above copyright
-#       notice, this list of conditions and the following disclaimer in the
-#       documentation and/or other materials provided with the distribution.
-#
-#     * Neither the name of ETH Zurich and UNC Chapel Hill nor the names of
-#       its contributors may be used to endorse or promote products derived
-#       from this software without specific prior written permission.
-#
-# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
-# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
-# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
-# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE
-# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
-# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
-# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
-# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
-# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
-# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
-# POSSIBILITY OF SUCH DAMAGE.
-#
-# Author: Johannes L. Schoenberger (jsch-at-demuc-dot-de)
+# SPDX-FileCopyrightText: (C) 2025 Intel Corporation
+# SPDX-FileCopyrightText: 2018 ETH Zurich and UNC Chapel Hill.
+# SPDX-License-Identifier: Apache-2.0
 
-import argparse
-import collections
-import logging
 import os
-import struct
-
+import collections
 import numpy as np
+import struct
+import argparse
+import logging
 
 logger = logging.getLogger(__name__)
 
@@ -53,8 +25,8 @@
 
 
 class Image(BaseImage):
-    def qvec2rotmat(self):
-        return qvec2rotmat(self.qvec)
+  def qvec2rotmat(self):
+    return qvec2rotmat(self.qvec)
 
 
 CAMERA_MODELS = {
@@ -79,510 +51,524 @@
 
 
 def read_next_bytes(fid, num_bytes, format_char_sequence, endian_character="<"):
-    """Read and unpack the next bytes from a binary file.
-    :param fid:
-    :param num_bytes: Sum of combination of {2, 4, 8}, e.g. 2, 6, 16, 30, etc.
-    :param format_char_sequence: List of {c, e, f, d, h, H, i, I, l, L, q, Q}.
-    :param endian_character: Any of {@, =, <, >, !}
-    :return: Tuple of read and unpacked values.
-    """
-    data = fid.read(num_bytes)
-    return struct.unpack(endian_character + format_char_sequence, data)
+  """Read and unpack the next bytes from a binary file.
+  :param fid:
+  :param num_bytes: Sum of combination of {2, 4, 8}, e.g. 2, 6, 16, 30, etc.
+  :param format_char_sequence: List of {c, e, f, d, h, H, i, I, l, L, q, Q}.
+  :param endian_character: Any of {@, =, <, >, !}
+  :return: Tuple of read and unpacked values.
+  """
+  data = fid.read(num_bytes)
+  return struct.unpack(endian_character + format_char_sequence, data)
 
 
 def write_next_bytes(fid, data, format_char_sequence, endian_character="<"):
-    """pack and write to a binary file.
-    :param fid:
-    :param data: data to send, if multiple elements are sent at the same time,
-    they should be encapsuled either in a list or a tuple
-    :param format_char_sequence: List of {c, e, f, d, h, H, i, I, l, L, q, Q}.
-    should be the same length as the data list or tuple
-    :param endian_character: Any of {@, =, <, >, !}
-    """
-    if isinstance(data, (list, tuple)):
-        bytes = struct.pack(endian_character + format_char_sequence, *data)
-    else:
-        bytes = struct.pack(endian_character + format_char_sequence, data)
-    fid.write(bytes)
+  """pack and write to a binary file.
+  :param fid:
+  :param data: data to send, if multiple elements are sent at the same time,
+  they should be encapsuled either in a list or a tuple
+  :param format_char_sequence: List of {c, e, f, d, h, H, i, I, l, L, q, Q}.
+  should be the same length as the data list or tuple
+  :param endian_character: Any of {@, =, <, >, !}
+  """
+  if isinstance(data, (list, tuple)):
+    bytes = struct.pack(endian_character + format_char_sequence, *data)
+  else:
+    bytes = struct.pack(endian_character + format_char_sequence, data)
+  fid.write(bytes)
 
 
 def read_cameras_text(path):
-    """
-    see: src/base/reconstruction.cc
-        void Reconstruction::WriteCamerasText(const std::string& path)
-        void Reconstruction::ReadCamerasText(const std::string& path)
-    """
-    cameras = {}
-    with open(path, "r") as fid:
-        while True:
-            line = fid.readline()
-            if not line:
-                break
-            line = line.strip()
-            if len(line) > 0 and line[0] != "#":
-                elems = line.split()
-                camera_id = int(elems[0])
-                model = elems[1]
-                width = int(elems[2])
-                height = int(elems[3])
-                params = np.array(tuple(map(float, elems[4:])))
-                cameras[camera_id] = Camera(
-                    id=camera_id, model=model, width=width, height=height, params=params
-                )
-    return cameras
+  """
+  see: src/base/reconstruction.cc
+      void Reconstruction::WriteCamerasText(const std::string& path)
+      void Reconstruction::ReadCamerasText(const std::string& path)
+  """
+  cameras = {}
+  with open(path, "r") as fid:
+    while True:
+      line = fid.readline()
+      if not line:
+        break
+      line = line.strip()
+      if len(line) > 0 and line[0] != "#":
+        elems = line.split()
+        camera_id = int(elems[0])
+        model = elems[1]
+        width = int(elems[2])
+        height = int(elems[3])
+        params = np.array(tuple(map(float, elems[4:])))
+        cameras[camera_id] = Camera(
+            id=camera_id, model=model, width=width, height=height, params=params
+        )
+  return cameras
 
 
 def read_cameras_binary(path_to_model_file):
-    """
-    see: src/base/reconstruction.cc
-        void Reconstruction::WriteCamerasBinary(const std::string& path)
-        void Reconstruction::ReadCamerasBinary(const std::string& path)
-    """
-    cameras = {}
-    with open(path_to_model_file, "rb") as fid:
-        num_cameras = read_next_bytes(fid, 8, "Q")[0]
-        for _ in range(num_cameras):
-            camera_properties = read_next_bytes(
-                fid, num_bytes=24, format_char_sequence="iiQQ"
-            )
-            camera_id = camera_properties[0]
-            model_id = camera_properties[1]
-            model_name = CAMERA_MODEL_IDS[camera_properties[1]].model_name
-            width = camera_properties[2]
-            height = camera_properties[3]
-            num_params = CAMERA_MODEL_IDS[model_id].num_params
-            params = read_next_bytes(
-                fid, num_bytes=8 * num_params, format_char_sequence="d" * num_params
-            )
-            cameras[camera_id] = Camera(
-                id=camera_id,
-                model=model_name,
-                width=width,
-                height=height,
-                params=np.array(params),
-            )
-        assert len(cameras) == num_cameras
-    return cameras
+  """
+  see: src/base/reconstruction.cc
+      void Reconstruction::WriteCamerasBinary(const std::string& path)
+      void Reconstruction::ReadCamerasBinary(const std::string& path)
+  """
+  cameras = {}
+  with open(path_to_model_file, "rb") as fid:
+    num_cameras = read_next_bytes(fid, 8, "Q")[0]
+    for _ in range(num_cameras):
+      camera_properties = read_next_bytes(
+          fid, num_bytes=24, format_char_sequence="iiQQ"
+      )
+      camera_id = camera_properties[0]
+      model_id = camera_properties[1]
+      model_name = CAMERA_MODEL_IDS[camera_properties[1]].model_name
+      width = camera_properties[2]
+      height = camera_properties[3]
+      num_params = CAMERA_MODEL_IDS[model_id].num_params
+      params = read_next_bytes(
+          fid, num_bytes=8 * num_params, format_char_sequence="d" * num_params
+      )
+      cameras[camera_id] = Camera(
+          id=camera_id,
+          model=model_name,
+          width=width,
+          height=height,
+          params=np.array(params),
+      )
+    assert len(cameras) == num_cameras
+  return cameras
 
 
 def write_cameras_text(cameras, path):
-    """
-    see: src/base/reconstruction.cc
-        void Reconstruction::WriteCamerasText(const std::string& path)
-        void Reconstruction::ReadCamerasText(const std::string& path)
-    """
-    HEADER = (
-        "# Camera list with one line of data per camera:\n"
-        + "#   CAMERA_ID, MODEL, WIDTH, HEIGHT, PARAMS[]\n"
-        + "# Number of cameras: {}\n".format(len(cameras))
-    )
-    with open(path, "w") as fid:
-        fid.write(HEADER)
-        for _, cam in cameras.items():
-            to_write = [cam.id, cam.model, cam.width, cam.height, *cam.params]
-            line = " ".join([str(elem) for elem in to_write])
-            fid.write(line + "\n")
+  """
+  see: src/base/reconstruction.cc
+      void Reconstruction::WriteCamerasText(const std::string& path)
+      void Reconstruction::ReadCamerasText(const std::string& path)
+  """
+  HEADER = (
+      "# Camera list with one line of data per camera:\n"
+      + "#   CAMERA_ID, MODEL, WIDTH, HEIGHT, PARAMS[]\n"
+      + "# Number of cameras: {}\n".format(len(cameras))
+  )
+  with open(path, "w") as fid:
+    fid.write(HEADER)
+    for _, cam in cameras.items():
+      to_write = [cam.id, cam.model, cam.width, cam.height, *cam.params]
+      line = " ".join([str(elem) for elem in to_write])
+      fid.write(line + "\n")
 
 
 def write_cameras_binary(cameras, path_to_model_file):
-    """
-    see: src/base/reconstruction.cc
-        void Reconstruction::WriteCamerasBinary(const std::string& path)
-        void Reconstruction::ReadCamerasBinary(const std::string& path)
-    """
-    with open(path_to_model_file, "wb") as fid:
-        write_next_bytes(fid, len(cameras), "Q")
-        for _, cam in cameras.items():
-            model_id = CAMERA_MODEL_NAMES[cam.model].model_id
-            camera_properties = [cam.id, model_id, cam.width, cam.height]
-            write_next_bytes(fid, camera_properties, "iiQQ")
-            for p in cam.params:
-                write_next_bytes(fid, float(p), "d")
-    return cameras
+  """
+  see: src/base/reconstruction.cc
+      void Reconstruction::WriteCamerasBinary(const std::string& path)
+      void Reconstruction::ReadCamerasBinary(const std::string& path)
+  """
+  with open(path_to_model_file, "wb") as fid:
+    write_next_bytes(fid, len(cameras), "Q")
+    for _, cam in cameras.items():
+      model_id = CAMERA_MODEL_NAMES[cam.model].model_id
+      camera_properties = [cam.id, model_id, cam.width, cam.height]
+      write_next_bytes(fid, camera_properties, "iiQQ")
+      for p in cam.params:
+        write_next_bytes(fid, float(p), "d")
+  return cameras
 
 
 def read_images_text(path):
-    """
-    see: src/base/reconstruction.cc
-        void Reconstruction::ReadImagesText(const std::string& path)
-        void Reconstruction::WriteImagesText(const std::string& path)
-    """
-    images = {}
-    with open(path, "r") as fid:
-        while True:
-            line = fid.readline()
-            if not line:
-                break
-            line = line.strip()
-            if len(line) > 0 and line[0] != "#":
-                elems = line.split()
-                image_id = int(elems[0])
-                qvec = np.array(tuple(map(float, elems[1:5])))
-                tvec = np.array(tuple(map(float, elems[5:8])))
-                camera_id = int(elems[8])
-                image_name = elems[9]
-                elems = fid.readline().split()
-                xys = np.column_stack(
-                    [tuple(map(float, elems[0::3])), tuple(map(float, elems[1::3]))]
-                )
-                point3D_ids = np.array(tuple(map(int, elems[2::3])))
-                images[image_id] = Image(
-                    id=image_id,
-                    qvec=qvec,
-                    tvec=tvec,
-                    camera_id=camera_id,
-                    name=image_name,
-                    xys=xys,
-                    point3D_ids=point3D_ids,
-                )
-    return images
+  """
+  see: src/base/reconstruction.cc
+      void Reconstruction::ReadImagesText(const std::string& path)
+      void Reconstruction::WriteImagesText(const std::string& path)
+  """
+  images = {}
+  with open(path, "r") as fid:
+    while True:
+      line = fid.readline()
+      if not line:
+        break
+      line = line.strip()
+      if len(line) > 0 and line[0] != "#":
+        elems = line.split()
+        image_id = int(elems[0])
+        qvec = np.array(tuple(map(float, elems[1:5])))
+        tvec = np.array(tuple(map(float, elems[5:8])))
+        camera_id = int(elems[8])
+        image_name = elems[9]
+        elems = fid.readline().split()
+        xys = np.column_stack(
+            [tuple(map(float, elems[0::3])), tuple(map(float, elems[1::3]))]
+        )
+        point3D_ids = np.array(tuple(map(int, elems[2::3])))
+        images[image_id] = Image(
+            id=image_id,
+            qvec=qvec,
+            tvec=tvec,
+            camera_id=camera_id,
+            name=image_name,
+            xys=xys,
+            point3D_ids=point3D_ids,
+        )
+  return images
 
 
 def read_images_binary(path_to_model_file):
-    """
-    see: src/base/reconstruction.cc
-        void Reconstruction::ReadImagesBinary(const std::string& path)
-        void Reconstruction::WriteImagesBinary(const std::string& path)
-    """
-    images = {}
-    with open(path_to_model_file, "rb") as fid:
-        num_reg_images = read_next_bytes(fid, 8, "Q")[0]
-        for _ in range(num_reg_images):
-            binary_image_properties = read_next_bytes(
-                fid, num_bytes=64, format_char_sequence="idddddddi"
-            )
-            image_id = binary_image_properties[0]
-            qvec = np.array(binary_image_properties[1:5])
-            tvec = np.array(binary_image_properties[5:8])
-            camera_id = binary_image_properties[8]
-            image_name = ""
-            current_char = read_next_bytes(fid, 1, "c")[0]
-            while current_char != b"\x00":  # look for the ASCII 0 entry
-                image_name += current_char.decode("utf-8")
-                current_char = read_next_bytes(fid, 1, "c")[0]
-            num_points2D = read_next_bytes(fid, num_bytes=8, format_char_sequence="Q")[
-                0
-            ]
-            x_y_id_s = read_next_bytes(
-                fid,
-                num_bytes=24 * num_points2D,
-                format_char_sequence="ddq" * num_points2D,
-            )
-            xys = np.column_stack(
-                [tuple(map(float, x_y_id_s[0::3])), tuple(map(float, x_y_id_s[1::3]))]
-            )
-            point3D_ids = np.array(tuple(map(int, x_y_id_s[2::3])))
-            images[image_id] = Image(
-                id=image_id,
-                qvec=qvec,
-                tvec=tvec,
-                camera_id=camera_id,
-                name=image_name,
-                xys=xys,
-                point3D_ids=point3D_ids,
-            )
-    return images
+  """
+  see: src/base/reconstruction.cc
+      void Reconstruction::ReadImagesBinary(const std::string& path)
+      void Reconstruction::WriteImagesBinary(const std::string& path)
+  """
+  images = {}
+  with open(path_to_model_file, "rb") as fid:
+    num_reg_images = read_next_bytes(fid, 8, "Q")[0]
+    for _ in range(num_reg_images):
+      binary_image_properties = read_next_bytes(
+          fid, num_bytes=64, format_char_sequence="idddddddi"
+      )
+      image_id = binary_image_properties[0]
+      qvec = np.array(binary_image_properties[1:5])
+      tvec = np.array(binary_image_properties[5:8])
+      camera_id = binary_image_properties[8]
+      image_name = ""
+      current_char = read_next_bytes(fid, 1, "c")[0]
+      while current_char != b"\x00":  # look for the ASCII 0 entry
+        image_name += current_char.decode("utf-8")
+        current_char = read_next_bytes(fid, 1, "c")[0]
+      num_points2D = read_next_bytes(fid, num_bytes=8, format_char_sequence="Q")[
+          0
+      ]
+      x_y_id_s = read_next_bytes(
+          fid,
+          num_bytes=24 * num_points2D,
+          format_char_sequence="ddq" * num_points2D,
+      )
+      xys = np.column_stack(
+          [tuple(map(float, x_y_id_s[0::3])), tuple(map(float, x_y_id_s[1::3]))]
+      )
+      point3D_ids = np.array(tuple(map(int, x_y_id_s[2::3])))
+      images[image_id] = Image(
+          id=image_id,
+          qvec=qvec,
+          tvec=tvec,
+          camera_id=camera_id,
+          name=image_name,
+          xys=xys,
+          point3D_ids=point3D_ids,
+      )
+  return images
 
 
 def write_images_text(images, path):
-    """
-    see: src/base/reconstruction.cc
-        void Reconstruction::ReadImagesText(const std::string& path)
-        void Reconstruction::WriteImagesText(const std::string& path)
-    """
-    if len(images) == 0:
-        mean_observations = 0
-    else:
-        mean_observations = sum(
-            (len(img.point3D_ids) for _, img in images.items())
-        ) / len(images)
-    HEADER = (
-        "# Image list with two lines of data per image:\n"
-        + "#   IMAGE_ID, QW, QX, QY, QZ, TX, TY, TZ, CAMERA_ID, NAME\n"
-        + "#   POINTS2D[] as (X, Y, POINT3D_ID)\n"
-        + "# Number of images: {}, mean observations per image: {}\n".format(
-            len(images), mean_observations
-        )
-    )
-
-    with open(path, "w") as fid:
-        fid.write(HEADER)
-        for _, img in images.items():
-            image_header = [img.id, *img.qvec, *img.tvec, img.camera_id, img.name]
-            first_line = " ".join(map(str, image_header))
-            fid.write(first_line + "\n")
-
-            points_strings = []
-            for xy, point3D_id in zip(img.xys, img.point3D_ids):
-                points_strings.append(" ".join(map(str, [*xy, point3D_id])))
-            fid.write(" ".join(points_strings) + "\n")
+  """
+  see: src/base/reconstruction.cc
+      void Reconstruction::ReadImagesText(const std::string& path)
+      void Reconstruction::WriteImagesText(const std::string& path)
+  """
+  if len(images) == 0:
+    mean_observations = 0
+  else:
+    mean_observations = sum(
+        (len(img.point3D_ids) for _, img in images.items())
+    ) / len(images)
+  HEADER = (
+      "# Image list with two lines of data per image:\n"
+      + "#   IMAGE_ID, QW, QX, QY, QZ, TX, TY, TZ, CAMERA_ID, NAME\n"
+      + "#   POINTS2D[] as (X, Y, POINT3D_ID)\n"
+      + "# Number of images: {}, mean observations per image: {}\n".format(
+          len(images), mean_observations
+      )
+  )
+
+  with open(path, "w") as fid:
+    fid.write(HEADER)
+    for _, img in images.items():
+      image_header = [img.id, *img.qvec, *img.tvec, img.camera_id, img.name]
+      first_line = " ".join(map(str, image_header))
+      fid.write(first_line + "\n")
+
+      points_strings = []
+      for xy, point3D_id in zip(img.xys, img.point3D_ids):
+        points_strings.append(" ".join(map(str, [*xy, point3D_id])))
+      fid.write(" ".join(points_strings) + "\n")
 
 
 def write_images_binary(images, path_to_model_file):
-    """
-    see: src/base/reconstruction.cc
-        void Reconstruction::ReadImagesBinary(const std::string& path)
-        void Reconstruction::WriteImagesBinary(const std::string& path)
-    """
-    with open(path_to_model_file, "wb") as fid:
-        write_next_bytes(fid, len(images), "Q")
-        for _, img in images.items():
-            write_next_bytes(fid, img.id, "i")
-            write_next_bytes(fid, img.qvec.tolist(), "dddd")
-            write_next_bytes(fid, img.tvec.tolist(), "ddd")
-            write_next_bytes(fid, img.camera_id, "i")
-            for char in img.name:
-                write_next_bytes(fid, char.encode("utf-8"), "c")
-            write_next_bytes(fid, b"\x00", "c")
-            write_next_bytes(fid, len(img.point3D_ids), "Q")
-            for xy, p3d_id in zip(img.xys, img.point3D_ids):
-                write_next_bytes(fid, [*xy, p3d_id], "ddq")
+  """
+  see: src/base/reconstruction.cc
+      void Reconstruction::ReadImagesBinary(const std::string& path)
+      void Reconstruction::WriteImagesBinary(const std::string& path)
+  """
+  with open(path_to_model_file, "wb") as fid:
+    write_next_bytes(fid, len(images), "Q")
+    for _, img in images.items():
+      write_next_bytes(fid, img.id, "i")
+      write_next_bytes(fid, img.qvec.tolist(), "dddd")
+      write_next_bytes(fid, img.tvec.tolist(), "ddd")
+      write_next_bytes(fid, img.camera_id, "i")
+      for char in img.name:
+        write_next_bytes(fid, char.encode("utf-8"), "c")
+      write_next_bytes(fid, b"\x00", "c")
+      write_next_bytes(fid, len(img.point3D_ids), "Q")
+      for xy, p3d_id in zip(img.xys, img.point3D_ids):
+        write_next_bytes(fid, [*xy, p3d_id], "ddq")
 
 
 def read_points3D_text(path):
-    """
-    see: src/base/reconstruction.cc
-        void Reconstruction::ReadPoints3DText(const std::string& path)
-        void Reconstruction::WritePoints3DText(const std::string& path)
-    """
-    points3D = {}
-    with open(path, "r") as fid:
-        while True:
-            line = fid.readline()
-            if not line:
-                break
-            line = line.strip()
-            if len(line) > 0 and line[0] != "#":
-                elems = line.split()
-                point3D_id = int(elems[0])
-                xyz = np.array(tuple(map(float, elems[1:4])))
-                rgb = np.array(tuple(map(int, elems[4:7])))
-                error = float(elems[7])
-                image_ids = np.array(tuple(map(int, elems[8::2])))
-                point2D_idxs = np.array(tuple(map(int, elems[9::2])))
-                points3D[point3D_id] = Point3D(
-                    id=point3D_id,
-                    xyz=xyz,
-                    rgb=rgb,
-                    error=error,
-                    image_ids=image_ids,
-                    point2D_idxs=point2D_idxs,
-                )
-    return points3D
+  """
+  see: src/base/reconstruction.cc
+      void Reconstruction::ReadPoints3DText(const std::string& path)
+      void Reconstruction::WritePoints3DText(const std::string& path)
+  """
+  points3D = {}
+  with open(path, "r") as fid:
+    while True:
+      line = fid.readline()
+      if not line:
+        break
+      line = line.strip()
+      if len(line) > 0 and line[0] != "#":
+        elems = line.split()
+        point3D_id = int(elems[0])
+        xyz = np.array(tuple(map(float, elems[1:4])))
+        rgb = np.array(tuple(map(int, elems[4:7])))
+        error = float(elems[7])
+        image_ids = np.array(tuple(map(int, elems[8::2])))
+        point2D_idxs = np.array(tuple(map(int, elems[9::2])))
+        points3D[point3D_id] = Point3D(
+            id=point3D_id,
+            xyz=xyz,
+            rgb=rgb,
+            error=error,
+            image_ids=image_ids,
+            point2D_idxs=point2D_idxs,
+        )
+  return points3D
 
 
 def read_points3D_binary(path_to_model_file):
-    """
-    see: src/base/reconstruction.cc
-        void Reconstruction::ReadPoints3DBinary(const std::string& path)
-        void Reconstruction::WritePoints3DBinary(const std::string& path)
-    """
-    points3D = {}
-    with open(path_to_model_file, "rb") as fid:
-        num_points = read_next_bytes(fid, 8, "Q")[0]
-        for _ in range(num_points):
-            binary_point_line_properties = read_next_bytes(
-                fid, num_bytes=43, format_char_sequence="QdddBBBd"
-            )
-            point3D_id = binary_point_line_properties[0]
-            xyz = np.array(binary_point_line_properties[1:4])
-            rgb = np.array(binary_point_line_properties[4:7])
-            error = np.array(binary_point_line_properties[7])
-            track_length = read_next_bytes(fid, num_bytes=8, format_char_sequence="Q")[
-                0
-            ]
-            track_elems = read_next_bytes(
-                fid,
-                num_bytes=8 * track_length,
-                format_char_sequence="ii" * track_length,
-            )
-            image_ids = np.array(tuple(map(int, track_elems[0::2])))
-            point2D_idxs = np.array(tuple(map(int, track_elems[1::2])))
-            points3D[point3D_id] = Point3D(
-                id=point3D_id,
-                xyz=xyz,
-                rgb=rgb,
-                error=error,
-                image_ids=image_ids,
-                point2D_idxs=point2D_idxs,
-            )
-    return points3D
+  """
+  see: src/base/reconstruction.cc
+      void Reconstruction::ReadPoints3DBinary(const std::string& path)
+      void Reconstruction::WritePoints3DBinary(const std::string& path)
+  """
+  points3D = {}
+  with open(path_to_model_file, "rb") as fid:
+    num_points = read_next_bytes(fid, 8, "Q")[0]
+    for _ in range(num_points):
+      binary_point_line_properties = read_next_bytes(
+          fid, num_bytes=43, format_char_sequence="QdddBBBd"
+      )
+      point3D_id = binary_point_line_properties[0]
+      xyz = np.array(binary_point_line_properties[1:4])
+      rgb = np.array(binary_point_line_properties[4:7])
+      error = np.array(binary_point_line_properties[7])
+      track_length = read_next_bytes(fid, num_bytes=8, format_char_sequence="Q")[
+          0
+      ]
+      track_elems = read_next_bytes(
+          fid,
+          num_bytes=8 * track_length,
+          format_char_sequence="ii" * track_length,
+      )
+      image_ids = np.array(tuple(map(int, track_elems[0::2])))
+      point2D_idxs = np.array(tuple(map(int, track_elems[1::2])))
+      points3D[point3D_id] = Point3D(
+          id=point3D_id,
+          xyz=xyz,
+          rgb=rgb,
+          error=error,
+          image_ids=image_ids,
+          point2D_idxs=point2D_idxs,
+      )
+  return points3D
 
 
 def write_points3D_text(points3D, path):
-    """
-    see: src/base/reconstruction.cc
-        void Reconstruction::ReadPoints3DText(const std::string& path)
-        void Reconstruction::WritePoints3DText(const std::string& path)
-    """
-    if len(points3D) == 0:
-        mean_track_length = 0
-    else:
-        mean_track_length = sum(
-            (len(pt.image_ids) for _, pt in points3D.items())
-        ) / len(points3D)
-    HEADER = (
-        "# 3D point list with one line of data per point:\n"
-        + "#   POINT3D_ID, X, Y, Z, R, G, B, ERROR, TRACK[] as (IMAGE_ID, POINT2D_IDX)\n"  # noqa: E501
-        + "# Number of points: {}, mean track length: {}\n".format(
-            len(points3D), mean_track_length
-        )
-    )
-
-    with open(path, "w") as fid:
-        fid.write(HEADER)
-        for _, pt in points3D.items():
-            point_header = [pt.id, *pt.xyz, *pt.rgb, pt.error]
-            fid.write(" ".join(map(str, point_header)) + " ")
-            track_strings = []
-            for image_id, point2D in zip(pt.image_ids, pt.point2D_idxs):
-                track_strings.append(" ".join(map(str, [image_id, point2D])))
-            fid.write(" ".join(track_strings) + "\n")
+  """
+  see: src/base/reconstruction.cc
+      void Reconstruction::ReadPoints3DText(const std::string& path)
+      void Reconstruction::WritePoints3DText(const std::string& path)
+  """
+  if len(points3D) == 0:
+    mean_track_length = 0
+  else:
+    mean_track_length = sum(
+        (len(pt.image_ids) for _, pt in points3D.items())
+    ) / len(points3D)
+  HEADER = (
+      "# 3D point list with one line of data per point:\n"
+      + "#   POINT3D_ID, X, Y, Z, R, G, B, ERROR, TRACK[] as (IMAGE_ID, POINT2D_IDX)\n"
+      + "# Number of points: {}, mean track length: {}\n".format(
+          len(points3D), mean_track_length
+      )
+  )
+
+  with open(path, "w") as fid:
+    fid.write(HEADER)
+    for _, pt in points3D.items():
+      point_header = [pt.id, *pt.xyz, *pt.rgb, pt.error]
+      fid.write(" ".join(map(str, point_header)) + " ")
+      track_strings = []
+      for image_id, point2D in zip(pt.image_ids, pt.point2D_idxs):
+        track_strings.append(" ".join(map(str, [image_id, point2D])))
+      fid.write(" ".join(track_strings) + "\n")
 
 
 def write_points3D_binary(points3D, path_to_model_file):
-    """
-    see: src/base/reconstruction.cc
-        void Reconstruction::ReadPoints3DBinary(const std::string& path)
-        void Reconstruction::WritePoints3DBinary(const std::string& path)
-    """
-    with open(path_to_model_file, "wb") as fid:
-        write_next_bytes(fid, len(points3D), "Q")
-        for _, pt in points3D.items():
-            write_next_bytes(fid, pt.id, "Q")
-            write_next_bytes(fid, pt.xyz.tolist(), "ddd")
-            write_next_bytes(fid, pt.rgb.tolist(), "BBB")
-            write_next_bytes(fid, pt.error, "d")
-            track_length = pt.image_ids.shape[0]
-            write_next_bytes(fid, track_length, "Q")
-            for image_id, point2D_id in zip(pt.image_ids, pt.point2D_idxs):
-                write_next_bytes(fid, [image_id, point2D_id], "ii")
+  """
+  see: src/base/reconstruction.cc
+      void Reconstruction::ReadPoints3DBinary(const std::string& path)
+      void Reconstruction::WritePoints3DBinary(const std::string& path)
+  """
+  with open(path_to_model_file, "wb") as fid:
+    write_next_bytes(fid, len(points3D), "Q")
+    for _, pt in points3D.items():
+      write_next_bytes(fid, pt.id, "Q")
+      write_next_bytes(fid, pt.xyz.tolist(), "ddd")
+      write_next_bytes(fid, pt.rgb.tolist(), "BBB")
+      write_next_bytes(fid, pt.error, "d")
+      track_length = pt.image_ids.shape[0]
+      write_next_bytes(fid, track_length, "Q")
+      for image_id, point2D_id in zip(pt.image_ids, pt.point2D_idxs):
+        write_next_bytes(fid, [image_id, point2D_id], "ii")
+
+
+def write_points3D_file(points3D, file_path, scale=1.0):
+  from open3d.t.geometry import PointCloud
+  from open3d.t.io import write_point_cloud
+
+  xyz = np.empty((len(points3D), 3), dtype=np.float32)
+  rgb = np.empty((len(points3D), 3), dtype=np.uint8)
+  for i, pt in enumerate(points3D.values()):
+    xyz[i] = pt.xyz * scale
+    rgb[i] = pt.rgb
+  write_point_cloud(str(file_path), PointCloud({"positions": xyz, "colors": rgb}))
 
 
 def detect_model_format(path, ext):
-    if (
-        os.path.isfile(os.path.join(path, "cameras" + ext))
-        and os.path.isfile(os.path.join(path, "images" + ext))
-        and os.path.isfile(os.path.join(path, "points3D" + ext))
-    ):
-        return True
+  if (
+      os.path.isfile(os.path.join(path, "cameras" + ext))
+      and os.path.isfile(os.path.join(path, "images" + ext))
+      and os.path.isfile(os.path.join(path, "points3D" + ext))
+  ):
+    return True
 
-    return False
+  return False
 
 
 def read_model(path, ext=""):
-    # try to detect the extension automatically
-    if ext == "":
-        if detect_model_format(path, ".bin"):
-            ext = ".bin"
-        elif detect_model_format(path, ".txt"):
-            ext = ".txt"
-        else:
-            try:
-                cameras, images, points3D = read_model(os.path.join(path, "model/"))
-                logger.warning("This SfM file structure was deprecated in hloc v1.1")
-                return cameras, images, points3D
-            except FileNotFoundError:
-                raise FileNotFoundError(
-                    f"Could not find binary or text COLMAP model at {path}"
-                )
-
-    if ext == ".txt":
-        cameras = read_cameras_text(os.path.join(path, "cameras" + ext))
-        images = read_images_text(os.path.join(path, "images" + ext))
-        points3D = read_points3D_text(os.path.join(path, "points3D") + ext)
+  if not os.path.isdir(path):
+    raise NotADirectoryError(f"Path {path} is not a directory")
+  # try to detect the extension automatically
+  if ext == "":
+    if detect_model_format(path, ".bin"):
+      ext = ".bin"
+    elif detect_model_format(path, ".txt"):
+      ext = ".txt"
     else:
-        cameras = read_cameras_binary(os.path.join(path, "cameras" + ext))
-        images = read_images_binary(os.path.join(path, "images" + ext))
-        points3D = read_points3D_binary(os.path.join(path, "points3D") + ext)
-    return cameras, images, points3D
+      try:
+        cameras, images, points3D = read_model(os.path.join(path, "model/"))
+        logger.warning("This SfM file structure was deprecated in hloc v1.1")
+        return cameras, images, points3D
+      except FileNotFoundError:
+        raise FileNotFoundError(
+            f"Could not find binary or text COLMAP model at {path}"
+        )
+
+  if ext == ".txt":
+    cameras = read_cameras_text(os.path.join(path, "cameras" + ext))
+    images = read_images_text(os.path.join(path, "images" + ext))
+    points3D = read_points3D_text(os.path.join(path, "points3D") + ext)
+  else:
+    cameras = read_cameras_binary(os.path.join(path, "cameras" + ext))
+    images = read_images_binary(os.path.join(path, "images" + ext))
+    points3D = read_points3D_binary(os.path.join(path, "points3D") + ext)
+  return cameras, images, points3D
 
 
 def write_model(cameras, images, points3D, path, ext=".bin"):
-    if ext == ".txt":
-        write_cameras_text(cameras, os.path.join(path, "cameras" + ext))
-        write_images_text(images, os.path.join(path, "images" + ext))
-        write_points3D_text(points3D, os.path.join(path, "points3D") + ext)
-    else:
-        write_cameras_binary(cameras, os.path.join(path, "cameras" + ext))
-        write_images_binary(images, os.path.join(path, "images" + ext))
-        write_points3D_binary(points3D, os.path.join(path, "points3D") + ext)
-    return cameras, images, points3D
+  if ext == ".txt":
+    write_cameras_text(cameras, os.path.join(path, "cameras" + ext))
+    write_images_text(images, os.path.join(path, "images" + ext))
+    write_points3D_text(points3D, os.path.join(path, "points3D") + ext)
+  else:
+    write_cameras_binary(cameras, os.path.join(path, "cameras" + ext))
+    write_images_binary(images, os.path.join(path, "images" + ext))
+    write_points3D_binary(points3D, os.path.join(path, "points3D") + ext)
+  return cameras, images, points3D
 
 
 def qvec2rotmat(qvec):
-    return np.array(
-        [
-            [
-                1 - 2 * qvec[2] ** 2 - 2 * qvec[3] ** 2,
-                2 * qvec[1] * qvec[2] - 2 * qvec[0] * qvec[3],
-                2 * qvec[3] * qvec[1] + 2 * qvec[0] * qvec[2],
-            ],
-            [
-                2 * qvec[1] * qvec[2] + 2 * qvec[0] * qvec[3],
-                1 - 2 * qvec[1] ** 2 - 2 * qvec[3] ** 2,
-                2 * qvec[2] * qvec[3] - 2 * qvec[0] * qvec[1],
-            ],
-            [
-                2 * qvec[3] * qvec[1] - 2 * qvec[0] * qvec[2],
-                2 * qvec[2] * qvec[3] + 2 * qvec[0] * qvec[1],
-                1 - 2 * qvec[1] ** 2 - 2 * qvec[2] ** 2,
-            ],
-        ]
-    )
+  return np.array(
+      [
+          [
+              1 - 2 * qvec[2] ** 2 - 2 * qvec[3] ** 2,
+              2 * qvec[1] * qvec[2] - 2 * qvec[0] * qvec[3],
+              2 * qvec[3] * qvec[1] + 2 * qvec[0] * qvec[2],
+          ],
+          [
+              2 * qvec[1] * qvec[2] + 2 * qvec[0] * qvec[3],
+              1 - 2 * qvec[1] ** 2 - 2 * qvec[3] ** 2,
+              2 * qvec[2] * qvec[3] - 2 * qvec[0] * qvec[1],
+          ],
+          [
+              2 * qvec[3] * qvec[1] - 2 * qvec[0] * qvec[2],
+              2 * qvec[2] * qvec[3] + 2 * qvec[0] * qvec[1],
+              1 - 2 * qvec[1] ** 2 - 2 * qvec[2] ** 2,
+          ],
+      ]
+  )
 
 
 def rotmat2qvec(R):
-    Rxx, Ryx, Rzx, Rxy, Ryy, Rzy, Rxz, Ryz, Rzz = R.flat
-    K = (
-        np.array(
-            [
-                [Rxx - Ryy - Rzz, 0, 0, 0],
-                [Ryx + Rxy, Ryy - Rxx - Rzz, 0, 0],
-                [Rzx + Rxz, Rzy + Ryz, Rzz - Rxx - Ryy, 0],
-                [Ryz - Rzy, Rzx - Rxz, Rxy - Ryx, Rxx + Ryy + Rzz],
-            ]
-        )
-        / 3.0
-    )
-    eigvals, eigvecs = np.linalg.eigh(K)
-    qvec = eigvecs[[3, 0, 1, 2], np.argmax(eigvals)]
-    if qvec[0] < 0:
-        qvec *= -1
-    return qvec
+  Rxx, Ryx, Rzx, Rxy, Ryy, Rzy, Rxz, Ryz, Rzz = R.flat
+  K = (
+      np.array(
+          [
+              [Rxx - Ryy - Rzz, 0, 0, 0],
+              [Ryx + Rxy, Ryy - Rxx - Rzz, 0, 0],
+              [Rzx + Rxz, Rzy + Ryz, Rzz - Rxx - Ryy, 0],
+              [Ryz - Rzy, Rzx - Rxz, Rxy - Ryx, Rxx + Ryy + Rzz],
+          ]
+      )
+      / 3.0
+  )
+  eigvals, eigvecs = np.linalg.eigh(K)
+  qvec = eigvecs[[3, 0, 1, 2], np.argmax(eigvals)]
+  if qvec[0] < 0:
+    qvec *= -1
+  return qvec
 
 
 def main():
-    parser = argparse.ArgumentParser(
-        description="Read and write COLMAP binary and text models"
-    )
-    parser.add_argument("--input_model", help="path to input model folder")
-    parser.add_argument(
-        "--input_format",
-        choices=[".bin", ".txt"],
-        help="input model format",
-        default="",
-    )
-    parser.add_argument("--output_model", help="path to output model folder")
-    parser.add_argument(
-        "--output_format",
-        choices=[".bin", ".txt"],
-        help="outut model format",
-        default=".txt",
+  parser = argparse.ArgumentParser(
+      description="Read and write COLMAP binary and text models"
+  )
+  parser.add_argument("--input_model", help="path to input model folder")
+  parser.add_argument(
+      "--input_format",
+      choices=[".bin", ".txt"],
+      help="input model format",
+      default="",
+  )
+  parser.add_argument("--output_model", help="path to output model folder")
+  parser.add_argument(
+      "--output_format",
+      choices=[".bin", ".txt"],
+      help="outut model format",
+      default=".txt",
+  )
+  args = parser.parse_args()
+
+  cameras, images, points3D = read_model(path=args.input_model, ext=args.input_format)
+
+  print("num_cameras:", len(cameras))
+  print("num_images:", len(images))
+  print("num_points3D:", len(points3D))
+
+  if args.output_model is not None:
+    write_model(
+        cameras, images, points3D, path=args.output_model, ext=args.output_format
     )
-    args = parser.parse_args()
-
-    cameras, images, points3D = read_model(path=args.input_model, ext=args.input_format)
-
-    print("num_cameras:", len(cameras))
-    print("num_images:", len(images))
-    print("num_points3D:", len(points3D))
-
-    if args.output_model is not None:
-        write_model(
-            cameras, images, points3D, path=args.output_model, ext=args.output_format
-        )
 
 
 if __name__ == "__main__":
-    main()
+  main()
diff -Naur '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=pipelines' '--exclude=extractors' '--exclude=matchers' /tmp/hloc-latest/hloc/utils/tools.py hloc/utils/tools.py
--- /tmp/hloc-latest/hloc/utils/tools.py	1969-12-31 17:00:00.000000000 -0700
+++ hloc/utils/tools.py	2026-01-16 16:16:08.547418686 -0700
@@ -0,0 +1,20 @@
+# SPDX-FileCopyrightText: (C) 2025 Intel Corporation
+# SPDX-License-Identifier: Apache-2.0
+
+import collections.abc as collections
+import torch
+string_classes = str
+
+
+def map_tensor(input_, func):
+  if isinstance(input_, torch.Tensor):
+    return func(input_)
+  elif isinstance(input_, string_classes):
+    return input_
+  elif isinstance(input_, collections.Mapping):
+    return {k: map_tensor(sample, func) for k, sample in input_.items()}
+  elif isinstance(input_, collections.Sequence):
+    return [map_tensor(sample, func) for sample in input_]
+  else:
+    raise TypeError(
+        f'input must be tensor, dict or list; found {type(input_)}')
diff -Naur '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=pipelines' '--exclude=extractors' '--exclude=matchers' /tmp/hloc-latest/hloc/utils/viz_3d.py hloc/utils/viz_3d.py
--- /tmp/hloc-latest/hloc/utils/viz_3d.py	2026-01-16 16:13:56.815239491 -0700
+++ hloc/utils/viz_3d.py	1969-12-31 17:00:00.000000000 -0700
@@ -1,212 +0,0 @@
-"""
-3D visualization based on plotly.
-Works for a small number of points and cameras, might be slow otherwise.
-
-1) Initialize a figure with `init_figure`
-2) Add 3D points, camera frustums, or both as a pycolmap.Reconstruction
-
-Written by Paul-Edouard Sarlin and Philipp Lindenberger.
-"""
-
-from typing import Optional
-
-import numpy as np
-import plotly.graph_objects as go
-import pycolmap
-
-
-def to_homogeneous(points):
-    pad = np.ones((points.shape[:-1] + (1,)), dtype=points.dtype)
-    return np.concatenate([points, pad], axis=-1)
-
-
-def init_figure(height: int = 800) -> go.Figure:
-    """Initialize a 3D figure."""
-    fig = go.Figure()
-    axes = dict(
-        visible=False,
-        showbackground=False,
-        showgrid=False,
-        showline=False,
-        showticklabels=True,
-        autorange=True,
-    )
-    fig.update_layout(
-        template="plotly_dark",
-        height=height,
-        scene_camera=dict(
-            eye=dict(x=0.0, y=-0.1, z=-2),
-            up=dict(x=0, y=-1.0, z=0),
-            projection=dict(type="orthographic"),
-        ),
-        scene=dict(
-            xaxis=axes,
-            yaxis=axes,
-            zaxis=axes,
-            aspectmode="data",
-            dragmode="orbit",
-        ),
-        margin=dict(l=0, r=0, b=0, t=0, pad=0),
-        legend=dict(orientation="h", yanchor="top", y=0.99, xanchor="left", x=0.1),
-    )
-    return fig
-
-
-def plot_points(
-    fig: go.Figure,
-    pts: np.ndarray,
-    color: str = "rgba(255, 0, 0, 1)",
-    ps: int = 2,
-    colorscale: Optional[str] = None,
-    name: Optional[str] = None,
-):
-    """Plot a set of 3D points."""
-    x, y, z = pts.T
-    tr = go.Scatter3d(
-        x=x,
-        y=y,
-        z=z,
-        mode="markers",
-        name=name,
-        legendgroup=name,
-        marker=dict(size=ps, color=color, line_width=0.0, colorscale=colorscale),
-    )
-    fig.add_trace(tr)
-
-
-def plot_camera(
-    fig: go.Figure,
-    R: np.ndarray,
-    t: np.ndarray,
-    K: np.ndarray,
-    color: str = "rgb(0, 0, 255)",
-    name: Optional[str] = None,
-    legendgroup: Optional[str] = None,
-    fill: bool = False,
-    size: float = 1.0,
-    text: Optional[str] = None,
-):
-    """Plot a camera frustum from pose and intrinsic matrix."""
-    W, H = K[0, 2] * 2, K[1, 2] * 2
-    corners = np.array([[0, 0], [W, 0], [W, H], [0, H], [0, 0]])
-    if size is not None:
-        image_extent = max(size * W / 1024.0, size * H / 1024.0)
-        world_extent = max(W, H) / (K[0, 0] + K[1, 1]) / 0.5
-        scale = 0.5 * image_extent / world_extent
-    else:
-        scale = 1.0
-    corners = to_homogeneous(corners) @ np.linalg.inv(K).T
-    corners = (corners / 2 * scale) @ R.T + t
-    legendgroup = legendgroup if legendgroup is not None else name
-
-    x, y, z = np.concatenate(([t], corners)).T
-    i = [0, 0, 0, 0]
-    j = [1, 2, 3, 4]
-    k = [2, 3, 4, 1]
-
-    if fill:
-        pyramid = go.Mesh3d(
-            x=x,
-            y=y,
-            z=z,
-            color=color,
-            i=i,
-            j=j,
-            k=k,
-            legendgroup=legendgroup,
-            name=name,
-            showlegend=False,
-            hovertemplate=text.replace("\n", "<br>") if text else None,
-        )
-        fig.add_trace(pyramid)
-
-    triangles = np.vstack((i, j, k)).T
-    vertices = np.concatenate(([t], corners))
-    tri_points = np.array([vertices[i] for i in triangles.reshape(-1)])
-    x, y, z = tri_points.T
-
-    pyramid = go.Scatter3d(
-        x=x,
-        y=y,
-        z=z,
-        mode="lines",
-        legendgroup=legendgroup,
-        name=name,
-        line=dict(color=color, width=1),
-        showlegend=False,
-        hovertemplate=text.replace("\n", "<br>") if text else None,
-    )
-    fig.add_trace(pyramid)
-
-
-def plot_camera_colmap(
-    fig: go.Figure, cam_from_world: pycolmap.Rigid3d, camera: pycolmap.Camera, **kwargs
-):
-    """Plot a camera frustum from PyCOLMAP objects"""
-    world_t_camera = cam_from_world.inverse()
-    plot_camera(
-        fig,
-        world_t_camera.rotation.matrix(),
-        world_t_camera.translation,
-        camera.calibration_matrix(),
-        **kwargs
-    )
-
-
-def plot_image_colmap(
-    fig: go.Figure,
-    image: pycolmap.Image,
-    camera: pycolmap.Camera,
-    name: Optional[str] = None,
-    **kwargs
-):
-    """Plot a camera frustum from a PyCOLMAP image."""
-    plot_camera_colmap(
-        fig,
-        image.cam_from_world(),
-        camera,
-        name=name or str(image.image_id),
-        text=str(image),
-        **kwargs
-    )
-
-
-def plot_cameras(fig: go.Figure, reconstruction: pycolmap.Reconstruction, **kwargs):
-    """Plot a camera as a cone with camera frustum."""
-    for image_id, image in reconstruction.images.items():
-        plot_image_colmap(fig, image, reconstruction.cameras[image.camera_id], **kwargs)
-
-
-def plot_reconstruction(
-    fig: go.Figure,
-    rec: pycolmap.Reconstruction,
-    max_reproj_error: float = 6.0,
-    color: str = "rgb(0, 0, 255)",
-    name: Optional[str] = None,
-    min_track_length: int = 2,
-    points: bool = True,
-    cameras: bool = True,
-    points_rgb: bool = True,
-    cs: float = 1.0,
-):
-    # Filter outliers
-    bbs = rec.compute_bounding_box(0.001, 0.999)
-    # Filter points, use original reproj error here
-    p3Ds = [
-        p3D
-        for _, p3D in rec.points3D.items()
-        if (
-            bbs.contains_point(p3D.xyz)
-            and p3D.error <= max_reproj_error
-            and p3D.track.length() >= min_track_length
-        )
-    ]
-    xyzs = [p3D.xyz for p3D in p3Ds]
-    if points_rgb:
-        pcolor = [p3D.color for p3D in p3Ds]
-    else:
-        pcolor = color
-    if points:
-        plot_points(fig, np.array(xyzs), color=pcolor, ps=1, name=name)
-    if cameras:
-        plot_cameras(fig, rec, color=color, legendgroup=name, size=cs)
diff -Naur '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=pipelines' '--exclude=extractors' '--exclude=matchers' /tmp/hloc-latest/hloc/utils/viz.py hloc/utils/viz.py
--- /tmp/hloc-latest/hloc/utils/viz.py	2026-01-16 16:13:56.815239491 -0700
+++ hloc/utils/viz.py	2026-01-16 16:16:08.547549327 -0700
@@ -1,3 +1,6 @@
+# SPDX-FileCopyrightText: (C) 2025 Intel Corporation
+# SPDX-License-Identifier: Apache-2.0
+
 """
 2D visualization primitives based on Matplotlib.
 
@@ -7,109 +10,114 @@
 """
 
 import matplotlib
-import matplotlib.patheffects as path_effects
 import matplotlib.pyplot as plt
+import matplotlib.patheffects as path_effects
 import numpy as np
 
 
 def cm_RdGn(x):
-    """Custom colormap: red (0) -> yellow (0.5) -> green (1)."""
-    x = np.clip(x, 0, 1)[..., None] * 2
-    c = x * np.array([[0, 1.0, 0]]) + (2 - x) * np.array([[1.0, 0, 0]])
-    return np.clip(c, 0, 1)
-
-
-def plot_images(
-    imgs, titles=None, cmaps="gray", dpi=100, pad=0.5, adaptive=True, figsize=4.5
-):
-    """Plot a set of images horizontally.
-    Args:
-        imgs: a list of NumPy or PyTorch images, RGB (H, W, 3) or mono (H, W).
-        titles: a list of strings, as titles for each image.
-        cmaps: colormaps for monochrome images.
-        adaptive: whether the figure size should fit the image aspect ratios.
-    """
-    n = len(imgs)
-    if not isinstance(cmaps, (list, tuple)):
-        cmaps = [cmaps] * n
-
-    if adaptive:
-        ratios = [i.shape[1] / i.shape[0] for i in imgs]  # W / H
-    else:
-        ratios = [4 / 3] * n
-    figsize = [sum(ratios) * figsize, figsize]
-    fig, axs = plt.subplots(
-        1, n, figsize=figsize, dpi=dpi, gridspec_kw={"width_ratios": ratios}
-    )
-    if n == 1:
-        axs = [axs]
-    for i, (img, ax) in enumerate(zip(imgs, axs)):
-        ax.imshow(img, cmap=plt.get_cmap(cmaps[i]))
-        ax.set_axis_off()
-        if titles:
-            ax.set_title(titles[i])
-    fig.tight_layout(pad=pad)
+  """Custom colormap: red (0) -> yellow (0.5) -> green (1)."""
+  x = np.clip(x, 0, 1)[..., None] * 2
+  c = x * np.array([[0, 1.0, 0]]) + (2 - x) * np.array([[1.0, 0, 0]])
+  return np.clip(c, 0, 1)
+
+
+def plot_images(imgs, titles=None, cmaps="gray", dpi=100, pad=0.5, adaptive=True):
+  """Plot a set of images horizontally.
+  Args:
+      imgs: a list of NumPy or PyTorch images, RGB (H, W, 3) or mono (H, W).
+      titles: a list of strings, as titles for each image.
+      cmaps: colormaps for monochrome images.
+      adaptive: whether the figure size should fit the image aspect ratios.
+  """
+  n = len(imgs)
+  if not isinstance(cmaps, (list, tuple)):
+    cmaps = [cmaps] * n
+
+  if adaptive:
+    ratios = [i.shape[1] / i.shape[0] for i in imgs]  # W / H
+  else:
+    ratios = [4 / 3] * n
+  figsize = [sum(ratios) * 4.5, 4.5]
+  fig, ax = plt.subplots(
+      1, n, figsize=figsize, dpi=dpi, gridspec_kw={"width_ratios": ratios}
+  )
+  if n == 1:
+    ax = [ax]
+  for i in range(n):
+    ax[i].imshow(imgs[i], cmap=plt.get_cmap(cmaps[i]))
+    ax[i].get_yaxis().set_ticks([])
+    ax[i].get_xaxis().set_ticks([])
+    ax[i].set_axis_off()
+    for spine in ax[i].spines.values():  # remove frame
+      spine.set_visible(False)
+    if titles:
+      ax[i].set_title(titles[i])
+  fig.tight_layout(pad=pad)
 
 
 def plot_keypoints(kpts, colors="lime", ps=4):
-    """Plot keypoints for existing images.
-    Args:
-        kpts: list of ndarrays of size (N, 2).
-        colors: string, or list of list of tuples (one for each keypoints).
-        ps: size of the keypoints as float.
-    """
-    if not isinstance(colors, list):
-        colors = [colors] * len(kpts)
-    axes = plt.gcf().axes
-    for a, k, c in zip(axes, kpts, colors):
-        a.scatter(k[:, 0], k[:, 1], c=c, s=ps, linewidths=0)
+  """Plot keypoints for existing images.
+  Args:
+      kpts: list of ndarrays of size (N, 2).
+      colors: string, or list of list of tuples (one for each keypoints).
+      ps: size of the keypoints as float.
+  """
+  if not isinstance(colors, list):
+    colors = [colors] * len(kpts)
+  axes = plt.gcf().axes
+  for a, k, c in zip(axes, kpts, colors):
+    if k.size > 0:
+      a.scatter(k[:, 0], k[:, 1], c=c, s=ps, linewidths=0)
 
 
 def plot_matches(kpts0, kpts1, color=None, lw=1.5, ps=4, indices=(0, 1), a=1.0):
-    """Plot matches for a pair of existing images.
-    Args:
-        kpts0, kpts1: corresponding keypoints of size (N, 2).
-        color: color of each match, string or RGB tuple. Random if not given.
-        lw: width of the lines.
-        ps: size of the end points (no endpoint if ps=0)
-        indices: indices of the images to draw the matches on.
-        a: alpha opacity of the match lines.
-    """
-    fig = plt.gcf()
-    ax = fig.axes
-    assert len(ax) > max(indices)
-    ax0, ax1 = ax[indices[0]], ax[indices[1]]
-    fig.canvas.draw()
-
-    assert len(kpts0) == len(kpts1)
-    if color is None:
-        color = matplotlib.cm.hsv(np.random.rand(len(kpts0))).tolist()
-    elif len(color) > 0 and not isinstance(color[0], (tuple, list)):
-        color = [color] * len(kpts0)
-
-    if lw > 0:
-        # transform the points into the figure coordinate system
-        for i in range(len(kpts0)):
-            fig.add_artist(
-                matplotlib.patches.ConnectionPatch(
-                    xyA=(kpts0[i, 0], kpts0[i, 1]),
-                    coordsA=ax0.transData,
-                    xyB=(kpts1[i, 0], kpts1[i, 1]),
-                    coordsB=ax1.transData,
-                    zorder=1,
-                    color=color[i],
-                    linewidth=lw,
-                    alpha=a,
-                )
-            )
-
-    # freeze the axes to prevent the transform to change
-    ax0.autoscale(enable=False)
-    ax1.autoscale(enable=False)
-
-    if ps > 0:
-        ax0.scatter(kpts0[:, 0], kpts0[:, 1], c=color, s=ps)
-        ax1.scatter(kpts1[:, 0], kpts1[:, 1], c=color, s=ps)
+  """Plot matches for a pair of existing images.
+  Args:
+      kpts0, kpts1: corresponding keypoints of size (N, 2).
+      color: color of each match, string or RGB tuple. Random if not given.
+      lw: width of the lines.
+      ps: size of the end points (no endpoint if ps=0)
+      indices: indices of the images to draw the matches on.
+      a: alpha opacity of the match lines.
+  """
+  fig = plt.gcf()
+  ax = fig.axes
+  assert len(ax) > max(indices)
+  ax0, ax1 = ax[indices[0]], ax[indices[1]]
+  fig.canvas.draw()
+
+  assert len(kpts0) == len(kpts1)
+  if color is None:
+    color = matplotlib.cm.hsv(np.random.rand(len(kpts0))).tolist()
+  elif len(color) > 0 and not isinstance(color[0], (tuple, list)):
+    color = [color] * len(kpts0)
+
+  if lw > 0:
+    # transform the points into the figure coordinate system
+    transFigure = fig.transFigure.inverted()
+    fkpts0 = transFigure.transform(ax0.transData.transform(kpts0))
+    fkpts1 = transFigure.transform(ax1.transData.transform(kpts1))
+    fig.lines += [
+        matplotlib.lines.Line2D(
+            (fkpts0[i, 0], fkpts1[i, 0]),
+            (fkpts0[i, 1], fkpts1[i, 1]),
+            zorder=1,
+            transform=fig.transFigure,
+            c=color[i],
+            linewidth=lw,
+            alpha=a,
+        )
+        for i in range(len(kpts0))
+    ]
+
+  # freeze the axes to prevent the transform to change
+  ax0.autoscale(enable=False)
+  ax1.autoscale(enable=False)
+
+  if ps > 0:
+    ax0.scatter(kpts0[:, 0], kpts0[:, 1], c=color, s=ps)
+    ax1.scatter(kpts1[:, 0], kpts1[:, 1], c=color, s=ps)
 
 
 def add_text(
@@ -123,19 +131,19 @@
     ha="left",
     va="top",
 ):
-    ax = plt.gcf().axes[idx]
-    t = ax.text(
-        *pos, text, fontsize=fs, ha=ha, va=va, color=color, transform=ax.transAxes
+  ax = plt.gcf().axes[idx]
+  t = ax.text(
+      *pos, text, fontsize=fs, ha=ha, va=va, color=color, transform=ax.transAxes
+  )
+  if lcolor is not None:
+    t.set_path_effects(
+        [
+            path_effects.Stroke(linewidth=lwidth, foreground=lcolor),
+            path_effects.Normal(),
+        ]
     )
-    if lcolor is not None:
-        t.set_path_effects(
-            [
-                path_effects.Stroke(linewidth=lwidth, foreground=lcolor),
-                path_effects.Normal(),
-            ]
-        )
 
 
 def save_plot(path, **kw):
-    """Save the current figure without any white margin."""
-    plt.savefig(path, bbox_inches="tight", pad_inches=0, **kw)
+  """Save the current figure without any white margin."""
+  plt.savefig(path, bbox_inches="tight", pad_inches=0, **kw)
diff -Naur '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=pipelines' '--exclude=extractors' '--exclude=matchers' /tmp/hloc-latest/hloc/visualization.py hloc/visualization.py
--- /tmp/hloc-latest/hloc/visualization.py	2026-01-16 16:13:56.815239491 -0700
+++ hloc/visualization.py	2026-01-16 16:16:08.547638666 -0700
@@ -1,68 +1,76 @@
-import pickle
-import random
+# SPDX-FileCopyrightText: (C) 2025 Intel Corporation
+# SPDX-License-Identifier: Apache-2.0
 
+from matplotlib import cm
+import random
 import numpy as np
+import pickle
+from pathlib import Path
+import h5py
 import pycolmap
-from matplotlib import cm
 
+from .utils.viz import (
+    plot_images,
+    plot_keypoints,
+    plot_matches,
+    cm_RdGn,
+    add_text,
+    save_plot,
+)
 from .utils.io import read_image
-from .utils.viz import add_text, cm_RdGn, plot_images, plot_keypoints, plot_matches
 
 
-def visualize_sfm_2d(
-    reconstruction, image_dir, color_by="visibility", selected=[], n=1, seed=0, dpi=75
-):
-    assert image_dir.exists()
-    if not isinstance(reconstruction, pycolmap.Reconstruction):
-        reconstruction = pycolmap.Reconstruction(reconstruction)
+def visualize_sfm_2d(reconstruction,
+                     image_dir,
+                     color_by="visibility",
+                     selected=[],
+                     n=1,
+                     seed=0,
+                     dpi=75):
+  assert image_dir.exists()
+  if not isinstance(reconstruction, pycolmap.Reconstruction):
+    reconstruction = pycolmap.Reconstruction(reconstruction)
+
+  if not selected:
+    image_ids = reconstruction.reg_image_ids()
+    selected = random.Random(seed).sample(image_ids,
+                                          min(n, len(image_ids)))
+
+  for i in selected:
+    image = reconstruction.images[i]
+    keypoints = np.array([p.xy for p in image.points2D])
+    visible = np.array([p.has_point3D() for p in image.points2D])
+
+    if color_by == "visibility":
+      color = [(0, 0, 1) if v else (1, 0, 0) for v in visible]
+      text = f"visible: {np.count_nonzero(visible)}/{len(visible)}"
+    elif color_by == "track_length":
+      tl = np.array([
+          reconstruction.points3D[p.point3D_id].track.length()
+          if p.has_point3D() else 1 for p in image.points2D
+      ])
+      max_, med_ = np.max(tl), np.median(tl[tl > 1])
+      tl = np.log(tl)
+      color = cm.jet(tl / tl.max()).tolist()
+      text = f"max/median track length: {max_}/{med_}"
+    elif color_by == "depth":
+      p3ids = [p.point3D_id for p in image.points2D if p.has_point3D()]
+      z = np.array([
+          image.transform_to_image(reconstruction.points3D[j].xyz)[-1]
+          for j in p3ids
+      ])
+      z -= z.min()
+      color = cm.jet(z / np.percentile(z, 99.9))
+      text = f"visible: {np.count_nonzero(visible)}/{len(visible)}"
+      keypoints = keypoints[visible]
+    else:
+      raise NotImplementedError(f"Coloring not implemented: {color_by}.")
 
-    if not selected:
-        image_ids = list(reconstruction.reg_image_ids())
-        selected = random.Random(seed).sample(image_ids, min(n, len(image_ids)))
-
-    for i in selected:
-        image = reconstruction.images[i]
-        keypoints = np.array([p.xy for p in image.points2D])
-        visible = np.array([p.has_point3D() for p in image.points2D])
-
-        if color_by == "visibility":
-            color = [(0, 0, 1) if v else (1, 0, 0) for v in visible]
-            text = f"visible: {np.count_nonzero(visible)}/{len(visible)}"
-        elif color_by == "track_length":
-            tl = np.array(
-                [
-                    (
-                        reconstruction.points3D[p.point3D_id].track.length()
-                        if p.has_point3D()
-                        else 1
-                    )
-                    for p in image.points2D
-                ]
-            )
-            max_, med_ = np.max(tl), np.median(tl[tl > 1])
-            tl = np.log(tl)
-            color = cm.jet(tl / tl.max()).tolist()
-            text = f"max/median track length: {max_}/{med_}"
-        elif color_by == "depth":
-            p3ids = [p.point3D_id for p in image.points2D if p.has_point3D()]
-            z = np.array(
-                [
-                    (image.cam_from_world() * reconstruction.points3D[j].xyz)[-1]
-                    for j in p3ids
-                ]
-            )
-            z -= z.min()
-            color = cm.jet(z / np.percentile(z, 99.9))
-            text = f"visible: {np.count_nonzero(visible)}/{len(visible)}"
-            keypoints = keypoints[visible]
-        else:
-            raise NotImplementedError(f"Coloring not implemented: {color_by}.")
-
-        name = image.name
-        plot_images([read_image(image_dir / name)], dpi=dpi)
-        plot_keypoints([keypoints], colors=[color], ps=4)
-        add_text(0, text)
-        add_text(0, name, pos=(0.01, 0.01), fs=5, lcolor=None, va="bottom")
+    name = image.name
+    plot_images([read_image(image_dir / name)], dpi=dpi)
+    plot_keypoints([keypoints], colors=[color], ps=4)
+    add_text(0, text)
+    add_text(0, name, pos=(0.01, 0.01), fs=5, lcolor=None, va="bottom")
 
 
 def visualize_loc(
@@ -76,26 +84,26 @@
     prefix=None,
     **kwargs,
 ):
-    assert image_dir.exists()
+  assert image_dir.exists()
 
-    with open(str(results) + "_logs.pkl", "rb") as f:
-        logs = pickle.load(f)
+  with open(str(results) + "_logs.pkl", "rb") as f:
+    logs = pickle.load(f)
 
-    if not selected:
-        queries = list(logs["loc"].keys())
-        if prefix:
-            queries = [q for q in queries if q.startswith(prefix)]
-        selected = random.Random(seed).sample(queries, min(n, len(queries)))
+  if not selected:
+    queries = list(logs["loc"].keys())
+    if prefix:
+      queries = [q for q in queries if q.startswith(prefix)]
+    selected = random.Random(seed).sample(queries, min(n, len(queries)))
 
-    if reconstruction is not None:
-        if not isinstance(reconstruction, pycolmap.Reconstruction):
-            reconstruction = pycolmap.Reconstruction(reconstruction)
+  if reconstruction is not None:
+    if not isinstance(reconstruction, pycolmap.Reconstruction):
+      reconstruction = pycolmap.Reconstruction(reconstruction)
 
-    for qname in selected:
-        loc = logs["loc"][qname]
-        visualize_loc_from_log(
-            image_dir, qname, loc, reconstruction, db_image_dir, **kwargs
-        )
+  for qname in selected:
+    loc = logs["loc"][qname]
+    if loc["PnP_ret"]["success"]:
+      visualize_loc_from_log(image_dir, qname, loc, reconstruction,
+                             db_image_dir, **kwargs)
 
 
 def visualize_loc_from_log(
@@ -106,60 +114,101 @@
     db_image_dir=None,
     top_k_db=2,
     dpi=75,
+    color_by="confidence",
+    feature_paths=None,
+    savefigdir=None,
 ):
-    q_image = read_image(image_dir / query_name)
-    if loc.get("covisibility_clustering", False):
-        # select the first, largest cluster if the localization failed
-        loc = loc["log_clusters"][loc["best_cluster"] or 0]
-
-    inliers = np.array(loc["PnP_ret"]["inlier_mask"])
-    mkp_q = loc["keypoints_query"]
-    n = len(loc["db"])
+  """
+  Args:
+      color_by (str): One of 'confidence and 'random'
+  """
+
+  q_image = read_image(image_dir / query_name)
+  if loc.get("covisibility_clustering", False):
+    # select the first, largest cluster if the localization failed
+    loc = loc["log_clusters"][loc["best_cluster"] or 0]
+
+  inliers = np.array(loc["PnP_ret"]["inliers"])
+  mkp_q = loc["keypoints_query"]
+  n = len(loc["db"])
+  if reconstruction is not None:
+    # for each pair of query keypoint and its matched 3D point,
+    # we need to find its corresponding keypoint in each database image
+    # that observes it. We also count the number of inliers in each.
+    kp_idxs, kp_to_3D_to_db = loc["keypoint_index_to_db"]
+    counts = np.zeros(n)
+    dbs_kp_q_db = [[] for _ in range(n)]
+    inliers_dbs = [[] for _ in range(n)]
+    for i, (inl, (p3D_id,
+                  db_idxs)) in enumerate(zip(inliers, kp_to_3D_to_db)):
+      track = reconstruction.points3D[p3D_id].track
+      track = {el.image_id: el.point2D_idx for el in track.elements}
+      for db_idx in db_idxs:
+        counts[db_idx] += inl
+        kp_db = track[loc["db"][db_idx]]
+        dbs_kp_q_db[db_idx].append((i, kp_db))
+        inliers_dbs[db_idx].append(inl)
+  else:
+    # for inloc the database keypoints are already in the logs
+    assert "keypoints_db" in loc
+    assert "indices_db" in loc
+    counts = np.array(
+        [np.sum(loc["indices_db"][inliers] == i) for i in range(n)])
+
+  if feature_paths is not None:
+    feature_files = list(h5py.File(fp, "r") for fp in feature_paths)
+  # display the database images with the most inlier matches
+  db_sort = np.argsort(-counts)
+  for db_idx in db_sort[:top_k_db]:
     if reconstruction is not None:
-        # for each pair of query keypoint and its matched 3D point,
-        # we need to find its corresponding keypoint in each database image
-        # that observes it. We also count the number of inliers in each.
-        kp_idxs, kp_to_3D_to_db = loc["keypoint_index_to_db"]
-        counts = np.zeros(n)
-        dbs_kp_q_db = [[] for _ in range(n)]
-        inliers_dbs = [[] for _ in range(n)]
-        for i, (inl, (p3D_id, db_idxs)) in enumerate(zip(inliers, kp_to_3D_to_db)):
-            track = reconstruction.points3D[p3D_id].track
-            track = {el.image_id: el.point2D_idx for el in track.elements}
-            for db_idx in db_idxs:
-                counts[db_idx] += inl
-                kp_db = track[loc["db"][db_idx]]
-                dbs_kp_q_db[db_idx].append((i, kp_db))
-                inliers_dbs[db_idx].append(inl)
+      db = reconstruction.images[loc["db"][db_idx]]
+      db_name = db.name
+      db_kp_q_db = np.array(dbs_kp_q_db[db_idx])
+      kp_q = mkp_q[db_kp_q_db[:, 0]]
+      kp_db = np.array([db.points2D[i].xy for i in db_kp_q_db[:, 1]])
+      inliers_db = inliers_dbs[db_idx]
     else:
-        # for inloc the database keypoints are already in the logs
-        assert "keypoints_db" in loc
-        assert "indices_db" in loc
-        counts = np.array([np.sum(loc["indices_db"][inliers] == i) for i in range(n)])
-
-    # display the database images with the most inlier matches
-    db_sort = np.argsort(-counts)
-    for db_idx in db_sort[:top_k_db]:
-        if reconstruction is not None:
-            db = reconstruction.images[loc["db"][db_idx]]
-            db_name = db.name
-            db_kp_q_db = np.array(dbs_kp_q_db[db_idx])
-            kp_q = mkp_q[db_kp_q_db[:, 0]]
-            kp_db = np.array([db.points2D[i].xy for i in db_kp_q_db[:, 1]])
-            inliers_db = inliers_dbs[db_idx]
-        else:
-            db_name = loc["db"][db_idx]
-            kp_q = mkp_q[loc["indices_db"] == db_idx]
-            kp_db = loc["keypoints_db"][loc["indices_db"] == db_idx]
-            inliers_db = inliers[loc["indices_db"] == db_idx]
-
-        db_image = read_image((db_image_dir or image_dir) / db_name)
-        color = cm_RdGn(inliers_db).tolist()
-        text = f"inliers: {sum(inliers_db)}/{len(inliers_db)}"
-
-        plot_images([q_image, db_image], dpi=dpi)
-        plot_matches(kp_q, kp_db, color, a=0.1)
-        add_text(0, text)
-        opts = dict(pos=(0.01, 0.01), fs=5, lcolor=None, va="bottom")
-        add_text(0, query_name, **opts)
-        add_text(1, db_name, **opts)
+      db_name = loc["db"][db_idx]
+      kp_q = mkp_q[loc["indices_db"] == db_idx]
+      kp_db = loc["keypoints_db"][loc["indices_db"] == db_idx]
+      inliers_db = inliers[loc["indices_db"] == db_idx]
+
+    db_image = read_image((db_image_dir or image_dir) / db_name)
+    # color = None => random colors
+    color = cm_RdGn(
+        inliers_db).tolist() if color_by == "confidence" else None
+    text = f"inliers: {sum(inliers_db)}/{len(inliers_db)}"
+    if "e_t" in loc:
+      text += f'\ne_t: {loc["e_t"]*100:.2f}cm'
+    if "e_t_rel" in loc:
+      text += f' ({loc["e_t_rel"]*100:.2f}%)'
+    if "e_R" in loc:
+      text += f' e_R: {loc["e_R"]:.2f}deg'
+
+    plot_images([q_image, db_image], dpi=dpi)
+    if feature_paths is not None:
+      kpq = np.vstack(tuple(ff[query_name]["keypoints"].__array__().astype(
+          np.float32) for ff in feature_files))
+      kpr = np.vstack(tuple(ff[db_name]["keypoints"].__array__().astype(
+          np.float32) for ff in feature_files))
+      plot_keypoints([kpq, kpr], colors='tab:orange', ps=1)
+    plot_matches(
+        kp_q[inliers_db],
+        kp_db[inliers_db],
+        None if color is None else color[inliers_db],
+        ps=4,
+        a=0.1,
+    )
+    plot_matches(kp_q[~inliers_db],
+                 kp_db[~inliers_db], [1.0, 0.0, 0.0],
+                 ps=1,
+                 a=0.1)
+    add_text(0, text, fs=8)
+    opts = dict(pos=(0.01, 0.01), fs=6, lcolor=None, va="bottom")
+    add_text(0, query_name, **opts)
+    add_text(1, db_name, **opts)
+    if savefigdir is not None:
+      figname = Path(
+          query_name.split(".")[0].replace("/", "_") + "_" +
+          db_name.split(".")[0].replace("/", "_") + ".jpg")
+      save_plot(savefigdir / figname)
